[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blogs",
    "section": "",
    "text": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\n100-days-of-polars\n\n\n\nLearn to maintain data integrity in Polars through null handling techniques and schema validation strategies\n\n\n\n\n\nFeb 2, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\n100 Days of Polars - Day 005: Working with Structs - Nested Data Structures\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\n100-days-of-polars\n\n\n\nUnderstanding how to work with struct columns in Polars for hierarchical and nested data\n\n\n\n\n\nJan 29, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\n100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\n100-days-of-polars\n\n\n\nUnderstanding how to work with nested list data in Polars using explode, implode, and list operations\n\n\n\n\n\nJan 28, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\n100 Days of Polars - Day 003: map_elements vs map_batches vs Native Expressions\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\n100-days-of-polars\n\n\n\nUnderstanding when to use map_elements, map_batches, and native Polars expressions\n\n\n\n\n\nJan 19, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\n100 Days of Polars - Day 002: Window Functions\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\n100-days-of-polars\n\n\n\nMaster window functions in Polars for advanced data transformations\n\n\n\n\n\nJan 18, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\n100 Days of Polars - Day 001: Polars Selectors\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\n100-days-of-polars\n\n\n\nPolars selectors for efficient column selection\n\n\n\n\n\nJan 17, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\nDifference between TFLite Python API vs TFLite Micro Python API\n\n\n\n\n\n\ntflite\n\n\ntflite-micro\n\n\ncode\n\n\ntinyml\n\n\n\n\n\n\n\n\n\nAug 5, 2025\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nAug 2, 2025\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "",
    "text": "Data integrity is the foundation of reliable data pipelines. Without proper validation and null handling, your analyses can produce misleading results or fail entirely. In Polars, you have powerful tools to ensure your data meets quality standards before processing.\nToday we’ll cover: - Null value handling: Detection, filling, and removal strategies - Schema validation: Ensuring data types match expectations - Data quality checks: Comprehensive validation patterns"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#introduction",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#introduction",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "",
    "text": "Data integrity is the foundation of reliable data pipelines. Without proper validation and null handling, your analyses can produce misleading results or fail entirely. In Polars, you have powerful tools to ensure your data meets quality standards before processing.\nToday we’ll cover: - Null value handling: Detection, filling, and removal strategies - Schema validation: Ensuring data types match expectations - Data quality checks: Comprehensive validation patterns"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#understanding-nulls-in-polars",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#understanding-nulls-in-polars",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Understanding Nulls in Polars",
    "text": "Understanding Nulls in Polars\nPolars uses null to represent missing values, distinct from NaN (not-a-number for floats) and None.\nimport polars as pl\nimport numpy as np\n\n# Create a DataFrame with various null-like values\ndf = pl.DataFrame({\n    \"id\": [1, 2, 3, 4, 5],\n    \"name\": [\"Alice\", \"Bob\", None, \"David\", \"Eve\"],\n    \"age\": [25, None, 30, 35, None],\n    \"score\": [85.5, 90.0, np.nan, 78.0, 92.5]\n})\n\nprint(df)\nshape: (5, 4)\n┌─────┬────────┬──────┬───────┐\n│ id  ┆ name   ┆ age  ┆ score │\n│ --- ┆ ---    ┆ ---  ┆ ---   │\n│ i64 ┆ str    ┆ i64  ┆ f64   │\n╞═════╪════════╪══════╪═══════╡\n│ 1   ┆ Alice  ┆ 25   ┆ 85.5  │\n│ 2   ┆ Bob    ┆ null ┆ 90.0  │\n│ 3   ┆ null   ┆ 30   ┆ NaN   │\n│ 4   ┆ David  ┆ 35   ┆ 78.0  │\n│ 5   ┆ Eve    ┆ null ┆ 92.5  │\n└─────┴────────┴──────┴───────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#detecting-null-values",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#detecting-null-values",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Detecting Null Values",
    "text": "Detecting Null Values\n\nBasic Null Detection\n# Check for nulls in each column\nnull_counts = df.null_count()\nprint(null_counts)\n\n# Check if any nulls exist in the entire DataFrame\nhas_nulls = df.null_count().sum_horizontal().max() &gt; 0\nprint(f\"DataFrame has nulls: {has_nulls}\")\n\n\nColumn-Specific Null Checks\n# Count nulls per column\nfor col in df.columns:\n    null_count = df[col].is_null().sum()\n    print(f\"{col}: {null_count} nulls\")\n\n# Alternative: Get null percentage\nnull_pct = df.null_count() / df.height * 100\nprint(null_pct)\n\n\nFinding Rows with Nulls\n# Find rows with any null values\ndf_with_nulls = df.filter(\n    pl.any_horizontal(pl.all().is_null())\n)\nprint(df_with_nulls)\n\n# Find rows where specific columns have nulls\ndf_name_or_age_null = df.filter(\n    pl.col(\"name\").is_null() | pl.col(\"age\").is_null()\n)\nprint(df_name_or_age_null)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#handling-null-values",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#handling-null-values",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Handling Null Values",
    "text": "Handling Null Values\n\nStrategy 1: Drop Nulls\n# Drop rows with ANY null values\ndf_clean = df.drop_nulls()\n\n# Drop rows with nulls in specific columns\ndf_clean_names = df.drop_nulls(subset=[\"name\"])\n\n# Drop columns that contain any nulls\ndf_no_null_cols = df.select([\n    col for col in df.columns \n    if df[col].null_count() == 0\n])\n\n\nStrategy 2: Fill Nulls\n# Fill with a constant value\ndf_filled = df.with_columns(\n    pl.col(\"age\").fill_null(0),\n    pl.col(\"name\").fill_null(\"Unknown\")\n)\n\n# Fill with forward fill (previous value)\ndf_ffill = df.with_columns(\n    pl.col(\"age\").fill_null(strategy=\"forward\")\n)\n\n# Fill with backward fill (next value)\ndf_bfill = df.with_columns(\n    pl.col(\"age\").fill_null(strategy=\"backward\")\n)\n\n# Fill with mean (for numeric columns)\ndf_mean_filled = df.with_columns(\n    pl.col(\"age\").fill_null(pl.col(\"age\").mean())\n)\n\n# Fill with median\nfrom polars import DataFrame\n\ndf_median_filled = df.with_columns(\n    pl.col(\"age\").fill_null(pl.col(\"age\").median())\n)\n\n# Fill with interpolation (linear)\ndf_interpolated = df.with_columns(\n    pl.col(\"age\").interpolate()\n)\n\n\nStrategy 3: Conditional Filling\n# Fill based on conditions\ndf_conditional = df.with_columns(\n    pl.when(pl.col(\"age\").is_null())\n    .then(18)  # Default age if missing\n    .otherwise(pl.col(\"age\"))\n    .alias(\"age_filled\")\n)\n\n# Fill based on group statistics\ndf_grouped_fill = df.with_columns(\n    pl.col(\"age\").fill_null(\n        pl.col(\"age\").mean().over(\"department\")  # If you have a department column\n    )\n)\n\n\nHandling NaN Values\n# Detect NaN (different from null)\ndf_with_nan_check = df.with_columns(\n    pl.col(\"score\").is_nan().alias(\"is_nan\")\n)\n\n# Replace NaN with null first, then handle\ndf_no_nan = df.with_columns(\n    pl.col(\"score\").replace({float(\"nan\"): None})\n)\n\n# Or fill NaN directly\ndf_filled_nan = df.with_columns(\n    pl.when(pl.col(\"score\").is_nan())\n    .then(0.0)\n    .otherwise(pl.col(\"score\"))\n)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#schema-validation",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#schema-validation",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Schema Validation",
    "text": "Schema Validation\n\nUnderstanding Polars Schemas\n# Check current schema\nprint(df.schema)\n\n# Get detailed schema information\nfor name, dtype in df.schema.items():\n    print(f\"{name}: {dtype}\")\n\n\nStrict Schema Definition\nfrom typing import Dict\n\n# Define expected schema\nexpected_schema: Dict[str, pl.DataType] = {\n    \"id\": pl.Int64,\n    \"name\": pl.Utf8,\n    \"age\": pl.Int64,\n    \"score\": pl.Float64,\n    \"is_active\": pl.Boolean\n}\n\ndef validate_schema(df: pl.DataFrame, expected: Dict[str, pl.DataType]) -&gt; bool:\n    \"\"\"Validate that DataFrame matches expected schema.\"\"\"\n    actual_schema = df.schema\n    \n    # Check all expected columns exist\n    for col, dtype in expected.items():\n        if col not in actual_schema:\n            print(f\"Missing column: {col}\")\n            return False\n        if actual_schema[col] != dtype:\n            print(f\"Type mismatch for {col}: expected {dtype}, got {actual_schema[col]}\")\n            return False\n    \n    return True\n\n# Use the validator\nis_valid = validate_schema(df, expected_schema)\nprint(f\"Schema valid: {is_valid}\")\n\n\nSchema Enforcement\ndef enforce_schema(df: pl.DataFrame, schema: Dict[str, pl.DataType]) -&gt; pl.DataFrame:\n    \"\"\"Cast columns to expected types, creating missing columns with nulls.\"\"\"\n    columns = []\n    \n    for col_name, dtype in schema.items():\n        if col_name in df.columns:\n            # Cast existing column to expected type\n            columns.append(pl.col(col_name).cast(dtype))\n        else:\n            # Create column with nulls of expected type\n            columns.append(pl.lit(None).cast(dtype).alias(col_name))\n    \n    return df.select(columns)\n\n# Apply schema enforcement\ndf_enforced = enforce_schema(df, expected_schema)\nprint(df_enforced)\nprint(df_enforced.schema)\n\n\nType Coercion with Error Handling\n# Safe casting that handles errors\ndef safe_cast(df: pl.DataFrame, column: str, dtype: pl.DataType, \n              default=None) -&gt; pl.DataFrame:\n    \"\"\"Attempt to cast column, filling errors with default value.\"\"\"\n    try:\n        return df.with_columns(\n            pl.col(column).cast(dtype, strict=False)\n        )\n    except Exception as e:\n        print(f\"Error casting {column}: {e}\")\n        if default is not None:\n            return df.with_columns(\n                pl.lit(default).cast(dtype).alias(column)\n            )\n        return df\n\n# Example: Cast age to Int32 with fallback\ndf_casted = safe_cast(df, \"age\", pl.Int32, default=0)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#comprehensive-data-quality-checks",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#comprehensive-data-quality-checks",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Comprehensive Data Quality Checks",
    "text": "Comprehensive Data Quality Checks\n\nNull Threshold Validation\ndef check_null_threshold(df: pl.DataFrame, threshold: float = 0.1) -&gt; Dict[str, bool]:\n    \"\"\"Check if any column exceeds null percentage threshold.\"\"\"\n    results = {}\n    null_pcts = df.null_count() / df.height\n    \n    for col in df.columns:\n        col_null_pct = null_pcts[col].item()\n        results[col] = col_null_pct &lt;= threshold\n        if col_null_pct &gt; threshold:\n            print(f\"WARNING: {col} has {col_null_pct:.1%} nulls (threshold: {threshold:.1%})\")\n    \n    return results\n\n# Check with 5% threshold\nnull_checks = check_null_threshold(df, threshold=0.05)\n\n\nRange Validation\ndef validate_ranges(df: pl.DataFrame, ranges: Dict[str, tuple]) -&gt; pl.DataFrame:\n    \"\"\"Add validation columns for numeric ranges.\"\"\"\n    validations = []\n    \n    for col, (min_val, max_val) in ranges.items():\n        validations.append(\n            pl.when((pl.col(col) &lt; min_val) | (pl.col(col) &gt; max_val))\n            .then(False)\n            .otherwise(True)\n            .alias(f\"{col}_valid\")\n        )\n    \n    return df.with_columns(validations)\n\n# Define expected ranges\nranges = {\n    \"age\": (0, 120),\n    \"score\": (0.0, 100.0)\n}\n\ndf_validated = validate_ranges(df, ranges)\nprint(df_validated)\n\n\nUniqueness Constraints\n# Check for duplicates\ndef check_uniqueness(df: pl.DataFrame, columns: list) -&gt; bool:\n    \"\"\"Check if specified columns have unique values.\"\"\"\n    unique_count = df.select(columns).n_unique()\n    total_count = df.height\n    \n    is_unique = unique_count == total_count\n    if not is_unique:\n        duplicates = total_count - unique_count\n        print(f\"Found {duplicates} duplicate rows in columns {columns}\")\n    \n    return is_unique\n\n# Check id column is unique\nis_id_unique = check_uniqueness(df, [\"id\"])\n\n# Find duplicates\nduplicates = df.filter(\n    pl.col(\"id\").is_duplicated()\n)\nprint(duplicates)\n\n\nPattern Validation (Strings)\n# Validate email format\ndef validate_emails(df: pl.DataFrame, email_col: str) -&gt; pl.DataFrame:\n    email_pattern = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n    \n    return df.with_columns(\n        pl.col(email_col)\n        .str.contains(email_pattern)\n        .alias(f\"{email_col}_valid\")\n    )\n\n# Create sample data with emails\nemail_df = pl.DataFrame({\n    \"id\": [1, 2, 3],\n    \"email\": [\"alice@example.com\", \"bob@invalid\", \"charlie@company.org\"]\n})\n\nemail_validated = validate_emails(email_df, \"email\")\nprint(email_validated)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#complete-data-validation-pipeline",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#complete-data-validation-pipeline",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Complete Data Validation Pipeline",
    "text": "Complete Data Validation Pipeline\nclass DataValidator:\n    \"\"\"Comprehensive data validation for Polars DataFrames.\"\"\"\n    \n    def __init__(self, schema: Dict[str, pl.DataType]):\n        self.schema = schema\n        self.errors = []\n    \n    def validate(self, df: pl.DataFrame) -&gt; tuple[bool, pl.DataFrame]:\n        \"\"\"Run all validations and return (is_valid, validated_df).\"\"\"\n        self.errors = []\n        \n        # Step 1: Schema validation\n        if not self._validate_schema(df):\n            return False, df\n        \n        # Step 2: Enforce schema\n        df = self._enforce_schema(df)\n        \n        # Step 3: Check null thresholds\n        self._check_nulls(df)\n        \n        # Step 4: Check duplicates\n        self._check_duplicates(df)\n        \n        is_valid = len(self.errors) == 0\n        return is_valid, df\n    \n    def _validate_schema(self, df: pl.DataFrame) -&gt; bool:\n        for col, dtype in self.schema.items():\n            if col not in df.columns:\n                self.errors.append(f\"Missing required column: {col}\")\n        return len(self.errors) == 0\n    \n    def _enforce_schema(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n        columns = []\n        for col, dtype in self.schema.items():\n            if col in df.columns:\n                columns.append(pl.col(col).cast(dtype, strict=False))\n            else:\n                columns.append(pl.lit(None).cast(dtype).alias(col))\n        return df.select(columns)\n    \n    def _check_nulls(self, df: pl.DataFrame, threshold: float = 0.1):\n        for col in df.columns:\n            null_pct = df[col].is_null().sum() / df.height\n            if null_pct &gt; threshold:\n                self.errors.append(f\"{col}: {null_pct:.1%} nulls exceeds threshold\")\n    \n    def _check_duplicates(self, df: pl.DataFrame):\n        dups = df.height - df.n_unique()\n        if dups &gt; 0:\n            self.errors.append(f\"Found {dups} duplicate rows\")\n    \n    def get_errors(self) -&gt; list:\n        return self.errors\n\n# Usage example\nvalidator = DataValidator(expected_schema)\nis_valid, validated_df = validator.validate(df)\n\nif not is_valid:\n    print(\"Validation errors:\")\n    for error in validator.get_errors():\n        print(f\"  - {error}\")\nelse:\n    print(\"Data validation passed!\")\n    print(validated_df)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#best-practices-for-data-integrity",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#best-practices-for-data-integrity",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Best Practices for Data Integrity",
    "text": "Best Practices for Data Integrity\n\n1. Always Validate at Ingestion\ndef load_and_validate_data(path: str, validator: DataValidator) -&gt; pl.DataFrame:\n    \"\"\"Load data with validation.\"\"\"\n    df = pl.read_csv(path)\n    \n    is_valid, df = validator.validate(df)\n    if not is_valid:\n        raise ValueError(f\"Data validation failed: {validator.get_errors()}\")\n    \n    return df\n\n\n2. Document Null Handling Strategy\ndef process_with_null_strategy(df: pl.DataFrame, \n                               strategy: str = \"drop\") -&gt; pl.DataFrame:\n    \"\"\"\n    Apply null handling strategy.\n    \n    Strategies:\n    - 'drop': Remove rows with nulls\n    - 'mean': Fill numeric nulls with mean\n    - 'zero': Fill numeric with 0, strings with \"Unknown\"\n    - 'interpolate': Linear interpolation\n    \"\"\"\n    if strategy == \"drop\":\n        return df.drop_nulls()\n    elif strategy == \"mean\":\n        return df.with_columns(\n            pl.col(col).fill_null(pl.col(col).mean())\n            for col in df.columns\n            if df[col].dtype in [pl.Float32, pl.Float64, pl.Int32, pl.Int64]\n        )\n    elif strategy == \"zero\":\n        return df.with_columns([\n            pl.col(col).fill_null(0) if df[col].dtype in [pl.Float32, pl.Float64, pl.Int32, pl.Int64]\n            else pl.col(col).fill_null(\"Unknown\")\n            for col in df.columns\n        ])\n    elif strategy == \"interpolate\":\n        return df.with_columns(\n            pl.col(col).interpolate()\n            for col in df.columns\n            if df[col].dtype in [pl.Float32, pl.Float64, pl.Int32, pl.Int64]\n        )\n    else:\n        raise ValueError(f\"Unknown strategy: {strategy}\")\n\n\n3. Create Data Quality Reports\ndef generate_data_report(df: pl.DataFrame) -&gt; dict:\n    \"\"\"Generate comprehensive data quality report.\"\"\"\n    report = {\n        \"shape\": (df.height, df.width),\n        \"schema\": {k: str(v) for k, v in df.schema.items()},\n        \"null_counts\": {col: df[col].null_count() for col in df.columns},\n        \"null_percentages\": {col: df[col].null_count() / df.height \n                            for col in df.columns},\n        \"memory_usage\": df.estimated_size(),\n        \"duplicates\": df.height - df.n_unique(),\n    }\n    return report\n\n# Generate and print report\nreport = generate_data_report(df)\nprint(f\"Data Quality Report:\")\nprint(f\"  Shape: {report['shape']}\")\nprint(f\"  Memory: {report['memory_usage']:,} bytes\")\nprint(f\"  Duplicates: {report['duplicates']}\")\nprint(\"  Null Percentages:\")\nfor col, pct in report['null_percentages'].items():\n    print(f\"    {col}: {pct:.1%}\")"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#practice-exercise",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#practice-exercise",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Practice Exercise",
    "text": "Practice Exercise\nScenario: You’re building a data pipeline for customer information:\nimport polars as pl\n\n# Sample data with quality issues\ncustomers = pl.DataFrame({\n    \"customer_id\": [1, 2, 3, 4, 4, 6],  # Duplicate ID (4)\n    \"name\": [\"Alice\", None, \"Charlie\", \"David\", \"Eve\", \"Frank\"],\n    \"email\": [\"alice@example.com\", \"bob@invalid\", None, \"david@test.com\", \n              \"eve@company.org\", \"frank@example.com\"],\n    \"age\": [25, 150, 30, -5, 35, None],  # Invalid ages (150, -5)\n    \"registration_date\": [\"2023-01-15\", \"invalid_date\", \"2023-03-20\",\n                          None, \"2023-05-10\", \"2023-06-01\"],\n    \"is_premium\": [True, False, True, None, False, True]\n})\nTasks:\n\nDefine a proper schema for this data with appropriate types\nDetect and report all null values\nFix the duplicate customer_id (keep the first occurrence)\nValidate and clean the age column (valid range: 18-100)\nValidate email format for non-null emails\nParse registration_date as Date type, handling errors gracefully\nGenerate a data quality report after cleaning\n\nBonus Challenge:\nCreate a validation pipeline that: - Rejects rows with invalid emails - Fills missing ages with the median age - Drops rows with duplicate IDs (keep first) - Converts registration_date to Date type or null if invalid\n\n\nClick to see solutions\n\nimport polars as pl\nfrom datetime import datetime\n\n# Task 1: Define schema\nexpected_schema = {\n    \"customer_id\": pl.Int64,\n    \"name\": pl.Utf8,\n    \"email\": pl.Utf8,\n    \"age\": pl.Int64,\n    \"registration_date\": pl.Date,\n    \"is_premium\": pl.Boolean\n}\n\n# Task 2: Detect nulls\nprint(\"Null counts:\")\nfor col in customers.columns:\n    null_count = customers[col].null_count()\n    print(f\"  {col}: {null_count}\")\n\n# Task 3: Remove duplicates (keep first)\ncustomers_clean = customers.unique(subset=[\"customer_id\"], keep=\"first\")\n\n# Task 4: Validate and clean age\n# First, see invalid ages\ninvalid_ages = customers_clean.filter(\n    (pl.col(\"age\") &lt; 18) | (pl.col(\"age\") &gt; 100)\n)\nprint(f\"\\nInvalid ages: {invalid_ages.height} rows\")\n\n# Clean ages: set invalid to null, then fill with median\nvalid_ages = customers_clean.with_columns(\n    pl.when((pl.col(\"age\") &gt;= 18) & (pl.col(\"age\") &lt;= 100))\n    .then(pl.col(\"age\"))\n    .otherwise(None)\n    .alias(\"age\")\n)\nmedian_age = valid_ages[\"age\"].median()\ncustomers_clean = valid_ages.with_columns(\n    pl.col(\"age\").fill_null(int(median_age))\n)\n\n# Task 5: Validate email format\nemail_pattern = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\ncustomers_clean = customers_clean.with_columns(\n    pl.when(pl.col(\"email\").is_null())\n    .then(None)\n    .when(pl.col(\"email\").str.contains(email_pattern))\n    .then(pl.col(\"email\"))\n    .otherwise(None)\n    .alias(\"email\")\n)\n\n# Task 6: Parse dates\ncustomers_clean = customers_clean.with_columns(\n    pl.col(\"registration_date\").str.to_date(format=\"%Y-%m-%d\", strict=False)\n)\n\n# Task 7: Generate report\ndef generate_report(df):\n    return {\n        \"shape\": (df.height, df.width),\n        \"null_percentages\": {\n            col: df[col].null_count() / df.height \n            for col in df.columns\n        },\n        \"duplicates\": df.height - df.n_unique()\n    }\n\nreport = generate_report(customers_clean)\nprint(\"\\nFinal Data Quality Report:\")\nprint(f\"  Shape: {report['shape']}\")\nprint(f\"  Duplicates: {report['duplicates']}\")\nprint(\"  Null Percentages:\")\nfor col, pct in report['null_percentages'].items():\n    print(f\"    {col}: {pct:.1%}\")\n\nprint(\"\\nCleaned DataFrame:\")\nprint(customers_clean)\n\n# Bonus: Complete validation pipeline\ndef validate_customers(df: pl.DataFrame) -&gt; pl.DataFrame:\n    # 1. Remove duplicates\n    df = df.unique(subset=[\"customer_id\"], keep=\"first\")\n    \n    # 2. Clean and fill ages\n    df = df.with_columns(\n        pl.when((pl.col(\"age\") &gt;= 18) & (pl.col(\"age\") &lt;= 100))\n        .then(pl.col(\"age\"))\n        .otherwise(None)\n        .alias(\"age\")\n    )\n    median_age = int(df[\"age\"].median())\n    df = df.with_columns(pl.col(\"age\").fill_null(median_age))\n    \n    # 3. Validate emails (set invalid to null)\n    email_pattern = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n    df = df.with_columns(\n        pl.when(pl.col(\"email\").str.contains(email_pattern))\n        .then(pl.col(\"email\"))\n        .otherwise(None)\n    )\n    \n    # 4. Parse dates\n    df = df.with_columns(\n        pl.col(\"registration_date\").str.to_date(format=\"%Y-%m-%d\", strict=False)\n    )\n    \n    # 5. Fill missing names\n    df = df.with_columns(pl.col(\"name\").fill_null(\"Unknown\"))\n    \n    # 6. Fill missing premium status\n    df = df.with_columns(pl.col(\"is_premium\").fill_null(False))\n    \n    return df\n\nfinal_df = validate_customers(customers)\nprint(\"\\nBonus - Validated DataFrame:\")\nprint(final_df)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#resources",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#resources",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Resources",
    "text": "Resources\n\nPolars Null Handling Documentation\nData Types and Schema\nCasting and Type Conversion\nData Validation Best Practices"
  }
]