[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Nomad's Wonderland",
    "section": "",
    "text": "List of projects I worked with."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "June 2024 - Present\n\nBuilding computer vision solutions for edge devices"
  },
  {
    "objectID": "cv.html#experience",
    "href": "cv.html#experience",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "June 2024 - Present\n\nBuilding computer vision solutions for edge devices"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "Education",
    "text": "Education\n\nMaster’s Degree in High Performance Computing, Chalmers University of Technology, Sweden\nBachelor’s Degree in Electrical Engineering, Beijing Normal University, China"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Curriculum Vitae",
    "section": "Skills",
    "text": "Skills\n\nSpecilization: Digital Image Processing, Computer Vision, Object Detection, Object Segmentation, Edge Computing, Efficient ML\nTools: PyTorch/Keras/TensorFlow, TFLite/Micro\nProgramming Languages: Python/C++"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html",
    "href": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html",
    "title": "100 Days of Polars - Day 003: map_elements vs map_batches vs Native Expressions",
    "section": "",
    "text": "Today we’ll explore how to apply custom logic in polars with map_elements when the functionality isn’t available through polars’s built-in expression.\nimport random\nimport string\nimport time\nimport polars as pl\n\ndef random_string(length=8):\n    return ''.join(random.choices(string.ascii_lowercase, k=length))\n\ndf = pl.DataFrame({\n    \"text\": [random_string() for _ in range(1_000_000)]\n})\n\n# Apply a custom function to each element\nt0 = time.time()\nresult = df.with_columns(\n    pl.col(\"text\").map_elements(lambda x: x.upper()).alias(\"uppercase\")\n)\nt1 = time.time()\nprint(f\"map_elements: {t1-t0}\")\nWhen you run this code in polars you will get a warning like this:\n&lt;ipython-input-51-4b0fe28e2086&gt;:9: PolarsInefficientMapWarning: \nExpr.map_elements is significantly slower than the native expressions API.\nOnly use if you absolutely CANNOT implement your logic otherwise.\nReplace this expression...\n  - pl.col(\"text\").map_elements(lambda x: ...)\nwith this one instead:\n  + pl.col(\"text\").str.to_uppercase()\n\n  pl.col(\"text\").map_elements(lambda x: x.upper()).alias(\"uppercase\")\nThe reason for this warning is unlike native polars expressions, map_elements are not parallelized and usually involves data copy betten polars’s rust engine and python interpreter. When we use the suggested code, time performance increased more than 3x for this small dataset and simple task.\nt0 = time.time()\nresult = df.with_columns(\n    pl.col(\"text\").str.to_uppercase().alias(\"uppercase\")\n)\nt1 = time.time()\nprint(f\"native expression: {t1-t0}\")"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html#introduction",
    "href": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html#introduction",
    "title": "100 Days of Polars - Day 003: map_elements vs map_batches vs Native Expressions",
    "section": "",
    "text": "Today we’ll explore how to apply custom logic in polars with map_elements when the functionality isn’t available through polars’s built-in expression.\nimport random\nimport string\nimport time\nimport polars as pl\n\ndef random_string(length=8):\n    return ''.join(random.choices(string.ascii_lowercase, k=length))\n\ndf = pl.DataFrame({\n    \"text\": [random_string() for _ in range(1_000_000)]\n})\n\n# Apply a custom function to each element\nt0 = time.time()\nresult = df.with_columns(\n    pl.col(\"text\").map_elements(lambda x: x.upper()).alias(\"uppercase\")\n)\nt1 = time.time()\nprint(f\"map_elements: {t1-t0}\")\nWhen you run this code in polars you will get a warning like this:\n&lt;ipython-input-51-4b0fe28e2086&gt;:9: PolarsInefficientMapWarning: \nExpr.map_elements is significantly slower than the native expressions API.\nOnly use if you absolutely CANNOT implement your logic otherwise.\nReplace this expression...\n  - pl.col(\"text\").map_elements(lambda x: ...)\nwith this one instead:\n  + pl.col(\"text\").str.to_uppercase()\n\n  pl.col(\"text\").map_elements(lambda x: x.upper()).alias(\"uppercase\")\nThe reason for this warning is unlike native polars expressions, map_elements are not parallelized and usually involves data copy betten polars’s rust engine and python interpreter. When we use the suggested code, time performance increased more than 3x for this small dataset and simple task.\nt0 = time.time()\nresult = df.with_columns(\n    pl.col(\"text\").str.to_uppercase().alias(\"uppercase\")\n)\nt1 = time.time()\nprint(f\"native expression: {t1-t0}\")"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html#map_batches",
    "href": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html#map_batches",
    "title": "100 Days of Polars - Day 003: map_elements vs map_batches vs Native Expressions",
    "section": "map_batches",
    "text": "map_batches\nmap_batches is another tool that applies a Python function to batches (chunks) of data in polars. It’s a middle group beteewn map_elements and native expression. It also allows using numpy/other libraries on chunks, so it might be much faster than native expression when these library are highly optimized.\nLet’s look at an example below:\nimport polars as pl\nimport numpy as np\nimport time\n\nn_rows = 1_000_000\nn_groups = 4\nunique_groups = [f\"grp_{i}\" for i in range(n_groups)]\ndf = pl.DataFrame({\n    \"value\": np.random.randn(n_rows),\n    \"group\": np.random.choice(unique_groups, n_rows)\n})\n\n\ngroup_stats = df.group_by(\"group\").agg([\n    pl.col(\"value\").mean().alias(\"mean\"),\n    pl.col(\"value\").std().alias(\"std\")\n])\n# Method 1: map_elements (SLOW)\nstart = time.time()\nresult = df.join(group_stats, on=\"group\").with_columns(\n    pl.struct([\"value\", \"mean\", \"std\"])\n    .map_elements(\n        lambda row: (row[\"value\"] - row[\"mean\"]) / row[\"std\"],\n        return_dtype=pl.Float64\n    )\n    .alias(\"z_score\")\n)\ntotal = time.time() - start\nprint(f\"map_elements (1M rows):     {total:.4f} seconds\")\n\n# Method 2: Native expression (FAST)\nstart = time.time()\nresult = df.with_columns(\n    ((pl.col(\"value\") - pl.col(\"value\").mean().over(\"group\")) / \n     pl.col(\"value\").std().over(\"group\"))\n    .alias(\"z_score\")\n)\ntotal = time.time() - start\nprint(f\"Native expression (1M rows): {total:.4f} seconds\")\n\n# Method 3: map_batches (MIDDLE GROUND)\ndef normalize_batch(series: pl.Series) -&gt; pl.Series:\n    # Using numpy for vectorized operations on the batch\n    arr = series.to_numpy()\n    return pl.Series((arr - arr.mean()) / arr.std())\n\nstart = time.time()\nresult = df.with_columns(\n    pl.col(\"value\").map_batches(normalize_batch).over(\"group\").alias(\"z_score\")\n)\ntotal = time.time() - start\nprint(f\"map_batches (1M rows): {total:.4f} seconds\")\nWhen you look at the result below, you will find out map_batches is faster than native expression and map_elements is the slowest.\nmap_elements (1M rows):     0.3763 seconds\nNative expression (1M rows): 0.0362 seconds\nmap_batches (1M rows): 0.0299 seconds\nBut if we increase the n_groups to 10000, we will get result below:\nmap_elements (1M rows):     0.6927 seconds\nNative expression (1M rows): 0.0621 seconds\nmap_batches (1M rows): 0.4327 seconds\nThe result reverted, the number of groups decies how many context switches polars has to perform from rust engine to numpy. The more complicate the logic is, the more benefits native expression will bring."
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html#resources",
    "href": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html#resources",
    "title": "100 Days of Polars - Day 003: map_elements vs map_batches vs Native Expressions",
    "section": "Resources",
    "text": "Resources\n\nPolars map_elements Documentation\nPolars map_batches Documentation\nPolars Expression API"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-29-day005-structs.html",
    "href": "posts/100-days-of-polars/2026-01-29-day005-structs.html",
    "title": "100 Days of Polars - Day 005: Working with Structs - Nested Data Structures",
    "section": "",
    "text": "Today we’ll explore structs in Polars, which allow you to store multiple fields as a single column. Think of structs as nested dictionaries or JSON objects within your DataFrame. They’re perfect for representing complex, hierarchical data while keeping your DataFrame organized.\nimport polars as pl\n\n# Create a DataFrame with a struct column\ndf = pl.DataFrame({\n    \"user_id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"address\": [\n        {\"street\": \"123 Main St\", \"city\": \"NYC\", \"zip\": \"10001\"},\n        {\"street\": \"456 Oak Ave\", \"city\": \"LA\", \"zip\": \"90001\"},\n        {\"street\": \"789 Pine Rd\", \"city\": \"Chicago\", \"zip\": \"60601\"}\n    ]\n})\n\nprint(df)\nshape: (3, 3)\n┌─────────┬─────────┬─────────────────────────────┐\n│ user_id ┆ name    ┆ address                     │\n│ ---     ┆ ---     ┆ ---                         │\n│ i64     ┆ str     ┆ struct[3]                   │\n╞═════════╪═════════╪═════════════════════════════╡\n│ 1       ┆ Alice   ┆ {123 Main St,NYC,10001}     │\n│ 2       ┆ Bob     ┆ {456 Oak Ave,LA,90001}      │\n│ 3       ┆ Charlie ┆ {789 Pine Rd,Chicago,60601} │\n└─────────┴─────────┴─────────────────────────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-29-day005-structs.html#introduction",
    "href": "posts/100-days-of-polars/2026-01-29-day005-structs.html#introduction",
    "title": "100 Days of Polars - Day 005: Working with Structs - Nested Data Structures",
    "section": "",
    "text": "Today we’ll explore structs in Polars, which allow you to store multiple fields as a single column. Think of structs as nested dictionaries or JSON objects within your DataFrame. They’re perfect for representing complex, hierarchical data while keeping your DataFrame organized.\nimport polars as pl\n\n# Create a DataFrame with a struct column\ndf = pl.DataFrame({\n    \"user_id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"address\": [\n        {\"street\": \"123 Main St\", \"city\": \"NYC\", \"zip\": \"10001\"},\n        {\"street\": \"456 Oak Ave\", \"city\": \"LA\", \"zip\": \"90001\"},\n        {\"street\": \"789 Pine Rd\", \"city\": \"Chicago\", \"zip\": \"60601\"}\n    ]\n})\n\nprint(df)\nshape: (3, 3)\n┌─────────┬─────────┬─────────────────────────────┐\n│ user_id ┆ name    ┆ address                     │\n│ ---     ┆ ---     ┆ ---                         │\n│ i64     ┆ str     ┆ struct[3]                   │\n╞═════════╪═════════╪═════════════════════════════╡\n│ 1       ┆ Alice   ┆ {123 Main St,NYC,10001}     │\n│ 2       ┆ Bob     ┆ {456 Oak Ave,LA,90001}      │\n│ 3       ┆ Charlie ┆ {789 Pine Rd,Chicago,60601} │\n└─────────┴─────────┴─────────────────────────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-29-day005-structs.html#creating-structs",
    "href": "posts/100-days-of-polars/2026-01-29-day005-structs.html#creating-structs",
    "title": "100 Days of Polars - Day 005: Working with Structs - Nested Data Structures",
    "section": "Creating Structs",
    "text": "Creating Structs\n\nFrom Python Dictionaries\nThe simplest way to create structs is from Python dictionaries:\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"person\": [\n        {\"name\": \"Alice\", \"age\": 30},\n        {\"name\": \"Bob\", \"age\": 25}\n    ]\n})\n\n\nUsing pl.struct()\nYou can create struct columns from existing columns:\ndf = pl.DataFrame({\n    \"first_name\": [\"Alice\", \"Bob\"],\n    \"last_name\": [\"Smith\", \"Jones\"],\n    \"age\": [30, 25]\n})\n\n# Combine columns into a struct\ndf_with_struct = df.select([\n    pl.struct([\"first_name\", \"last_name\", \"age\"]).alias(\"person\")\n])\nprint(df_with_struct)\nshape: (2, 1)\n┌─────────────────────────┐\n│ person                  │\n│ ---                     │\n│ struct[3]               │\n╞═════════════════════════╡\n│ {Alice,Smith,30}        │\n│ {Bob,Jones,25}          │\n└─────────────────────────┘\nYou can also create named structs with different field names:\ndf_renamed = df.select([\n    pl.struct(\n        first=pl.col(\"first_name\"),\n        last=pl.col(\"last_name\"),\n        years=pl.col(\"age\")\n    ).alias(\"person\")\n])"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-29-day005-structs.html#accessing-struct-fields",
    "href": "posts/100-days-of-polars/2026-01-29-day005-structs.html#accessing-struct-fields",
    "title": "100 Days of Polars - Day 005: Working with Structs - Nested Data Structures",
    "section": "Accessing Struct Fields",
    "text": "Accessing Struct Fields\n\nUsing .field()\nAccess individual fields from a struct column:\ndf = pl.DataFrame({\n    \"id\": [1, 2, 3],\n    \"address\": [\n        {\"street\": \"123 Main St\", \"city\": \"NYC\"},\n        {\"street\": \"456 Oak Ave\", \"city\": \"LA\"},\n        {\"street\": \"789 Pine Rd\", \"city\": \"Chicago\"}\n    ]\n})\n\n# Extract specific fields\ndf_with_fields = df.with_columns([\n    pl.col(\"address\").struct.field(\"city\").alias(\"city\"),\n    pl.col(\"address\").struct.field(\"street\").alias(\"street\")\n])\nprint(df_with_fields)\nshape: (3, 4)\n┌─────┬──────────────────────────┬─────────┬──────────────┐\n│ id  ┆ address                  ┆ city    ┆ street       │\n│ --- ┆ ---                      ┆ ---     ┆ ---          │\n│ i64 ┆ struct[2]                ┆ str     ┆ str          │\n╞═════╪══════════════════════════╪═════════╪══════════════╡\n│ 1   ┆ {123 Main St,NYC}        ┆ NYC     ┆ 123 Main St  │\n│ 2   ┆ {456 Oak Ave,LA}         ┆ LA      ┆ 456 Oak Ave  │\n│ 3   ┆ {789 Pine Rd,Chicago}    ┆ Chicago ┆ 789 Pine Rd  │\n└─────┴──────────────────────────┴─────────┴──────────────┘\n\n\nUsing .struct.unnest()\nUnnest a struct to expand all fields into separate columns:\ndf_unnested = df.unnest(\"address\")\nprint(df_unnested)\nshape: (3, 3)\n┌─────┬──────────────┬─────────┐\n│ id  ┆ street       ┆ city    │\n│ --- ┆ ---          ┆ ---     │\n│ i64 ┆ str          ┆ str     │\n╞═════╪══════════════╪═════════╡\n│ 1   ┆ 123 Main St  ┆ NYC     │\n│ 2   ┆ 456 Oak Ave  ┆ LA      │\n│ 3   ┆ 789 Pine Rd  ┆ Chicago │\n└─────┴──────────────┴─────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-29-day005-structs.html#renaming-struct-fields",
    "href": "posts/100-days-of-polars/2026-01-29-day005-structs.html#renaming-struct-fields",
    "title": "100 Days of Polars - Day 005: Working with Structs - Nested Data Structures",
    "section": "Renaming Struct Fields",
    "text": "Renaming Struct Fields\nYou can rename fields within a struct:\ndf = pl.DataFrame({\n    \"person\": [\n        {\"first\": \"Alice\", \"last\": \"Smith\"},\n        {\"first\": \"Bob\", \"last\": \"Jones\"}\n    ]\n})\n\n# Rename struct fields\ndf_renamed = df.with_columns(\n    pl.col(\"person\").struct.rename_fields([\"first_name\", \"last_name\"])\n)\nprint(df_renamed)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-29-day005-structs.html#working-with-nested-structs",
    "href": "posts/100-days-of-polars/2026-01-29-day005-structs.html#working-with-nested-structs",
    "title": "100 Days of Polars - Day 005: Working with Structs - Nested Data Structures",
    "section": "Working with Nested Structs",
    "text": "Working with Nested Structs\nStructs can contain other structs, creating deeply nested data:\ndf = pl.DataFrame({\n    \"user_id\": [1, 2],\n    \"profile\": [\n        {\n            \"name\": {\"first\": \"Alice\", \"last\": \"Smith\"},\n            \"contact\": {\"email\": \"alice@example.com\", \"phone\": \"555-0001\"}\n        },\n        {\n            \"name\": {\"first\": \"Bob\", \"last\": \"Jones\"},\n            \"contact\": {\"email\": \"bob@example.com\", \"phone\": \"555-0002\"}\n        }\n    ]\n})\n\n# Access nested fields\ndf_with_nested = df.with_columns([\n    pl.col(\"profile\").struct.field(\"name\").struct.field(\"first\").alias(\"first_name\"),\n    pl.col(\"profile\").struct.field(\"contact\").struct.field(\"email\").alias(\"email\")\n])\nprint(df_with_nested)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-29-day005-structs.html#json-to-struct",
    "href": "posts/100-days-of-polars/2026-01-29-day005-structs.html#json-to-struct",
    "title": "100 Days of Polars - Day 005: Working with Structs - Nested Data Structures",
    "section": "JSON to Struct",
    "text": "JSON to Struct\nPolars can parse JSON strings into structs:\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"json_data\": [\n        '{\"name\": \"Alice\", \"age\": 30, \"city\": \"NYC\"}',\n        '{\"name\": \"Bob\", \"age\": 25, \"city\": \"LA\"}'\n    ]\n})\n\n# Parse JSON strings to structs\ndf_parsed = df.with_columns(\n    pl.col(\"json_data\").str.json_decode(pl.Struct([\n            pl.Field(\"name\", pl.Utf8),\n            pl.Field(\"age\", pl.Int64),\n            pl.Field(\"city\", pl.Utf8)\n        ])).alias(\"parsed\")\n)\nprint(df_parsed)\n\n# Extract specific fields from parsed JSON\ndf_extracted = df_parsed.with_columns([\n    pl.col(\"parsed\").struct.field(\"name\").alias(\"name\"),\n    pl.col(\"parsed\").struct.field(\"age\").alias(\"age\")\n])\nprint(df_extracted)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-29-day005-structs.html#struct.with_fields",
    "href": "posts/100-days-of-polars/2026-01-29-day005-structs.html#struct.with_fields",
    "title": "100 Days of Polars - Day 005: Working with Structs - Nested Data Structures",
    "section": ".struct.with_fields()",
    "text": ".struct.with_fields()\nimport polars as pl\n\ndf = pl.DataFrame({\n    \"user_data\": [\n        {\"name\": \"alice\", \"score\": 85, \"active\": True},\n        {\"name\": \"bob\", \"score\": 42, \"active\": False}\n    ]\n})\n\n# Update score and add a new category field inside the struct\ndf_updated = df.with_columns(\n    pl.col(\"user_data\").struct.with_fields(\n        # 1. Update an existing field\n        pl.field(\"score\") + 5,\n        \n        # 2. Add a brand new field inside the struct\n        is_passing = pl.field(\"score\") &gt;= 50,\n        \n        # 3. Use logic from one field to update another\n        active = pl.when(pl.field(\"score\") &gt; 80).then(True).otherwise(pl.field(\"active\"))\n    )\n)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-29-day005-structs.html#combining-structs-and-lists",
    "href": "posts/100-days-of-polars/2026-01-29-day005-structs.html#combining-structs-and-lists",
    "title": "100 Days of Polars - Day 005: Working with Structs - Nested Data Structures",
    "section": "Combining Structs and Lists",
    "text": "Combining Structs and Lists\nYou can have lists of structs or structs containing lists:\n# List of structs\ndf_list_of_structs = pl.DataFrame({\n    \"user_id\": [1, 2],\n    \"orders\": [\n        [\n            {\"order_id\": 101, \"amount\": 50.0},\n            {\"order_id\": 102, \"amount\": 75.0}\n        ],\n        [\n            {\"order_id\": 201, \"amount\": 100.0}\n        ]\n    ]\n})\n\n# Explode the list and access struct fields\ndf_exploded = df_list_of_structs.explode(\"orders\").with_columns([\n    pl.col(\"orders\").struct.field(\"order_id\").alias(\"order_id\"),\n    pl.col(\"orders\").struct.field(\"amount\").alias(\"amount\")\n])\nprint(df_exploded)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-29-day005-structs.html#practice-exercise",
    "href": "posts/100-days-of-polars/2026-01-29-day005-structs.html#practice-exercise",
    "title": "100 Days of Polars - Day 005: Working with Structs - Nested Data Structures",
    "section": "Practice Exercise",
    "text": "Practice Exercise\nNow it’s time to practice! Try solving this exercise using struct operations:\nScenario: You’re analyzing customer data from an e-commerce API that returns nested JSON data:\nimport polars as pl\n\ncustomers = pl.DataFrame({\n    \"customer_id\": [1, 2, 3],\n    \"profile\": [\n        {\n            \"name\": {\"first\": \"Alice\", \"last\": \"Johnson\"},\n            \"age\": 28,\n            \"address\": {\"city\": \"NYC\", \"state\": \"NY\", \"zip\": \"10001\"}\n        },\n        {\n            \"name\": {\"first\": \"Bob\", \"last\": \"Smith\"},\n            \"age\": 35,\n            \"address\": {\"city\": \"Los Angeles\", \"state\": \"CA\", \"zip\": \"90001\"}\n        },\n        {\n            \"name\": {\"first\": \"Charlie\", \"last\": \"Brown\"},\n            \"age\": 42,\n            \"address\": {\"city\": \"Chicago\", \"state\": \"IL\", \"zip\": \"60601\"}\n        }\n    ],\n    \"orders\": [\n        [{\"order_id\": 101, \"total\": 150.50}, {\"order_id\": 102, \"total\": 200.00}],\n        [{\"order_id\": 201, \"total\": 89.99}],\n        [{\"order_id\": 301, \"total\": 450.00}, {\"order_id\": 302, \"total\": 125.75}, {\"order_id\": 303, \"total\": 75.00}]\n    ]\n})\nTasks:\n\nExtract the first name and city from each customer into separate columns\nCreate a new column full_name by combining first and last names\nUnnest the address information into separate columns (city, state, zip)\nFilter customers who are older than 30 years\nExplode the orders list and calculate the total amount spent per customer\n\nBonus Challenge: Create a new struct column called customer_summary that contains: - full_name (first + last) - location (city, state) - order_count (number of orders) - total_spent (sum of all order totals)\n\n\nClick to see solutions\n\n# Task 1: Extract first name and city\ntask1 = customers.with_columns([\n    pl.col(\"profile\").struct.field(\"name\").struct.field(\"first\").alias(\"first_name\"),\n    pl.col(\"profile\").struct.field(\"address\").struct.field(\"city\").alias(\"city\")\n])\nprint(task1)\n\n# Task 2: Create full_name column\ntask2 = customers.with_columns(\n    (pl.col(\"profile\").struct.field(\"name\").struct.field(\"first\") + \" \" +\n     pl.col(\"profile\").struct.field(\"name\").struct.field(\"last\")).alias(\"full_name\")\n)\nprint(task2)\n\n# Task 3: Unnest address information\ntask3 = customers.with_columns(\n    pl.col(\"profile\").struct.field(\"address\").alias(\"address\")\n).unnest(\"address\")\nprint(task3)\n\n# Task 4: Filter customers older than 30\ntask4 = customers.filter(\n    pl.col(\"profile\").struct.field(\"age\") &gt; 30\n)\nprint(task4)\n\n# Task 5: Explode orders and calculate total spent per customer\ntask5 = customers.explode(\"orders\").with_columns([\n    pl.col(\"orders\").struct.field(\"order_id\").alias(\"order_id\"),\n    pl.col(\"orders\").struct.field(\"total\").alias(\"order_total\")\n]).group_by(\"customer_id\").agg([\n    pl.col(\"order_total\").sum().alias(\"total_spent\"),\n    pl.col(\"order_id\").count().alias(\"order_count\")\n])\nprint(task5)\n\n# Bonus: Create customer_summary struct\nbonus = customers.explode(\"orders\").with_columns([\n    pl.col(\"orders\").struct.field(\"total\").alias(\"order_total\")\n]).group_by(\"customer_id\").agg([\n    pl.col(\"profile\").first().alias(\"profile\"),\n    pl.col(\"order_total\").sum().alias(\"total_spent\"),\n    pl.col(\"order_total\").count().alias(\"order_count\")\n]).with_columns(\n    pl.struct([\n        (pl.col(\"profile\").struct.field(\"name\").struct.field(\"first\") + \" \" +\n         pl.col(\"profile\").struct.field(\"name\").struct.field(\"last\")).alias(\"full_name\"),\n        (pl.col(\"profile\").struct.field(\"address\").struct.field(\"city\") + \", \" +\n         pl.col(\"profile\").struct.field(\"address\").struct.field(\"state\")).alias(\"location\"),\n        pl.col(\"order_count\"),\n        pl.col(\"total_spent\")\n    ]).alias(\"customer_summary\")\n)\nprint(bonus.select([\"customer_id\", \"customer_summary\"]))\nExpected output for Task 5:\nshape: (3, 3)\n┌─────────────┬─────────────┬─────────────┐\n│ customer_id ┆ total_spent ┆ order_count │\n│ ---         ┆ ---         ┆ ---         │\n│ i64         ┆ f64         ┆ u32         │\n╞═════════════╪═════════════╪═════════════╡\n│ 1           ┆ 350.5       ┆ 2           │\n│ 2           ┆ 89.99       ┆ 1           │\n│ 3           ┆ 650.75      ┆ 3           │\n└─────────────┴─────────────┴─────────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-29-day005-structs.html#resources",
    "href": "posts/100-days-of-polars/2026-01-29-day005-structs.html#resources",
    "title": "100 Days of Polars - Day 005: Working with Structs - Nested Data Structures",
    "section": "Resources",
    "text": "Resources\n\nPolars Struct Operations Documentation\nWorking with Nested Data Types\nJSON Parsing in Polars"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "",
    "text": "Today we’ll explore how to work with list columns in Polars. Lists are a powerful feature that allow you to store multiple values in a single cell, making it easy to represent nested or hierarchical data. We’ll cover explode to flatten lists, implode to create lists, and various list operations.\nimport polars as pl\n\n# Create a DataFrame with a list column\ndf = pl.DataFrame({\n    \"user_id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"purchases\": [\n        [\"laptop\", \"mouse\", \"keyboard\"],\n        [\"phone\"],\n        [\"tablet\", \"headphones\"]\n    ]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n})\n\nprint(df)\nshape: (3, 3)\n┌─────────┬─────────┬──────────────────────────────┐\n│ user_id ┆ name    ┆ purchases                    │\n│ ---     ┆ ---     ┆ ---                          │\n│ i64     ┆ str     ┆ list[str]                    │\n╞═════════╪═════════╪══════════════════════════════╡\n│ 1       ┆ Alice   ┆ [\"laptop\", \"mouse\", \"keyb... │\n│ 2       ┆ Bob     ┆ [\"phone\"]                    │\n│ 3       ┆ Charlie ┆ [\"tablet\", \"headphones\"]     │\n└─────────┴─────────┴──────────────────────────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#introduction",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#introduction",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "",
    "text": "Today we’ll explore how to work with list columns in Polars. Lists are a powerful feature that allow you to store multiple values in a single cell, making it easy to represent nested or hierarchical data. We’ll cover explode to flatten lists, implode to create lists, and various list operations.\nimport polars as pl\n\n# Create a DataFrame with a list column\ndf = pl.DataFrame({\n    \"user_id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"purchases\": [\n        [\"laptop\", \"mouse\", \"keyboard\"],\n        [\"phone\"],\n        [\"tablet\", \"headphones\"]\n    ]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n})\n\nprint(df)\nshape: (3, 3)\n┌─────────┬─────────┬──────────────────────────────┐\n│ user_id ┆ name    ┆ purchases                    │\n│ ---     ┆ ---     ┆ ---                          │\n│ i64     ┆ str     ┆ list[str]                    │\n╞═════════╪═════════╪══════════════════════════════╡\n│ 1       ┆ Alice   ┆ [\"laptop\", \"mouse\", \"keyb... │\n│ 2       ┆ Bob     ┆ [\"phone\"]                    │\n│ 3       ┆ Charlie ┆ [\"tablet\", \"headphones\"]     │\n└─────────┴─────────┴──────────────────────────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#exploding-lists",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#exploding-lists",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "Exploding Lists",
    "text": "Exploding Lists\nThe explode method transforms each element of a list into a separate row, duplicating the other column values. This is useful when you need to analyze individual list elements.\n# Explode the purchases column\nexploded_df = df.explode(\"purchases\")\nprint(exploded_df)\nshape: (6, 3)\n┌─────────┬─────────┬───────────┐\n│ user_id ┆ name    ┆ purchases │\n│ ---     ┆ ---     ┆ ---       │\n│ i64     ┆ str     ┆ str       │\n╞═════════╪═════════╪═══════════╡\n│ 1       ┆ Alice   ┆ laptop    │\n│ 1       ┆ Alice   ┆ mouse     │\n│ 1       ┆ Alice   ┆ keyboard  │\n│ 2       ┆ Bob     ┆ phone     │\n│ 3       ┆ Charlie ┆ tablet    │\n│ 3       ┆ Charlie ┆ headphones│\n└─────────┴─────────┴───────────┘\nYou can explode multiple columns simultaneously if they have the same length:\ndf_multi = pl.DataFrame({\n    \"id\": [1, 2],\n    \"values_a\": [[1, 2, 3], [4, 5]],\n    \"values_b\": [[\"a\", \"b\", \"c\"], [\"d\", \"e\"]]\n})\n\n# Explode both list columns\nexploded_multi = df_multi.explode([\"values_a\", \"values_b\"])\nprint(exploded_multi)\nshape: (5, 3)\n┌─────┬──────────┬──────────┐\n│ id  ┆ values_a ┆ values_b │\n│ --- ┆ ---      ┆ ---      │\n│ i64 ┆ i64      ┆ str      │\n╞═════╪══════════╪══════════╡\n│ 1   ┆ 1        ┆ a        │\n│ 1   ┆ 2        ┆ b        │\n│ 1   ┆ 3        ┆ c        │\n│ 2   ┆ 4        ┆ d        │\n│ 2   ┆ 5        ┆ e        │\n└─────┴──────────┴──────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#creating-lists-with-implode",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#creating-lists-with-implode",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "Creating Lists with implode",
    "text": "Creating Lists with implode\nThe implode method (also known as list in aggregation context) groups values back into lists. This is the inverse operation of explode. implode is called automatically in group_by().agg() operations.\n# Create individual purchase records\npurchases_df = pl.DataFrame({\n    \"user_id\": [1, 1, 1, 2, 3, 3],\n    \"item\": [\"laptop\", \"mouse\", \"keyboard\", \"phone\", \"tablet\", \"headphones\"],\n    \"price\": [1200, 25, 75, 800, 600, 150]\n})\n\n# Group items back into lists per user\ngrouped_df = purchases_df.group_by(\"user_id\").agg([\n    pl.col(\"item\").alias(\"items\"),\n    pl.col(\"price\").sum().alias(\"total_spent\")\n])\n\nprint(grouped_df)\nshape: (3, 3)\n┌─────────┬──────────────────────────────┬─────────────┐\n│ user_id ┆ items                        ┆ total_spent │\n│ ---     ┆ ---                          ┆ ---         │\n│ i64     ┆ list[str]                    ┆ i64         │\n╞═════════╪══════════════════════════════╪═════════════╡\n│ 1       ┆ [\"laptop\", \"mouse\", \"keyb... ┆ 1300        │\n│ 2       ┆ [\"phone\"]                    ┆ 800         │\n│ 3       ┆ [\"tablet\", \"headphones\"]     ┆ 750         │\n└─────────┴──────────────────────────────┴─────────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#list-operations",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#list-operations",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "List Operations",
    "text": "List Operations\nPolars provides a rich set of operations for working with list columns through the .list namespace.\n\nGetting List Length\ndf_with_length = df.with_columns(\n    pl.col(\"purchases\").list.len().alias(\"num_purchases\")\n)\nprint(df_with_length)\nshape: (3, 4)\n┌─────────┬─────────┬──────────────────────────────┬───────────────┐\n│ user_id ┆ name    ┆ purchases                    ┆ num_purchases │\n│ ---     ┆ ---     ┆ ---                          ┆ ---           │\n│ i64     ┆ str     ┆ list[str]                    ┆ u32           │\n╞═════════╪═════════╪══════════════════════════════╪═══════════════╡\n│ 1       ┆ Alice   ┆ [\"laptop\", \"mouse\", \"keyb... ┆ 3             │\n│ 2       ┆ Bob     ┆ [\"phone\"]                    ┆ 1             │\n│ 3       ┆ Charlie ┆ [\"tablet\", \"headphones\"]     ┆ 2             │\n└─────────┴─────────┴──────────────────────────────┴───────────────┘\n\n\nAccessing List Elements\n# Get the first item from each list\ndf_first = df.with_columns(\n    pl.col(\"purchases\").list.first().alias(\"first_purchase\")\n)\nprint(df_first)\n\n# Get the last item\ndf_last = df.with_columns(\n    pl.col(\"purchases\").list.last().alias(\"last_purchase\")\n)\n\n# Get item at specific index\ndf_indexed = df.with_columns(\n    pl.col(\"purchases\").list.get(1).alias(\"second_purchase\")\n)\nprint(df_indexed)\n\n\nSlicing Lists\n# Get first 2 items from each list\ndf_sliced = df.with_columns(\n    pl.col(\"purchases\").list.head(2).alias(\"first_two\")\n)\nprint(df_sliced)\n\n# Get all items except the first\ndf_tail = df.with_columns(\n    pl.col(\"purchases\").list.tail(-1).alias(\"rest\")\n)\n\n\nFiltering and Transforming Lists\n# Check if lists contain specific values\ndf_contains = df.with_columns(\n    pl.col(\"purchases\").list.contains(\"laptop\").alias(\"bought_laptop\")\n)\nprint(df_contains)\n\n# Apply expressions to list elements\ndf_numbers = pl.DataFrame({\n    \"id\": [1, 2, 3],\n    \"values\": [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n})\n\n# Square all values in each list\ndf_squared = df_numbers.with_columns(\n    pl.col(\"values\").list.eval(pl.element() * 2).alias(\"doubled\")\n)\nprint(df_squared)\n\n\nConcatenating Lists\ndf_concat = pl.DataFrame({\n    \"list1\": [[1, 2], [3, 4]],\n    \"list2\": [[5, 6], [7, 8]]\n})\n\n# Concatenate two list columns\ndf_merged = df_concat.with_columns(\n    pl.concat_list([\"list1\", \"list2\"]).alias(\"combined\")\n)\nprint(df_merged)\nshape: (2, 3)\n┌───────────┬───────────┬──────────────────┐\n│ list1     ┆ list2     ┆ combined         │\n│ ---       ┆ ---       ┆ ---              │\n│ list[i64] ┆ list[i64] ┆ list[i64]        │\n╞═══════════╪═══════════╪══════════════════╡\n│ [1, 2]    ┆ [5, 6]    ┆ [1, 2, 5, 6]     │\n│ [3, 4]    ┆ [7, 8]    ┆ [3, 4, 7, 8]     │\n└───────────┴───────────┴──────────────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#performance-considerations",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#performance-considerations",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "Performance Considerations",
    "text": "Performance Considerations\nWorking with list columns is generally efficient in Polars, but keep these tips in mind:\n\nUse native list operations instead of exploding and re-aggregating when possible\nExplode strategically - only when you need row-level operations\nList operations are vectorized - they’re much faster than using map_elements\nConsider Arrow’s memory layout - lists are stored efficiently in memory\n\nimport time\n\n# Create a larger dataset\nn = 100_000\nlarge_df = pl.DataFrame({\n    \"id\": range(n),\n    \"values\": [[i, i+1, i+2] for i in range(n)]\n})\n\n# Method 1: Using native list operations (FAST)\nstart = time.time()\nresult1 = large_df.with_columns(\n    pl.col(\"values\").list.sum().alias(\"sum\")\n)\ntime1 = time.time() - start\nprint(f\"Native list.sum(): {time1:.4f} seconds\")\n\n# Method 2: Explode and aggregate (SLOWER)\nstart = time.time()\nresult2 = large_df.explode(\"values\").group_by(\"id\").agg(\n    pl.col(\"values\").sum().alias(\"sum\")\n)\ntime2 = time.time() - start\nprint(f\"Explode + aggregate: {time2:.4f} seconds\")\n\nprint(f\"Native is {time2/time1:.1f}x faster\")"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#practice-exercise",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#practice-exercise",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "Practice Exercise",
    "text": "Practice Exercise\nNow it’s time to practice! Try solving this exercise using list operations:\nScenario: You’re analyzing e-commerce data with order information:\nimport polars as pl\n\n# Create sample e-commerce data\norders = pl.DataFrame({\n    \"order_id\": [1, 2, 3, 4],\n    \"customer_id\": [101, 102, 101, 103],\n    \"items\": [\n        [\"laptop\", \"mouse\"],\n        [\"phone\", \"case\", \"charger\"],\n        [\"keyboard\", \"monitor\"],\n        [\"tablet\"]\n    ],\n    \"prices\": [\n        [1200, 25],\n        [800, 15, 30],\n        [75, 300],\n        [600]\n    ]\n})\nTasks:\n\nAdd a column item_count that shows the number of items in each order\nExplode the data to create one row per item (hint: explode both items and prices columns)\nFilter to find only orders that have multiple items\nAdd a column max_price that shows the most expensive item in each order\nCreate a customer purchase history showing all items purchased, total amount spent, and number of unique orders per customer\n\nBonus Challenge: For each customer, create a new column that shows only items that cost more than $100. Use list operations to filter the prices and corresponding items lists.\n\n\nClick to see solutions\n\n# Task 1: Total items per order\nitems_per_order = orders.with_columns(\n    pl.col(\"items\").list.len().alias(\"item_count\")\n)\nprint(items_per_order)\n\n# Task 2: Explode to analyze individual items\nexploded_orders = orders.explode([\"items\", \"prices\"])\nprint(exploded_orders)\n\n# Task 3: Find customers who bought multiple items in an order\nmulti_item_orders = orders.filter(\n    pl.col(\"items\").list.len() &gt; 1\n)\nprint(multi_item_orders)\n\n# Task 4: Get most expensive item per order\nmost_expensive = orders.with_columns(\n    pl.col(\"prices\").list.max().alias(\"max_price\")\n)\nprint(most_expensive)\n\n# Task 5: Customer purchase history\ncustomer_history = exploded_orders.group_by(\"customer_id\").agg([\n    pl.col(\"items\").alias(\"all_items\"),\n    pl.col(\"prices\").sum().alias(\"total_spent\"),\n    pl.col(\"order_id\").n_unique().alias(\"num_orders\")\n])\nprint(customer_history)\n\n# Bonus: Filter expensive items per customer\nexpensive_items = orders.with_columns(\n    pl.col(\"items\").list.gather(\n        pl.col(\"prices\").list.eval(pl.arg_where(pl.element() &gt; 100))\n    ).alias(\"expensive_items\"),\n    \n    pl.col(\"prices\").list.gather(\n        pl.col(\"prices\").list.eval(pl.arg_where(pl.element() &gt; 100))\n    ).alias(\"expensive_prices\")\n)\nExpected output for Task 5:\nshape: (3, 4)\n┌─────────────┬──────────────────────────────┬─────────────┬────────────┐\n│ customer_id ┆ all_items                    ┆ total_spent ┆ num_orders │\n│ ---         ┆ ---                          ┆ ---         ┆ ---        │\n│ i64         ┆ list[str]                    ┆ i64         ┆ u32        │\n╞═════════════╪══════════════════════════════╪═════════════╪════════════╡\n│ 101         ┆ [\"laptop\", \"mouse\", \"keyb... ┆ 1600        ┆ 2          │\n│ 102         ┆ [\"phone\", \"case\", \"charger\"] ┆ 845         ┆ 1          │\n│ 103         ┆ [\"tablet\"]                   ┆ 600         ┆ 1          │\n└─────────────┴──────────────────────────────┴─────────────┴────────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#resources",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#resources",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "Resources",
    "text": "Resources\n\nPolars List Operations Documentation\nPolars explode Documentation\nWorking with Nested Data in Polars"
  },
  {
    "objectID": "posts/2025-08-05-tfl-vs-tflm/index.html",
    "href": "posts/2025-08-05-tfl-vs-tflm/index.html",
    "title": "Difference between TFLite Python API vs TFLite Micro Python API",
    "section": "",
    "text": "TFLite Micro is the most popular Neural Network inference engine for micro CPUs. It’s designed for light weight model running on low power hardwares.\nMost common scenario is you use some training framework like PyTorch and TensorFlow, quantize it to tflite model. Finally you run your model on embedded device through the TFLite Micro library. However, the precision of the model usually loses during the quantization process. People would like to use the TFLite Python API to run the model on their computer, and verify the accuracy of the model is reduced to an acceptable level before deploying it to the embedded device. The problem is that the TFLite Python API cannot really simulate the model’s precision when running on the embedded device with TFLite Micro library. There are already lots of discussions about this issue, see github discussions 1 and 2\nUnknown to many people, TFLite Micro has a Python API, but the TFLite Micro Python API is different from the TFLite Python API. The TFLite Micro Python API is designed to run on the same principles as the TFLite Micro C++ library, which means it can provide a more accurate simulation of how the model will perform on embedded devices.\nIn this blog I will compare the TFLite Python API and the TFLite Micro Python API differences, and show how the two different APIs can be used to run the same model but have different results."
  },
  {
    "objectID": "posts/2025-08-05-tfl-vs-tflm/index.html#tflite-python-api",
    "href": "posts/2025-08-05-tfl-vs-tflm/index.html#tflite-python-api",
    "title": "Difference between TFLite Python API vs TFLite Micro Python API",
    "section": "TFLite Python API",
    "text": "TFLite Python API\nThe TFLite was previously decoupled from TensorFlow and was a standalone library called LiteRT now.\n\nLoading model:\nfrom ai-edge-litert.interpreter import Interpreter\ninterpreter = Interpreter(model_path=model)\nAllocate tensors:\ninterpreter.allocate_tensors()\nIt performs dynamic memory allocation for: a. allocates memory buffers for model inputs and outputs, b. allocates memory for all intermediate computation results between layers, c. allocates workspace memory needed during inference, d. finalizes the computation graph and prepares it for execution.\nGet input and output tensors details:\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nSet input tensor:\ninterpreter.set_tensor(input_details[0]['index'], x)\nRun inference:\ninterpreter.invoke()\nGet output tensor:\noutput_data = interpreter.get_tensor(output_details[0]['index'])"
  },
  {
    "objectID": "posts/2025-08-05-tfl-vs-tflm/index.html#tflite-micro-python-api",
    "href": "posts/2025-08-05-tfl-vs-tflm/index.html#tflite-micro-python-api",
    "title": "Difference between TFLite Python API vs TFLite Micro Python API",
    "section": "TFLite Micro Python API",
    "text": "TFLite Micro Python API\nThe TFLite Micro Python API is designed to run on the same principles as the TFLite Micro C++ library, it’s more aligned with the C++ API.\n\nLoading model, there are two ways to load the model file, one is from file, another is from bytearray.\nfrom tflite_micro.python.tflite_micro import runtime\n# Load from file\ninterpreter = runtime.Interpreter.from_file(model_path=model)\n# Load from bytearray\ninterpreter = runtime.Interpreter.from_bytes(model=model_bytes)\nThere is no need to allocate tensors. TFLite Micro pre-allocates all memory at compile time or initialization using a fixed-size arena. Embedded systems often lack heap allocation or have strict memory constraints, so TFLite Micro avoids malloc/free entirely.\nGet input and output tensors details:\ninput_details = interpreter.get_input_details(index)\noutput_details = interpreter.get_output_details(index)\nCompare to TFLite Python API where you can get all input and output details through one function call. In TFLite Micro Python API, you need to specify the index of the input or output tensor to get its details. For example, if you have multiple inputs or outputs, you can get the details of each one by specifying its index:\ninput_details = interpreter.get_input_details(0)  # Get details of first input tensor\noutput_details = interpreter.get_output_details(1)  # Get details of second output tensor\nSet input tensor:\n# Set input tensor using the input index\ninterpreter.set_input(x, 0)\n# If you have second input tensor:\ninterpreter.set_input(y, 1)\nCompared to TFLite Python API where you set the input tensor using the input details index, in TFLite Micro Python API, you set the input tensor using the input index directly. The index range depends on how many inputs you have. The order between the data and the index was reversed as well.\nRun inference:\ninterpreter.invoke()\nGet output tensor:\noutput_data = interpreter.get_output(0)\nThe same as set input tensor, you can use the output index depending on how many outputs you have."
  },
  {
    "objectID": "posts/2025-08-05-tfl-vs-tflm/index.html#difference-between-tflite-and-tflite-micro-output",
    "href": "posts/2025-08-05-tfl-vs-tflm/index.html#difference-between-tflite-and-tflite-micro-output",
    "title": "Difference between TFLite Python API vs TFLite Micro Python API",
    "section": "Difference between TFLite and TFLite Micro Output",
    "text": "Difference between TFLite and TFLite Micro Output\nI will use the yolov8n example from ultralytics to compare the difference between tflite and tflite-micro. They provide a script to inference yolov8 tflite model and visualize the result, makes it quite convenient. I make another class that subclass the YOLOv8TFLite example but using tflite-micro python package to do the inference.\nIf you are interested to follow the example you can refer my repo tflite_vs_tflite-micro.\n\nInference Speed\nCompared to TFLite python package, the TFLite Micro python package is much slower to run on PC. In this example, tflite python package only takes 0.04 second while tflite micro python package takes 76.7 second - 1917 times slower!!.\n\n\nInference Result\n\n\n \n\nIf you observe in detail, you could see that the bounding box of the bus is different and tflite predicts 0.34 confidence for the cut off person while tflite-micro predicts 0.39."
  },
  {
    "objectID": "posts/2025-08-05-tfl-vs-tflm/index.html#conclusion",
    "href": "posts/2025-08-05-tfl-vs-tflm/index.html#conclusion",
    "title": "Difference between TFLite Python API vs TFLite Micro Python API",
    "section": "Conclusion",
    "text": "Conclusion\nFrom the example above, we could see that due to the implementation difference TFLite Python API and TFLite Micro Python produced different result. I have seen worse cases than where the tflite result and tflite-micro result diverge a lot this from my past experience. My suggestion is if you want to check or reproduce your embedded tflite-micro result with python script, tflite-micro python package is a better choice than tflite python package. But due to the slow inference speed of tflite-micro package, it’s not suitable for evaluation the whole evaluation dataset."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a machine learning enginer with focus on computer vision and effcient machine learning solutions on edge based in Sweden.\nI mainly work with image related tasks nowadays from image signal processing, object detection/tracking/segmentation, face recognition systems."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "",
    "text": "Today we will explore what polars selectors are, and what we can do with them."
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#introduction",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#introduction",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "",
    "text": "Today we will explore what polars selectors are, and what we can do with them."
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#what-are-polars-selectors",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#what-are-polars-selectors",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "What are Polars Selectors?",
    "text": "What are Polars Selectors?\nSelectors in polars are tools that you can use to select columns based on their properties, data types or patterns. For example if you want to select all string/numeric columns in your data frames:\nimport polars as pl\nimport polars.selectors as cs\n\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"test_score\": [88, 92],\n    \"final_score\": [95, 89],\n    \"category\": [\"A\", \"B\"],\n    \"updated_at\": [None, None]\n})\n\n# select all string columns\ndf.select(cs.string())\n\n# select all numeric columns\ndf.select(cs.numeric())\nThere are mainly three types of selectors in polars, 1. type based - you select columns based on data type; 2. pattern based - select columns based on pattern matching; 3. set logic selectors - combing multiple selectors together.\nWe will look at them by category"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#type-based-selectors",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#type-based-selectors",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "Type Based Selectors",
    "text": "Type Based Selectors\n\ncs.numeric()\n\n\ncs.string()\n\n\ncs.temporal()\nSelector targets columns with time-based data type.\nimport polars as pl\nimport polars.selectors as cs\nfrom datetime import datetime\n\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"transaction_date\": [datetime(2023, 5, 12), datetime(2023, 6, 15)],\n    \"upload_at\": [datetime(2023, 5, 13, 10, 0), datetime(2023, 6, 16, 11, 0)],\n    \"amount\": [100.0, 200.0]\n})\n\n# Use cs.temporal() to apply a date operation to all time columns\nresult = df.with_columns(\n    cs.temporal().dt.month_start()\n)\n\n\ncs.by_name()\n\nSelecting columns by name\n\nimport polars as pl\nimport polars.selectors as cs\n\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"test_score\": [88, 92],\n    \"final_score\": [95, 89],\n    \"category\": [\"A\", \"B\"],\n    \"updated_at\": [None, None]\n})\n\n# Select specific columns by exact name\ndf.select(cs.by_name(\"id\", \"category\"))\n\n\ncs.by_dtype()\n\nSelecting columns by data type"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#pattern-based-selectors",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#pattern-based-selectors",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "Pattern-based Selectors",
    "text": "Pattern-based Selectors\n\ncs.contains()\ncs.contains(\"score\")\n\n\ncs.matches()\n\nAllow to use regex patterns\n\nimport polars as pl\nimport polars.selectors as cs\n\ndf = pl.DataFrame({\n    \"abc_123\": [1],\n    \"abc_456\": [2],\n    \"xyz_123\": [3],\n    \"id_primary\": [4],\n    \"id_secondary\": [5]\n})\n# Select columns that start with 3 letters, an underscore, and then numbers\n# Pattern: ^[a-z]{3}_\\d+$\nresult = df.select(\n    cs.matches(r\"^[a-z]{3}_\\d+$\")\n)\n\n\ncs.starts_with()\ncs.starts_with(\"sale_\")\n\n\ncs.ends_with()\ncs.ends_with(\"sum\")"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#combining-selectors",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#combining-selectors",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "Combining Selectors",
    "text": "Combining Selectors\n\nSet Operations\n\nUnion (|)\n\ntarget_cols = cs.starts_with(\"sale_\") | cs.numeric()\n\ndf.select(target_cols)\n\nIntersection (&)\n\ntarget_cols = cs.starts_with(\"sale_\") & cs.numeric()\n\ndf.select(target_cols)\n\nDifference (-): used to exclude some columns\n\n# Logic: All Numerics MINUS the columns we want to protect\nfeatures = cs.numeric() - cs.by_name(\"user_id\", \"target_label\")\n\ndf.with_columns(\n    features.standardize()\n)\n\nComplement (~)\n\ndf.select(~cs.string())"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#practice-exercise",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#practice-exercise",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "Practice Exercise",
    "text": "Practice Exercise\nNow it’s time to practice! Try solving this exercise using selectors:\nScenario: You have a sales dataset with the following structure:\nimport polars as pl\nimport polars.selectors as cs\nfrom datetime import datetime\n\nsales_df = pl.DataFrame({\n    \"order_id\": [1001, 1002, 1003, 1004, 1005],\n    \"customer_id\": [501, 502, 503, 504, 505],\n    \"product_price\": [29.99, 149.99, 79.99, 199.99, 49.99],\n    \"shipping_cost\": [5.99, 12.99, 8.99, 15.99, 6.99],\n    \"tax_amount\": [2.40, 12.00, 6.40, 16.00, 4.00],\n    \"sale_region\": [\"North\", \"South\", \"East\", \"West\", \"North\"],\n    \"sale_channel\": [\"Online\", \"Store\", \"Online\", \"Store\", \"Online\"],\n    \"order_date\": [\n        datetime(2026, 1, 10),\n        datetime(2026, 1, 11),\n        datetime(2026, 1, 12),\n        datetime(2026, 1, 13),\n        datetime(2026, 1, 14)\n    ],\n    \"delivery_date\": [\n        datetime(2026, 1, 15),\n        datetime(2026, 1, 16),\n        datetime(2026, 1, 17),\n        datetime(2026, 1, 18),\n        datetime(2026, 1, 19)\n    ]\n})\nTasks:\n\nSelect all columns that contain the word “sale” in their name\nSelect all numeric columns EXCEPT the ID columns (order_id and customer_id)\nCalculate the sum of all columns that end with “_cost” or “_amount”\nExtract just the month from all temporal columns\nSelect all string columns that start with “sale_” and convert them to lowercase\n\nBonus Challenge: Create a single expression that selects all numeric columns (except IDs), rounds them to 2 decimal places, and adds a “_rounded” suffix to each column name.\n\n\nClick to see solutions\n\n# Task 1: Select columns containing \"sale\"\nsales_df.select(cs.contains(\"sale\"))\n\n# Task 2: Numeric columns excluding IDs\nsales_df.select(cs.numeric() - cs.by_name(\"order_id\", \"customer_id\"))\n\n# Task 3: Sum of cost and amount columns\nsales_df.select(\n    (cs.ends_with(\"_cost\") | cs.ends_with(\"_amount\")).sum()\n)\n\n# Task 4: Extract month from temporal columns\nsales_df.with_columns(\n    cs.temporal().dt.month()\n)\n\n# Task 5: Lowercase string columns starting with \"sale_\"\nsales_df.with_columns(\n    (cs.starts_with(\"sale_\") & cs.string()).str.to_lowercase()\n)\n\n# Bonus: Round numeric columns (except IDs) with suffix\nsales_df.with_columns(\n    (cs.numeric() - cs.by_name(\"order_id\", \"customer_id\"))\n    .round(2)\n    .name.suffix(\"_rounded\")\n)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#resources",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#resources",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "Resources",
    "text": "Resources\n\nPolars Documentation on Selectors"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "",
    "text": "Data integrity is the foundation of reliable data pipelines. Without proper validation and null handling, your analyses can produce misleading results or fail entirely. In Polars, you have powerful tools to ensure your data meets quality standards before processing.\nToday we’ll cover: - Null value handling: Detection, filling, and removal strategies - Schema validation: Ensuring data types match expectations - Data quality checks: Comprehensive validation patterns"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#introduction",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#introduction",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "",
    "text": "Data integrity is the foundation of reliable data pipelines. Without proper validation and null handling, your analyses can produce misleading results or fail entirely. In Polars, you have powerful tools to ensure your data meets quality standards before processing.\nToday we’ll cover: - Null value handling: Detection, filling, and removal strategies - Schema validation: Ensuring data types match expectations - Data quality checks: Comprehensive validation patterns"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#understanding-nulls-in-polars",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#understanding-nulls-in-polars",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Understanding Nulls in Polars",
    "text": "Understanding Nulls in Polars\nPolars uses null to represent missing values, distinct from NaN (not-a-number for floats) and None.\nimport polars as pl\nimport numpy as np\n\n# Create a DataFrame with various null-like values\ndf = pl.DataFrame({\n    \"id\": [1, 2, 3, 4, 5],\n    \"name\": [\"Alice\", \"Bob\", None, \"David\", \"Eve\"],\n    \"age\": [25, None, 30, 35, None],\n    \"score\": [85.5, 90.0, np.nan, 78.0, 92.5]\n})\n\nprint(df)\nshape: (5, 4)\n┌─────┬────────┬──────┬───────┐\n│ id  ┆ name   ┆ age  ┆ score │\n│ --- ┆ ---    ┆ ---  ┆ ---   │\n│ i64 ┆ str    ┆ i64  ┆ f64   │\n╞═════╪════════╪══════╪═══════╡\n│ 1   ┆ Alice  ┆ 25   ┆ 85.5  │\n│ 2   ┆ Bob    ┆ null ┆ 90.0  │\n│ 3   ┆ null   ┆ 30   ┆ NaN   │\n│ 4   ┆ David  ┆ 35   ┆ 78.0  │\n│ 5   ┆ Eve    ┆ null ┆ 92.5  │\n└─────┴────────┴──────┴───────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#detecting-null-values",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#detecting-null-values",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Detecting Null Values",
    "text": "Detecting Null Values\n\nBasic Null Detection\n# Check for nulls in each column\nnull_counts = df.null_count()\nprint(null_counts)\n\n# Check if any nulls exist in the entire DataFrame\nhas_nulls = df.null_count().sum_horizontal().max() &gt; 0\nprint(f\"DataFrame has nulls: {has_nulls}\")\n\n\nColumn-Specific Null Checks\n# Count nulls per column\nfor col in df.columns:\n    null_count = df[col].is_null().sum()\n    print(f\"{col}: {null_count} nulls\")\n\n# Alternative: Get null percentage\nnull_pct = df.null_count() / df.height * 100\nprint(null_pct)\n\n\nFinding Rows with Nulls\n# Find rows with any null values\ndf_with_nulls = df.filter(\n    pl.any_horizontal(pl.all().is_null())\n)\nprint(df_with_nulls)\n\n# Find rows where specific columns have nulls\ndf_name_or_age_null = df.filter(\n    pl.col(\"name\").is_null() | pl.col(\"age\").is_null()\n)\nprint(df_name_or_age_null)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#handling-null-values",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#handling-null-values",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Handling Null Values",
    "text": "Handling Null Values\n\nStrategy 1: Drop Nulls\n# Drop rows with ANY null values\ndf_clean = df.drop_nulls()\n\n# Drop rows with nulls in specific columns\ndf_clean_names = df.drop_nulls(subset=[\"name\"])\n\n# Drop columns that contain any nulls\ndf_no_null_cols = df.select([\n    col for col in df.columns \n    if df[col].null_count() == 0\n])\n\n\nStrategy 2: Fill Nulls\n# Fill with a constant value\ndf_filled = df.with_columns(\n    pl.col(\"age\").fill_null(0),\n    pl.col(\"name\").fill_null(\"Unknown\")\n)\n\n# Fill with forward fill (previous value)\ndf_ffill = df.with_columns(\n    pl.col(\"age\").fill_null(strategy=\"forward\")\n)\n\n# Fill with backward fill (next value)\ndf_bfill = df.with_columns(\n    pl.col(\"age\").fill_null(strategy=\"backward\")\n)\n\n# Fill with mean (for numeric columns)\ndf_mean_filled = df.with_columns(\n    pl.col(\"age\").fill_null(pl.col(\"age\").mean())\n)\n\n# Fill with median\ndf_median_filled = df.with_columns(\n    pl.col(\"age\").fill_null(pl.col(\"age\").median())\n)\n\n# Fill with interpolation (linear)\ndf_interpolated = df.with_columns(\n    pl.col(\"age\").interpolate()\n)\n\n\nStrategy 3: Conditional Filling\n# Fill based on conditions\ndf_conditional = df.with_columns(\n    pl.when(pl.col(\"age\").is_null())\n    .then(18)  # Default age if missing\n    .otherwise(pl.col(\"age\"))\n    .alias(\"age_filled\")\n)\n\n# Fill based on group statistics\ndf_grouped_fill = df.with_columns(\n    pl.col(\"age\").fill_null(\n        pl.col(\"age\").mean().over(\"department\")  # If you have a department column\n    )\n)\n\n\nHandling NaN Values\n# Detect NaN (different from null)\ndf_with_nan_check = df.with_columns(\n    pl.col(\"score\").is_nan().alias(\"is_nan\")\n)\n\n# Replace NaN with null first, then handle\ndf_no_nan = df.with_columns(\n    pl.col(\"score\").replace({float(\"nan\"): None})\n)\n\n# Or fill NaN directly\ndf_filled_nan = df.with_columns(\n    pl.when(pl.col(\"score\").is_nan())\n    .then(0.0)\n    .otherwise(pl.col(\"score\"))\n)\n\ndf_filled_nan = df.fill_nan(0.0)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#schema-validation",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#schema-validation",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Schema Validation",
    "text": "Schema Validation\n\nUnderstanding Polars Schemas\n# Check current schema\nprint(df.schema)\n\n# Get detailed schema information\nfor name, dtype in df.schema.items():\n    print(f\"{name}: {dtype}\")\n\n\nStrict Schema Definition\nfrom typing import Dict\n\n# Define expected schema\nexpected_schema: Dict[str, pl.DataType] = {\n    \"id\": pl.Int64,\n    \"name\": pl.Utf8,\n    \"age\": pl.Int64,\n    \"score\": pl.Float64,\n    \"is_active\": pl.Boolean\n}\n\ndef validate_schema(df: pl.DataFrame, expected: Dict[str, pl.DataType]) -&gt; bool:\n    \"\"\"Validate that DataFrame matches expected schema.\"\"\"\n    actual_schema = df.schema\n    \n    # Check all expected columns exist\n    for col, dtype in expected.items():\n        if col not in actual_schema:\n            print(f\"Missing column: {col}\")\n            return False\n        if actual_schema[col] != dtype:\n            print(f\"Type mismatch for {col}: expected {dtype}, got {actual_schema[col]}\")\n            return False\n    \n    return True\n\n# Use the validator\nis_valid = validate_schema(df, expected_schema)\nprint(f\"Schema valid: {is_valid}\")\n\n\nSchema Enforcement\ndef enforce_schema(df: pl.DataFrame, schema: Dict[str, pl.DataType]) -&gt; pl.DataFrame:\n    \"\"\"Cast columns to expected types, creating missing columns with nulls.\"\"\"\n    columns = []\n    \n    for col_name, dtype in schema.items():\n        if col_name in df.columns:\n            # Cast existing column to expected type\n            columns.append(pl.col(col_name).cast(dtype))\n        else:\n            # Create column with nulls of expected type\n            columns.append(pl.lit(None).cast(dtype).alias(col_name))\n    \n    return df.select(columns)\n\n# Apply schema enforcement\ndf_enforced = enforce_schema(df, expected_schema)\nprint(df_enforced)\nprint(df_enforced.schema)\n\n\nType Coercion with Error Handling\n# Safe casting that handles errors\ndef safe_cast(df: pl.DataFrame, column: str, dtype: pl.DataType, \n              default=None) -&gt; pl.DataFrame:\n    \"\"\"Attempt to cast column, filling errors with default value.\"\"\"\n    try:\n        return df.with_columns(\n            pl.col(column).cast(dtype, strict=False)\n        )\n    except Exception as e:\n        print(f\"Error casting {column}: {e}\")\n        if default is not None:\n            return df.with_columns(\n                pl.lit(default).cast(dtype).alias(column)\n            )\n        return df\n\n# Example: Cast age to Int32 with fallback\ndf_casted = safe_cast(df, \"age\", pl.Int32, default=0)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#comprehensive-data-quality-checks",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#comprehensive-data-quality-checks",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Comprehensive Data Quality Checks",
    "text": "Comprehensive Data Quality Checks\n\nUniqueness Constraints\n# Check for duplicates\ndef check_uniqueness(df: pl.DataFrame, columns: list) -&gt; bool:\n    \"\"\"Check if specified columns have unique values.\"\"\"\n    unique_count = df.select(columns).n_unique()\n    total_count = df.height\n    \n    is_unique = unique_count == total_count\n    if not is_unique:\n        duplicates = total_count - unique_count\n        print(f\"Found {duplicates} duplicate rows in columns {columns}\")\n    \n    return is_unique\n\n# Check id column is unique\nis_id_unique = check_uniqueness(df, [\"id\"])\n\n# Find duplicates\nduplicates = df.filter(\n    pl.col(\"id\").is_duplicated()\n)\nprint(duplicates)\n\n\nPattern Validation (Strings)\n# Validate email format\ndef validate_emails(df: pl.DataFrame, email_col: str) -&gt; pl.DataFrame:\n    email_pattern = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n    \n    return df.with_columns(\n        pl.col(email_col)\n        .str.contains(email_pattern)\n        .alias(f\"{email_col}_valid\")\n    )\n\n# Create sample data with emails\nemail_df = pl.DataFrame({\n    \"id\": [1, 2, 3],\n    \"email\": [\"alice@example.com\", \"bob@invalid\", \"charlie@company.org\"]\n})\n\nemail_validated = validate_emails(email_df, \"email\")\nprint(email_validated)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#complete-data-validation-pipeline",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#complete-data-validation-pipeline",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Complete Data Validation Pipeline",
    "text": "Complete Data Validation Pipeline\nclass DataValidator:\n    \"\"\"Comprehensive data validation for Polars DataFrames.\"\"\"\n    \n    def __init__(self, schema: Dict[str, pl.DataType]):\n        self.schema = schema\n        self.errors = []\n    \n    def validate(self, df: pl.DataFrame) -&gt; tuple[bool, pl.DataFrame]:\n        \"\"\"Run all validations and return (is_valid, validated_df).\"\"\"\n        self.errors = []\n        \n        # Step 1: Schema validation\n        if not self._validate_schema(df):\n            return False, df\n        \n        # Step 2: Enforce schema\n        df = self._enforce_schema(df)\n        \n        # Step 3: Check null thresholds\n        self._check_nulls(df)\n        \n        # Step 4: Check duplicates\n        self._check_duplicates(df)\n        \n        is_valid = len(self.errors) == 0\n        return is_valid, df\n    \n    def _validate_schema(self, df: pl.DataFrame) -&gt; bool:\n        for col, dtype in self.schema.items():\n            if col not in df.columns:\n                self.errors.append(f\"Missing required column: {col}\")\n        return len(self.errors) == 0\n    \n    def _enforce_schema(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n        columns = []\n        for col, dtype in self.schema.items():\n            if col in df.columns:\n                columns.append(pl.col(col).cast(dtype, strict=False))\n            else:\n                columns.append(pl.lit(None).cast(dtype).alias(col))\n        return df.select(columns)\n    \n    def _check_nulls(self, df: pl.DataFrame, threshold: float = 0.1):\n        for col in df.columns:\n            null_pct = df[col].is_null().sum() / df.height\n            if null_pct &gt; threshold:\n                self.errors.append(f\"{col}: {null_pct:.1%} nulls exceeds threshold\")\n    \n    def _check_duplicates(self, df: pl.DataFrame):\n        dups = df.height - df.n_unique()\n        if dups &gt; 0:\n            self.errors.append(f\"Found {dups} duplicate rows\")\n    \n    def get_errors(self) -&gt; list:\n        return self.errors\n\n# Usage example\nvalidator = DataValidator(expected_schema)\nis_valid, validated_df = validator.validate(df)\n\nif not is_valid:\n    print(\"Validation errors:\")\n    for error in validator.get_errors():\n        print(f\"  - {error}\")\nelse:\n    print(\"Data validation passed!\")\n    print(validated_df)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#best-practices-for-data-integrity",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#best-practices-for-data-integrity",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Best Practices for Data Integrity",
    "text": "Best Practices for Data Integrity\n\n1. Always Validate at Ingestion\ndef load_and_validate_data(path: str, validator: DataValidator) -&gt; pl.DataFrame:\n    \"\"\"Load data with validation.\"\"\"\n    df = pl.read_csv(path)\n    \n    is_valid, df = validator.validate(df)\n    if not is_valid:\n        raise ValueError(f\"Data validation failed: {validator.get_errors()}\")\n    \n    return df"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#practice-exercise",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#practice-exercise",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Practice Exercise",
    "text": "Practice Exercise\nScenario: You’re building a data pipeline for customer information:\nimport polars as pl\n\n# Sample data with quality issues\ncustomers = pl.DataFrame({\n    \"customer_id\": [1, 2, 3, 4, 4, 6],  # Duplicate ID (4)\n    \"name\": [\"Alice\", None, \"Charlie\", \"David\", \"Eve\", \"Frank\"],\n    \"email\": [\"alice@example.com\", \"bob@invalid\", None, \"david@test.com\", \n              \"eve@company.org\", \"frank@example.com\"],\n    \"age\": [25, 150, 30, -5, 35, None],  # Invalid ages (150, -5)\n    \"registration_date\": [\"2023-01-15\", \"invalid_date\", \"2023-03-20\",\n                          None, \"2023-05-10\", \"2023-06-01\"],\n    \"is_premium\": [True, False, True, None, False, True]\n})\nTasks:\n\nDefine a proper schema for this data with appropriate types\nDetect and report all null values\nFix the duplicate customer_id (keep the first occurrence)\nValidate and clean the age column (valid range: 18-100)\nValidate email format for non-null emails\nParse registration_date as Date type, handling errors gracefully\nGenerate a data quality report after cleaning\n\nBonus Challenge:\nCreate a validation pipeline that: - Rejects rows with invalid emails - Fills missing ages with the median age - Drops rows with duplicate IDs (keep first) - Converts registration_date to Date type or null if invalid\n\n\nClick to see solutions\n\nimport polars as pl\nfrom datetime import datetime\n\n# Task 1: Define schema\nexpected_schema = {\n    \"customer_id\": pl.Int64,\n    \"name\": pl.Utf8,\n    \"email\": pl.Utf8,\n    \"age\": pl.Int64,\n    \"registration_date\": pl.Date,\n    \"is_premium\": pl.Boolean\n}\n\n# Task 2: Detect nulls\nprint(\"Null counts:\")\nfor col in customers.columns:\n    null_count = customers[col].null_count()\n    print(f\"  {col}: {null_count}\")\n\n# Task 3: Remove duplicates (keep first)\ncustomers_clean = customers.unique(subset=[\"customer_id\"], keep=\"first\")\n\n# Task 4: Validate and clean age\n# First, see invalid ages\ninvalid_ages = customers_clean.filter(\n    (pl.col(\"age\") &lt; 18) | (pl.col(\"age\") &gt; 100)\n)\nprint(f\"\\nInvalid ages: {invalid_ages.height} rows\")\n\n# Clean ages: set invalid to null, then fill with median\nvalid_ages = customers_clean.with_columns(\n    pl.when((pl.col(\"age\") &gt;= 18) & (pl.col(\"age\") &lt;= 100))\n    .then(pl.col(\"age\"))\n    .otherwise(None)\n    .alias(\"age\")\n)\nmedian_age = valid_ages[\"age\"].median()\ncustomers_clean = valid_ages.with_columns(\n    pl.col(\"age\").fill_null(int(median_age))\n)\n\n# Task 5: Validate email format\nemail_pattern = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\ncustomers_clean = customers_clean.with_columns(\n    pl.when(pl.col(\"email\").is_null())\n    .then(None)\n    .when(pl.col(\"email\").str.contains(email_pattern))\n    .then(pl.col(\"email\"))\n    .otherwise(None)\n    .alias(\"email\")\n)\n\n# Task 6: Parse dates\ncustomers_clean = customers_clean.with_columns(\n    pl.col(\"registration_date\").str.to_date(format=\"%Y-%m-%d\", strict=False)\n)\n\n# Task 7: Generate report\ndef generate_report(df):\n    return {\n        \"shape\": (df.height, df.width),\n        \"null_percentages\": {\n            col: df[col].null_count() / df.height \n            for col in df.columns\n        },\n        \"duplicates\": df.height - df.n_unique()\n    }\n\nreport = generate_report(customers_clean)\nprint(\"\\nFinal Data Quality Report:\")\nprint(f\"  Shape: {report['shape']}\")\nprint(f\"  Duplicates: {report['duplicates']}\")\nprint(\"  Null Percentages:\")\nfor col, pct in report['null_percentages'].items():\n    print(f\"    {col}: {pct:.1%}\")\n\nprint(\"\\nCleaned DataFrame:\")\nprint(customers_clean)\n\n# Bonus: Complete validation pipeline\ndef validate_customers(df: pl.DataFrame) -&gt; pl.DataFrame:\n    # 1. Remove duplicates\n    df = df.unique(subset=[\"customer_id\"], keep=\"first\")\n    \n    # 2. Clean and fill ages\n    df = df.with_columns(\n        pl.when((pl.col(\"age\") &gt;= 18) & (pl.col(\"age\") &lt;= 100))\n        .then(pl.col(\"age\"))\n        .otherwise(None)\n        .alias(\"age\")\n    )\n    median_age = int(df[\"age\"].median())\n    df = df.with_columns(pl.col(\"age\").fill_null(median_age))\n    \n    # 3. Validate emails (set invalid to null)\n    email_pattern = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n    df = df.with_columns(\n        pl.when(pl.col(\"email\").str.contains(email_pattern))\n        .then(pl.col(\"email\"))\n        .otherwise(None)\n    )\n    \n    # 4. Parse dates\n    df = df.with_columns(\n        pl.col(\"registration_date\").str.to_date(format=\"%Y-%m-%d\", strict=False)\n    )\n    \n    # 5. Fill missing names\n    df = df.with_columns(pl.col(\"name\").fill_null(\"Unknown\"))\n    \n    # 6. Fill missing premium status\n    df = df.with_columns(pl.col(\"is_premium\").fill_null(False))\n    \n    return df\n\nfinal_df = validate_customers(customers)\nprint(\"\\nBonus - Validated DataFrame:\")\nprint(final_df)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#resources",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#resources",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Resources",
    "text": "Resources\n\nPolars Null Handling Documentation\nData Types and Schema\nCasting and Type Conversion\nData Validation Best Practices"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-18-day002-window-function.html",
    "href": "posts/100-days-of-polars/2026-01-18-day002-window-function.html",
    "title": "100 Days of Polars - Day 002: Window Functions",
    "section": "",
    "text": "Window functions are one of the most powerful features in Polars for performing calculations across rows in a group. Unlike aggregations group_by that collapse rows into single values, window functions maintain all rows while computing values based on a “window” of data.\nWindow function allows you to:\n\nCalculate running totals and moving averages\nRank rows within groups\nCompare values with group averages\nPerform time-based calculations\n\nWindow function can be used through .over() in polars. You can achive the result as window function with group_by and join in polars.\nimport polars as pl\nfrom polars import window_function as wf\n\ndf = pl.DataFrame({\n    \"department\": [\"Sales\", \"Sales\", \"Sales\", \"Engineering\", \"Engineering\", \"Engineering\"],\n    \"employee\": [\"Alice\", \"Bob\", \"Carol\", \"David\", \"Eve\", \"Frank\"],\n    \"salary\": [50000, 60000, 55000, 75000, 80000, 72000]\n})\n\n# Add a column showing each employee's salary compared to their department's average.\ndf.with_columns(\n    dept_avg_salary=pl.col(\"salary\").mean().over(\"department\")\n)\n\n# Equivalent with group_by and join\ndept_avg = df.group_by(\"department\").agg(\n    pl.col(\"salary\").mean().alias(\"dept_avg_salary\")\n)\ndf.join(dept_avg, on=\"department\")"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#introduction-what-are-window-functions",
    "href": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#introduction-what-are-window-functions",
    "title": "100 Days of Polars - Day 002: Window Functions",
    "section": "",
    "text": "Window functions are one of the most powerful features in Polars for performing calculations across rows in a group. Unlike aggregations group_by that collapse rows into single values, window functions maintain all rows while computing values based on a “window” of data.\nWindow function allows you to:\n\nCalculate running totals and moving averages\nRank rows within groups\nCompare values with group averages\nPerform time-based calculations\n\nWindow function can be used through .over() in polars. You can achive the result as window function with group_by and join in polars.\nimport polars as pl\nfrom polars import window_function as wf\n\ndf = pl.DataFrame({\n    \"department\": [\"Sales\", \"Sales\", \"Sales\", \"Engineering\", \"Engineering\", \"Engineering\"],\n    \"employee\": [\"Alice\", \"Bob\", \"Carol\", \"David\", \"Eve\", \"Frank\"],\n    \"salary\": [50000, 60000, 55000, 75000, 80000, 72000]\n})\n\n# Add a column showing each employee's salary compared to their department's average.\ndf.with_columns(\n    dept_avg_salary=pl.col(\"salary\").mean().over(\"department\")\n)\n\n# Equivalent with group_by and join\ndept_avg = df.group_by(\"department\").agg(\n    pl.col(\"salary\").mean().alias(\"dept_avg_salary\")\n)\ndf.join(dept_avg, on=\"department\")"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#common-window-functions",
    "href": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#common-window-functions",
    "title": "100 Days of Polars - Day 002: Window Functions",
    "section": "Common Window Functions",
    "text": "Common Window Functions\n\nAggregations\nAll standard aggregation functions work as window functions, aggregation functions must come before window functions .over:\ndf.with_columns(\n    dept_salary_sum=pl.col(\"salary\").sum().over(\"department\"),\n    dept_salary_min=pl.col(\"salary\").min().over(\"department\"),\n    dept_salary_max=pl.col(\"salary\").max().over(\"department\"),\n    dept_salary_count=pl.col(\"salary\").count().over(\"department\")\n)\n\n\nCumulative Sums\ndf.sort(\"salary\").with_columns(\n    running_total=pl.col(\"salary\").cum_sum().over(\"department\")\n)\n\n\nRanking\ndf.with_columns(\n    salary_rank=pl.col(\"salary\").rank().over(\"department\")\n)\nAvailable ranking methods: - \"ordinal\" - sequential integers (1, 2, 3…) - \"dense\" - dense ranking without gaps - \"min\", \"max\", \"avg\" - various average ranking methods\n\n\nFirst and Last Values\ndf.sort(\"salary\").with_columns(\n    lowest_in_dept=pl.col(\"salary\").first().over(\"department\"),\n    highest_in_dept=pl.col(\"salary\").last().over(\"department\")\n)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#practice-exercise",
    "href": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#practice-exercise",
    "title": "100 Days of Polars - Day 002: Window Functions",
    "section": "Practice Exercise",
    "text": "Practice Exercise\nScenario: You have a sales dataset with regional sales data:\nimport polars as pl\nfrom datetime import datetime\n\nsales_df = pl.DataFrame({\n    \"region\": [\"North\", \"North\", \"North\", \"South\", \"South\", \"South\", \"East\", \"East\"],\n    \"salesperson\": [\"Alice\", \"Bob\", \"Carol\", \"David\", \"Eve\", \"Frank\", \"Grace\", \"Henry\"],\n    \"quarter\": [\"Q1\", \"Q1\", \"Q2\", \"Q1\", \"Q2\", \"Q2\", \"Q1\", \"Q2\"],\n    \"sales\": [10000, 15000, 12000, 20000, 18000, 22000, 14000, 16000]\n})\nTasks:\n\nCalculate each salesperson’s sales as a percentage of their region’s total\nRank salespeople within each region by sales amount\nCalculate running total of sales within each region (sorted by sales amount)\nFind the top performer in each region and quarter\n\n\n\nClick to see solutions\n\n# Task 1: Percentage of regional total\nsales_df.with_columns(\n    region_total=pl.col(\"sales\").sum().over(\"region\"),\n    pct_of_region=(pl.col(\"sales\") / pl.col(\"region_total\")) * 100\n).drop(\"region_total\")\n\n# Task 2: Rank within region\nsales_df.with_columns(\n    rank_in_region=pl.col(\"sales\").rank(\"ordinal\", descending=True).over(\"region\")\n)\n\n# Task 3: Running total within region\nsales_df.sort(\"sales\", descending=True).with_columns(\n    running_total=pl.col(\"sales\").cum_sum().over(\"region\")\n)\n\n# Task 4: Top performer in each region and quarter\nsales_df.with_columns(\n    max_sales=pl.col(\"sales\").max().over(\"region\", \"quarter\")\n).filter(pl.col(\"sales\") == pl.col(\"max_sales\")).drop(\"max_sales\")"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#resources",
    "href": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#resources",
    "title": "100 Days of Polars - Day 002: Window Functions",
    "section": "Resources",
    "text": "Resources\n\nPolars Documentation on Window Functions\nPolars Window Function API"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blogs",
    "section": "",
    "text": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\n100-days-of-polars\n\n\n\nLearn to maintain data integrity in Polars through null handling techniques and schema validation strategies\n\n\n\n\n\nFeb 2, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\n100 Days of Polars - Day 005: Working with Structs - Nested Data Structures\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\n100-days-of-polars\n\n\n\nUnderstanding how to work with struct columns in Polars for hierarchical and nested data\n\n\n\n\n\nJan 29, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\n100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\n100-days-of-polars\n\n\n\nUnderstanding how to work with nested list data in Polars using explode, implode, and list operations\n\n\n\n\n\nJan 28, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\n100 Days of Polars - Day 003: map_elements vs map_batches vs Native Expressions\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\n100-days-of-polars\n\n\n\nUnderstanding when to use map_elements, map_batches, and native Polars expressions\n\n\n\n\n\nJan 19, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\n100 Days of Polars - Day 002: Window Functions\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\n100-days-of-polars\n\n\n\nMaster window functions in Polars for advanced data transformations\n\n\n\n\n\nJan 18, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\n100 Days of Polars - Day 001: Polars Selectors\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\n100-days-of-polars\n\n\n\nPolars selectors for efficient column selection\n\n\n\n\n\nJan 17, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\nDifference between TFLite Python API vs TFLite Micro Python API\n\n\n\n\n\n\ntflite\n\n\ntflite-micro\n\n\ncode\n\n\ntinyml\n\n\n\n\n\n\n\n\n\nAug 5, 2025\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nAug 2, 2025\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  }
]