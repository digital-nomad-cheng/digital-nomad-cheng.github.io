[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Nomad's Wonderland",
    "section": "",
    "text": "List of projects I worked with."
  },
  {
    "objectID": "AWS_CREDENTIALS_GUIDE.html",
    "href": "AWS_CREDENTIALS_GUIDE.html",
    "title": "AWS Credentials Configuration Guide",
    "section": "",
    "text": "# Ubuntu/Debian\nsudo apt-get update\nsudo apt-get install awscli\n\n# macOS\nbrew install awscli\n\n# Or download from AWS\n# https://aws.amazon.com/cli/\n\n\n\naws configure\nYou’ll be prompted for:\nAWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE\nAWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nDefault region name [None]: us-east-1\nDefault output format [None]: json\n\n\n\naws sts get-caller-identity\nOutput should show:\n{\n    \"UserId\": \"AIDACKCEVSQ6C2EXAMPLE\",\n    \"Account\": \"123456789012\",\n    \"Arn\": \"arn:aws:iam::123456789012:user/your-username\"\n}\n\n\n\n\nSet temporarily in your shell:\nexport AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_DEFAULT_REGION=us-east-1\nOr add to ~/.bashrc or ~/.zshrc for persistence:\necho 'export AWS_ACCESS_KEY_ID=your_key' &gt;&gt; ~/.bashrc\necho 'export AWS_SECRET_ACCESS_KEY=your_secret' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n\n\nEdit ~/.aws/credentials:\n[default]\naws_access_key_id = AKIAIOSFODNN7EXAMPLE\naws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\n[work]\naws_access_key_id = AKIAI44QH8DHBEXAMPLE\naws_secret_access_key = je7MtGbClwBF/2Zp9Utk/h3yCo8nvbEXAMPLEKEY\n\n[personal]\naws_access_key_id = AKIAIOSFODNN7EXAMPLE\naws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nEdit ~/.aws/config for profiles:\n[default]\nregion = us-east-1\noutput = json\n\n[profile work]\nregion = us-west-2\noutput = json\n\n[profile personal]\nregion = eu-west-1\noutput = yaml\n\n\n# Set default profile\nexport AWS_PROFILE=work\n\n# Or use with aws command\naws s3 ls --profile work\n\n# Or in Python/Polars\nimport polars as pl\ndf = pl.read_parquet(\"s3://bucket/file.parquet\", storage_options={\"profile\": \"work\"})\n\n\n\n\nIf running on AWS infrastructure, no configuration needed!\nimport polars as pl\n\n# Automatically uses instance role\ndf = pl.read_parquet(\"s3://my-private-bucket/data.parquet\")\n\n\n\nFor temporary credentials (MFA, assumed roles):\nexport AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_SESSION_TOKEN=IQoJb3JpZ2luX2VjEHYaCXVzLWVhc3QtMSJ...\n\n\n\n\n\n\nSign in to AWS Console\nClick your name (top right) → “Security Credentials”\nClick “Access keys” → “Create access key”\nDownload the .csv file (you won’t see the secret key again!)\n\n\n\n\n# Create access key\naws iam create-access-key --user-name your-username\n\n# List access keys\naws iam list-access-keys --user-name your-username\n\n\n\n# Assume a role\naws sts assume-role \\\n  --role-arn arn:aws:iam::123456789012:role/MyRole \\\n  --role-session-name MySession \\\n  --duration-seconds 3600\n\n# Set the temporary credentials from the output\nexport AWS_ACCESS_KEY_ID=...\nexport AWS_SECRET_ACCESS_KEY=...\nexport AWS_SESSION_TOKEN=...\n\n\n\n\n\n\n# List S3 buckets\naws s3 ls\n\n# List specific bucket\naws s3 ls s3://my-bucket-name/\n\n\n\nimport boto3\n\n# Using default credentials\ns3 = boto3.client('s3')\nresponse = s3.list_buckets()\nprint([bucket['Name'] for bucket in response['Buckets']])\n\n\n\nimport polars as pl\n\n# Test reading from private bucket\ndf = pl.read_parquet(\"s3://my-bucket/data.parquet\")\nprint(df.head())\n\n\n\n\n\n\n\nUse IAM roles when possible (EC2/ECS/Lambda)\nRotate access keys regularly\nUse least-privilege permissions\nEnable MFA for root and IAM users\nUse environment variables for CI/CD\nStore credentials in AWS Secrets Manager\n\n\n\n\n\nHardcode credentials in code\nCommit credentials to Git\nShare credentials between users\nUse root account access keys\nStore keys in plain text files\n\n\n\n\n\n\n\n# Check if credentials file exists\nls -la ~/.aws/credentials\n\n# Check environment variables\nenv | grep AWS\n\n\n\n# Verify identity\naws sts get-caller-identity\n\n# Check permissions\naws iam get-user\n\n# Test specific bucket\naws s3api head-bucket --bucket my-bucket-name\n\n\n\n# Reconfigure\naws configure\n\n# Or delete and recreate\naws iam delete-access-key --access-key-id OLD_KEY --user-name your-username\naws iam create-access-key --user-name your-username\n\n\n\n\n\n\n\nMethod\nBest For\nPersistence\n\n\n\n\naws configure\nLocal development\nPermanent\n\n\nEnvironment vars\nCI/CD, scripts\nSession only\n\n\nCredentials file\nMultiple profiles\nPermanent\n\n\nIAM roles\nAWS infrastructure\nAutomatic\n\n\nSession tokens\nMFA, temp access\nTemporary\n\n\n\n\n\n\nOnce configured, you can:\n# Download the OSM dataset\naws s3 cp s3://daylight-openstreetmap/parquet/osm_features/ ./osm_data/ --recursive\n\n# Or access private buckets\naws s3 ls s3://my-private-bucket/\n\n# Or use with Polars\npython3 -c \"import polars as pl; print(pl.read_parquet('s3://daylight-openstreetmap/parquet/osm_features/release=v1.58/type=way/20241112_191814_00139_grr7u_fea4d477-4748-4e7d-9aed-90290d792f01').head())\""
  },
  {
    "objectID": "AWS_CREDENTIALS_GUIDE.html#method-1-using-aws-cli-recommended",
    "href": "AWS_CREDENTIALS_GUIDE.html#method-1-using-aws-cli-recommended",
    "title": "AWS Credentials Configuration Guide",
    "section": "",
    "text": "# Ubuntu/Debian\nsudo apt-get update\nsudo apt-get install awscli\n\n# macOS\nbrew install awscli\n\n# Or download from AWS\n# https://aws.amazon.com/cli/\n\n\n\naws configure\nYou’ll be prompted for:\nAWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE\nAWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nDefault region name [None]: us-east-1\nDefault output format [None]: json\n\n\n\naws sts get-caller-identity\nOutput should show:\n{\n    \"UserId\": \"AIDACKCEVSQ6C2EXAMPLE\",\n    \"Account\": \"123456789012\",\n    \"Arn\": \"arn:aws:iam::123456789012:user/your-username\"\n}"
  },
  {
    "objectID": "AWS_CREDENTIALS_GUIDE.html#method-2-environment-variables",
    "href": "AWS_CREDENTIALS_GUIDE.html#method-2-environment-variables",
    "title": "AWS Credentials Configuration Guide",
    "section": "",
    "text": "Set temporarily in your shell:\nexport AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_DEFAULT_REGION=us-east-1\nOr add to ~/.bashrc or ~/.zshrc for persistence:\necho 'export AWS_ACCESS_KEY_ID=your_key' &gt;&gt; ~/.bashrc\necho 'export AWS_SECRET_ACCESS_KEY=your_secret' &gt;&gt; ~/.bashrc\nsource ~/.bashrc"
  },
  {
    "objectID": "AWS_CREDENTIALS_GUIDE.html#method-3-credentials-file",
    "href": "AWS_CREDENTIALS_GUIDE.html#method-3-credentials-file",
    "title": "AWS Credentials Configuration Guide",
    "section": "",
    "text": "Edit ~/.aws/credentials:\n[default]\naws_access_key_id = AKIAIOSFODNN7EXAMPLE\naws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\n[work]\naws_access_key_id = AKIAI44QH8DHBEXAMPLE\naws_secret_access_key = je7MtGbClwBF/2Zp9Utk/h3yCo8nvbEXAMPLEKEY\n\n[personal]\naws_access_key_id = AKIAIOSFODNN7EXAMPLE\naws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nEdit ~/.aws/config for profiles:\n[default]\nregion = us-east-1\noutput = json\n\n[profile work]\nregion = us-west-2\noutput = json\n\n[profile personal]\nregion = eu-west-1\noutput = yaml\n\n\n# Set default profile\nexport AWS_PROFILE=work\n\n# Or use with aws command\naws s3 ls --profile work\n\n# Or in Python/Polars\nimport polars as pl\ndf = pl.read_parquet(\"s3://bucket/file.parquet\", storage_options={\"profile\": \"work\"})"
  },
  {
    "objectID": "AWS_CREDENTIALS_GUIDE.html#method-4-iam-role-ec2ecslambda",
    "href": "AWS_CREDENTIALS_GUIDE.html#method-4-iam-role-ec2ecslambda",
    "title": "AWS Credentials Configuration Guide",
    "section": "",
    "text": "If running on AWS infrastructure, no configuration needed!\nimport polars as pl\n\n# Automatically uses instance role\ndf = pl.read_parquet(\"s3://my-private-bucket/data.parquet\")"
  },
  {
    "objectID": "AWS_CREDENTIALS_GUIDE.html#method-5-session-token-temporary-credentials",
    "href": "AWS_CREDENTIALS_GUIDE.html#method-5-session-token-temporary-credentials",
    "title": "AWS Credentials Configuration Guide",
    "section": "",
    "text": "For temporary credentials (MFA, assumed roles):\nexport AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_SESSION_TOKEN=IQoJb3JpZ2luX2VjEHYaCXVzLWVhc3QtMSJ..."
  },
  {
    "objectID": "AWS_CREDENTIALS_GUIDE.html#getting-your-aws-credentials",
    "href": "AWS_CREDENTIALS_GUIDE.html#getting-your-aws-credentials",
    "title": "AWS Credentials Configuration Guide",
    "section": "",
    "text": "Sign in to AWS Console\nClick your name (top right) → “Security Credentials”\nClick “Access keys” → “Create access key”\nDownload the .csv file (you won’t see the secret key again!)\n\n\n\n\n# Create access key\naws iam create-access-key --user-name your-username\n\n# List access keys\naws iam list-access-keys --user-name your-username\n\n\n\n# Assume a role\naws sts assume-role \\\n  --role-arn arn:aws:iam::123456789012:role/MyRole \\\n  --role-session-name MySession \\\n  --duration-seconds 3600\n\n# Set the temporary credentials from the output\nexport AWS_ACCESS_KEY_ID=...\nexport AWS_SECRET_ACCESS_KEY=...\nexport AWS_SESSION_TOKEN=..."
  },
  {
    "objectID": "AWS_CREDENTIALS_GUIDE.html#testing-your-credentials",
    "href": "AWS_CREDENTIALS_GUIDE.html#testing-your-credentials",
    "title": "AWS Credentials Configuration Guide",
    "section": "",
    "text": "# List S3 buckets\naws s3 ls\n\n# List specific bucket\naws s3 ls s3://my-bucket-name/\n\n\n\nimport boto3\n\n# Using default credentials\ns3 = boto3.client('s3')\nresponse = s3.list_buckets()\nprint([bucket['Name'] for bucket in response['Buckets']])\n\n\n\nimport polars as pl\n\n# Test reading from private bucket\ndf = pl.read_parquet(\"s3://my-bucket/data.parquet\")\nprint(df.head())"
  },
  {
    "objectID": "AWS_CREDENTIALS_GUIDE.html#security-best-practices",
    "href": "AWS_CREDENTIALS_GUIDE.html#security-best-practices",
    "title": "AWS Credentials Configuration Guide",
    "section": "",
    "text": "Use IAM roles when possible (EC2/ECS/Lambda)\nRotate access keys regularly\nUse least-privilege permissions\nEnable MFA for root and IAM users\nUse environment variables for CI/CD\nStore credentials in AWS Secrets Manager\n\n\n\n\n\nHardcode credentials in code\nCommit credentials to Git\nShare credentials between users\nUse root account access keys\nStore keys in plain text files"
  },
  {
    "objectID": "AWS_CREDENTIALS_GUIDE.html#troubleshooting",
    "href": "AWS_CREDENTIALS_GUIDE.html#troubleshooting",
    "title": "AWS Credentials Configuration Guide",
    "section": "",
    "text": "# Check if credentials file exists\nls -la ~/.aws/credentials\n\n# Check environment variables\nenv | grep AWS\n\n\n\n# Verify identity\naws sts get-caller-identity\n\n# Check permissions\naws iam get-user\n\n# Test specific bucket\naws s3api head-bucket --bucket my-bucket-name\n\n\n\n# Reconfigure\naws configure\n\n# Or delete and recreate\naws iam delete-access-key --access-key-id OLD_KEY --user-name your-username\naws iam create-access-key --user-name your-username"
  },
  {
    "objectID": "AWS_CREDENTIALS_GUIDE.html#quick-reference",
    "href": "AWS_CREDENTIALS_GUIDE.html#quick-reference",
    "title": "AWS Credentials Configuration Guide",
    "section": "",
    "text": "Method\nBest For\nPersistence\n\n\n\n\naws configure\nLocal development\nPermanent\n\n\nEnvironment vars\nCI/CD, scripts\nSession only\n\n\nCredentials file\nMultiple profiles\nPermanent\n\n\nIAM roles\nAWS infrastructure\nAutomatic\n\n\nSession tokens\nMFA, temp access\nTemporary"
  },
  {
    "objectID": "AWS_CREDENTIALS_GUIDE.html#next-steps",
    "href": "AWS_CREDENTIALS_GUIDE.html#next-steps",
    "title": "AWS Credentials Configuration Guide",
    "section": "",
    "text": "Once configured, you can:\n# Download the OSM dataset\naws s3 cp s3://daylight-openstreetmap/parquet/osm_features/ ./osm_data/ --recursive\n\n# Or access private buckets\naws s3 ls s3://my-private-bucket/\n\n# Or use with Polars\npython3 -c \"import polars as pl; print(pl.read_parquet('s3://daylight-openstreetmap/parquet/osm_features/release=v1.58/type=way/20241112_191814_00139_grr7u_fea4d477-4748-4e7d-9aed-90290d792f01').head())\""
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-03-day007-datetime.html",
    "href": "posts/100-days-of-polars/2026-02-03-day007-datetime.html",
    "title": "100 Days of Polars - Day 007: Date/Time — Parsing, Durations, and Time Zones",
    "section": "",
    "text": "Dates and times are everywhere in data: logs, events, financial records, and more. Today we’ll cover the most useful Polars techniques for parsing date/time strings, measuring durations, and dealing with time zones so your pipelines remain robust and correct.\n\n\n\nParsing dates and datetimes from strings\nExtracting components (year, month, hour, …)\nComputing durations and converting units\nAssigning and converting time zones"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#introduction",
    "href": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#introduction",
    "title": "100 Days of Polars - Day 007: Date/Time — Parsing, Durations, and Time Zones",
    "section": "",
    "text": "Dates and times are everywhere in data: logs, events, financial records, and more. Today we’ll cover the most useful Polars techniques for parsing date/time strings, measuring durations, and dealing with time zones so your pipelines remain robust and correct.\n\n\n\nParsing dates and datetimes from strings\nExtracting components (year, month, hour, …)\nComputing durations and converting units\nAssigning and converting time zones"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#parsing-dates-and-datetimes",
    "href": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#parsing-dates-and-datetimes",
    "title": "100 Days of Polars - Day 007: Date/Time — Parsing, Durations, and Time Zones",
    "section": "Parsing dates and datetimes",
    "text": "Parsing dates and datetimes\nStart by parsing strings into typed date/datetime columns using str.strptime.\nimport polars as pl\n\ndf = pl.DataFrame({\n    \"date_str\": [\"2026-02-01\", \"2026-02-02\"],\n    \"dt_str\": [\"2026-02-01 08:30:00\", \"2026-02-02 17:45:30\"]\n})\n\n# Parse date-only and full timestamps\ndf_parsed = df.with_columns([\n    pl.col(\"date_str\").str.strptime(pl.Date, \"%Y-%m-%d\").alias(\"date\"),\n    pl.col(\"dt_str\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\").alias(\"ts\")\n])\n\nprint(df_parsed)\nAfter parsing you can use the dt namespace to extract fields:\ndf_parsed.select([\n    pl.col(\"ts\").dt.year().alias(\"year\"),\n    pl.col(\"ts\").dt.month().alias(\"month\"),\n    pl.col(\"ts\").dt.hour().alias(\"hour\")\n])"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#computing-durations",
    "href": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#computing-durations",
    "title": "100 Days of Polars - Day 007: Date/Time — Parsing, Durations, and Time Zones",
    "section": "Computing durations",
    "text": "Computing durations\nFind differences between timestamp columns to get a duration value. Durations are typed, and you can convert them to numeric units by casting.\ndf_times = pl.DataFrame({\n    \"start\": [\"2026-02-01 08:00:00\", \"2026-02-02 09:15:00\"],\n    \"end\":   [\"2026-02-01 10:30:00\", \"2026-02-02 11:00:00\"]\n}).with_columns([\n    pl.col(\"start\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\").alias(\"start\"),\n    pl.col(\"end\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\").alias(\"end\")\n])\n\n# Duration (typed) and total seconds (numeric)\ndf_dur = df_times.with_columns([\n    (pl.col(\"end\") - pl.col(\"start\")).alias(\"duration\"),\n    ((pl.col(\"end\") - pl.col(\"start\")).cast(pl.Int64) / 1_000_000_000).alias(\"duration_seconds\")\n])\n\nprint(df_dur)\nNotes: - Casting a duration to Int64 yields the underlying integer (nanoseconds), so dividing by 1_000_000_000 gives seconds. - You can similarly compute minutes/hours by dividing by 60 or 3600."
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#unix-timestamps",
    "href": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#unix-timestamps",
    "title": "100 Days of Polars - Day 007: Date/Time — Parsing, Durations, and Time Zones",
    "section": "Unix timestamps",
    "text": "Unix timestamps\nWorking with Unix timestamps (epoch time) is common in APIs and databases. Polars provides from_epoch to convert these to datetimes:\ndf_unix = pl.DataFrame({\n    \"ts_sec\": [1706782800, 1706869200],\n    \"ts_ms\": [1706782800000, 1706869200000]\n})\n\n# Convert seconds and milliseconds to datetime\ndf_unix.with_columns([\n    pl.from_epoch(pl.col(\"ts_sec\"), time_unit=\"s\").alias(\"from_seconds\"),\n    pl.from_epoch(pl.col(\"ts_ms\"), time_unit=\"ms\").alias(\"from_milliseconds\")\n])\nSupported time units: s (seconds), ms (milliseconds), us (microseconds), ns (nanoseconds)."
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#date-truncation",
    "href": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#date-truncation",
    "title": "100 Days of Polars - Day 007: Date/Time — Parsing, Durations, and Time Zones",
    "section": "Date truncation",
    "text": "Date truncation\nGrouping by time periods is essential for time-series analysis. Use dt.truncate to round down to calendar periods:\ndf_ts = pl.DataFrame({\n    \"ts\": [\"2026-02-03 14:35:00\", \"2026-02-03 14:55:00\", \"2026-02-04 09:00:00\"]\n}).with_columns(\n    pl.col(\"ts\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\").alias(\"ts\")\n)\n\n# Truncate to different periods\ndf_ts.with_columns([\n    pl.col(\"ts\").dt.truncate(\"1h\").alias(\"hourly\"),\n    pl.col(\"ts\").dt.truncate(\"1d\").alias(\"daily\"),\n    pl.col(\"ts\").dt.truncate(\"1w\").alias(\"weekly\")\n])\nCommon truncation periods: 1h (hour), 1d (day), 1w (week), 1mo (month)."
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#adding-durations",
    "href": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#adding-durations",
    "title": "100 Days of Polars - Day 007: Date/Time — Parsing, Durations, and Time Zones",
    "section": "Adding durations",
    "text": "Adding durations\nAdd or subtract durations using pl.duration(...) literals or by building expressions:\ndf = pl.DataFrame({\"ts\": [\"2026-02-01 12:00:00\"]}).with_columns(\n    pl.col(\"ts\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\").alias(\"ts\")\n)\n\n# Add 90 minutes\ndf.with_columns(\n    (pl.col(\"ts\") + pl.duration(minutes=90)).alias(\"ts_plus_90m\")\n)\nIf you need rolling windows by time, you can use time-based grouping/aggregation with timestamps as the grouping key."
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#offset-arithmetic",
    "href": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#offset-arithmetic",
    "title": "100 Days of Polars - Day 007: Date/Time — Parsing, Durations, and Time Zones",
    "section": "Offset arithmetic",
    "text": "Offset arithmetic\nFor calendar-based shifts (not just duration), use dt.offset_by which handles variable-length periods like months and leap years correctly:\ndf = pl.DataFrame({\"ts\": [\"2026-01-31 12:00:00\"]}).with_columns(\n    pl.col(\"ts\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\").alias(\"ts\")\n)\n\n# Add calendar periods\ndf.with_columns([\n    pl.col(\"ts\").dt.offset_by(\"1d\").alias(\"plus_1_day\"),\n    pl.col(\"ts\").dt.offset_by(\"1mo\").alias(\"plus_1_month\"),  # Handles Feb 28\n    pl.col(\"ts\").dt.offset_by(\"-1w\").alias(\"minus_1_week\")\n])\nOffset strings: d (days), w (weeks), mo (months), y (years), h (hours), m (minutes), s (seconds)."
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#filtering-by-date-range",
    "href": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#filtering-by-date-range",
    "title": "100 Days of Polars - Day 007: Date/Time — Parsing, Durations, and Time Zones",
    "section": "Filtering by date range",
    "text": "Filtering by date range\nUse is_between for efficient date range filtering:\nstart = pl.lit(datetime(2026, 2, 1))\nend = pl.lit(datetime(2026, 2, 28))\n\ndf.filter(pl.col(\"ts\").is_between(start, end))"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#time-zones-assign-vs-convert",
    "href": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#time-zones-assign-vs-convert",
    "title": "100 Days of Polars - Day 007: Date/Time — Parsing, Durations, and Time Zones",
    "section": "Time zones: assign vs convert",
    "text": "Time zones: assign vs convert\nTime zones are subtle but important: - Assigning a time zone interprets a naive timestamp as being in that zone (no clock shift). - Converting a time zone shifts the instant to represent the same absolute time in a different zone.\nExample:\ndf_tz = pl.DataFrame({\n    \"ts\": [\"2026-02-01 12:00:00\", \"2026-06-01 12:00:00\"]\n}).with_columns(\n    pl.col(\"ts\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\").alias(\"ts\")\n)\n\n# Interpret naive timestamps as UTC (assign tz)\ndf_tz = df_tz.with_columns(pl.col(\"ts\").dt.replace_time_zone(\"UTC\").alias(\"ts_utc\"))\n\n# Convert to America/Los_Angeles (shifts the clock)\ndf_tz = df_tz.with_columns(pl.col(\"ts_utc\").dt.convert_time_zone(\"America/Los_Angeles\").alias(\"ts_pt\"))\n\nprint(df_tz)\nWhen ingesting logs from multiple services, ensure you either normalize timestamps to UTC or carry the timezone information so downstream aggregations align."
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#common-pitfalls",
    "href": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#common-pitfalls",
    "title": "100 Days of Polars - Day 007: Date/Time — Parsing, Durations, and Time Zones",
    "section": "Common pitfalls",
    "text": "Common pitfalls\n\nParsing with the wrong format string will produce nulls; use strict=False if your data is noisy.\nBeware DST transitions when summarizing daily/hourly counts; converting to local time can cause duplicated or missing clock times.\nFloating math on durations requires attention to units (ns → s → hours)."
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#practice-exercise",
    "href": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#practice-exercise",
    "title": "100 Days of Polars - Day 007: Date/Time — Parsing, Durations, and Time Zones",
    "section": "Practice Exercise",
    "text": "Practice Exercise\nScenario: You have event data with a naive timestamp and a separate timezone field. Do the following:\n\nParse the timestamp as naive Datetime.\nAssign the timezone from the tz column (per-row assignment).\nConvert all timestamps to UTC.\nCompute event duration from start to end in minutes.\n\nStarter data:\nevents = pl.DataFrame({\n    \"event_id\": [1, 2],\n    \"start\": [\"2026-02-01 08:00:00\", \"2026-06-01 09:30:00\"],\n    \"end\":   [\"2026-02-01 10:15:00\", \"2026-06-01 10:00:00\"],\n    \"tz\": [\"Europe/Paris\", \"America/Los_Angeles\"]\n})\n\n\nClick to see solution\n\n# Solution (one approach)\nevents_parsed = events.with_columns([\n    pl.col(\"start\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\").alias(\"start\"),\n    pl.col(\"end\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\").alias(\"end\")\n])\n\n# Assign tz per-row using map_elements, then convert to UTC\nevents_tz = events_parsed.with_columns([\n    pl.struct([\"start\", \"tz\"]).map_elements(\n        lambda x: x[\"start\"].dt.replace_time_zone(x[\"tz\"])\n    ).alias(\"start_tz\"),\n    pl.struct([\"end\", \"tz\"]).map_elements(\n        lambda x: x[\"end\"].dt.replace_time_zone(x[\"tz\"])\n    ).alias(\"end_tz\")\n]).with_columns([\n    pl.col(\"start_tz\").dt.convert_time_zone(\"UTC\").alias(\"start_utc\"),\n    pl.col(\"end_tz\").dt.convert_time_zone(\"UTC\").alias(\"end_utc\")\n])\n\n# Duration in minutes\nevents_final = events_tz.with_columns(\n    ((pl.col(\"end_utc\") - pl.col(\"start_utc\")).cast(pl.Int64) / 1_000_000_000 / 60).alias(\"duration_minutes\")\n)\n\nprint(events_final.select([\"event_id\", \"duration_minutes\"]))"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#resources",
    "href": "posts/100-days-of-polars/2026-02-03-day007-datetime.html#resources",
    "title": "100 Days of Polars - Day 007: Date/Time — Parsing, Durations, and Time Zones",
    "section": "Resources",
    "text": "Resources\n\nPolars date/time docs\nPython tz database (IANA zones)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-18-day002-window-function.html",
    "href": "posts/100-days-of-polars/2026-01-18-day002-window-function.html",
    "title": "100 Days of Polars - Day 002: Window Functions",
    "section": "",
    "text": "Window functions are one of the most powerful features in Polars for performing calculations across rows in a group. Unlike aggregations group_by that collapse rows into single values, window functions maintain all rows while computing values based on a “window” of data.\nWindow function allows you to:\n\nCalculate running totals and moving averages\nRank rows within groups\nCompare values with group averages\nPerform time-based calculations\n\nWindow function can be used through .over() in polars. You can achive the result as window function with group_by and join in polars.\nimport polars as pl\nfrom polars import window_function as wf\n\ndf = pl.DataFrame({\n    \"department\": [\"Sales\", \"Sales\", \"Sales\", \"Engineering\", \"Engineering\", \"Engineering\"],\n    \"employee\": [\"Alice\", \"Bob\", \"Carol\", \"David\", \"Eve\", \"Frank\"],\n    \"salary\": [50000, 60000, 55000, 75000, 80000, 72000]\n})\n\n# Add a column showing each employee's salary compared to their department's average.\ndf.with_columns(\n    dept_avg_salary=pl.col(\"salary\").mean().over(\"department\")\n)\n\n# Equivalent with group_by and join\ndept_avg = df.group_by(\"department\").agg(\n    pl.col(\"salary\").mean().alias(\"dept_avg_salary\")\n)\ndf.join(dept_avg, on=\"department\")"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#introduction-what-are-window-functions",
    "href": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#introduction-what-are-window-functions",
    "title": "100 Days of Polars - Day 002: Window Functions",
    "section": "",
    "text": "Window functions are one of the most powerful features in Polars for performing calculations across rows in a group. Unlike aggregations group_by that collapse rows into single values, window functions maintain all rows while computing values based on a “window” of data.\nWindow function allows you to:\n\nCalculate running totals and moving averages\nRank rows within groups\nCompare values with group averages\nPerform time-based calculations\n\nWindow function can be used through .over() in polars. You can achive the result as window function with group_by and join in polars.\nimport polars as pl\nfrom polars import window_function as wf\n\ndf = pl.DataFrame({\n    \"department\": [\"Sales\", \"Sales\", \"Sales\", \"Engineering\", \"Engineering\", \"Engineering\"],\n    \"employee\": [\"Alice\", \"Bob\", \"Carol\", \"David\", \"Eve\", \"Frank\"],\n    \"salary\": [50000, 60000, 55000, 75000, 80000, 72000]\n})\n\n# Add a column showing each employee's salary compared to their department's average.\ndf.with_columns(\n    dept_avg_salary=pl.col(\"salary\").mean().over(\"department\")\n)\n\n# Equivalent with group_by and join\ndept_avg = df.group_by(\"department\").agg(\n    pl.col(\"salary\").mean().alias(\"dept_avg_salary\")\n)\ndf.join(dept_avg, on=\"department\")"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#common-window-functions",
    "href": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#common-window-functions",
    "title": "100 Days of Polars - Day 002: Window Functions",
    "section": "Common Window Functions",
    "text": "Common Window Functions\n\nAggregations\nAll standard aggregation functions work as window functions, aggregation functions must come before window functions .over:\ndf.with_columns(\n    dept_salary_sum=pl.col(\"salary\").sum().over(\"department\"),\n    dept_salary_min=pl.col(\"salary\").min().over(\"department\"),\n    dept_salary_max=pl.col(\"salary\").max().over(\"department\"),\n    dept_salary_count=pl.col(\"salary\").count().over(\"department\")\n)\n\n\nCumulative Sums\ndf.sort(\"salary\").with_columns(\n    running_total=pl.col(\"salary\").cum_sum().over(\"department\")\n)\n\n\nRanking\ndf.with_columns(\n    salary_rank=pl.col(\"salary\").rank().over(\"department\")\n)\nAvailable ranking methods: - \"ordinal\" - sequential integers (1, 2, 3…) - \"dense\" - dense ranking without gaps - \"min\", \"max\", \"avg\" - various average ranking methods\n\n\nFirst and Last Values\ndf.sort(\"salary\").with_columns(\n    lowest_in_dept=pl.col(\"salary\").first().over(\"department\"),\n    highest_in_dept=pl.col(\"salary\").last().over(\"department\")\n)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#practice-exercise",
    "href": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#practice-exercise",
    "title": "100 Days of Polars - Day 002: Window Functions",
    "section": "Practice Exercise",
    "text": "Practice Exercise\nScenario: You have a sales dataset with regional sales data:\nimport polars as pl\nfrom datetime import datetime\n\nsales_df = pl.DataFrame({\n    \"region\": [\"North\", \"North\", \"North\", \"South\", \"South\", \"South\", \"East\", \"East\"],\n    \"salesperson\": [\"Alice\", \"Bob\", \"Carol\", \"David\", \"Eve\", \"Frank\", \"Grace\", \"Henry\"],\n    \"quarter\": [\"Q1\", \"Q1\", \"Q2\", \"Q1\", \"Q2\", \"Q2\", \"Q1\", \"Q2\"],\n    \"sales\": [10000, 15000, 12000, 20000, 18000, 22000, 14000, 16000]\n})\nTasks:\n\nCalculate each salesperson’s sales as a percentage of their region’s total\nRank salespeople within each region by sales amount\nCalculate running total of sales within each region (sorted by sales amount)\nFind the top performer in each region and quarter\n\n\n\nClick to see solutions\n\n# Task 1: Percentage of regional total\nsales_df.with_columns(\n    region_total=pl.col(\"sales\").sum().over(\"region\"),\n    pct_of_region=(pl.col(\"sales\") / pl.col(\"region_total\")) * 100\n).drop(\"region_total\")\n\n# Task 2: Rank within region\nsales_df.with_columns(\n    rank_in_region=pl.col(\"sales\").rank(\"ordinal\", descending=True).over(\"region\")\n)\n\n# Task 3: Running total within region\nsales_df.sort(\"sales\", descending=True).with_columns(\n    running_total=pl.col(\"sales\").cum_sum().over(\"region\")\n)\n\n# Task 4: Top performer in each region and quarter\nsales_df.with_columns(\n    max_sales=pl.col(\"sales\").max().over(\"region\", \"quarter\")\n).filter(pl.col(\"sales\") == pl.col(\"max_sales\")).drop(\"max_sales\")"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#resources",
    "href": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#resources",
    "title": "100 Days of Polars - Day 002: Window Functions",
    "section": "Resources",
    "text": "Resources\n\nPolars Documentation on Window Functions\nPolars Window Function API"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "",
    "text": "Data integrity is the foundation of reliable data pipelines. Without proper validation and null handling, your analyses can produce misleading results or fail entirely. In Polars, you have powerful tools to ensure your data meets quality standards before processing.\nToday we’ll cover: - Null value handling: Detection, filling, and removal strategies - Schema validation: Ensuring data types match expectations - Data quality checks: Comprehensive validation patterns"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#introduction",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#introduction",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "",
    "text": "Data integrity is the foundation of reliable data pipelines. Without proper validation and null handling, your analyses can produce misleading results or fail entirely. In Polars, you have powerful tools to ensure your data meets quality standards before processing.\nToday we’ll cover: - Null value handling: Detection, filling, and removal strategies - Schema validation: Ensuring data types match expectations - Data quality checks: Comprehensive validation patterns"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#understanding-nulls-in-polars",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#understanding-nulls-in-polars",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Understanding Nulls in Polars",
    "text": "Understanding Nulls in Polars\nPolars uses null to represent missing values, distinct from NaN (not-a-number for floats) and None.\nimport polars as pl\nimport numpy as np\n\n# Create a DataFrame with various null-like values\ndf = pl.DataFrame({\n    \"id\": [1, 2, 3, 4, 5],\n    \"name\": [\"Alice\", \"Bob\", None, \"David\", \"Eve\"],\n    \"age\": [25, None, 30, 35, None],\n    \"score\": [85.5, 90.0, np.nan, 78.0, 92.5]\n})\n\nprint(df)\nshape: (5, 4)\n┌─────┬────────┬──────┬───────┐\n│ id  ┆ name   ┆ age  ┆ score │\n│ --- ┆ ---    ┆ ---  ┆ ---   │\n│ i64 ┆ str    ┆ i64  ┆ f64   │\n╞═════╪════════╪══════╪═══════╡\n│ 1   ┆ Alice  ┆ 25   ┆ 85.5  │\n│ 2   ┆ Bob    ┆ null ┆ 90.0  │\n│ 3   ┆ null   ┆ 30   ┆ NaN   │\n│ 4   ┆ David  ┆ 35   ┆ 78.0  │\n│ 5   ┆ Eve    ┆ null ┆ 92.5  │\n└─────┴────────┴──────┴───────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#detecting-null-values",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#detecting-null-values",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Detecting Null Values",
    "text": "Detecting Null Values\n\nBasic Null Detection\n# Check for nulls in each column\nnull_counts = df.null_count()\nprint(null_counts)\n\n# Check if any nulls exist in the entire DataFrame\nhas_nulls = df.null_count().sum_horizontal().max() &gt; 0\nprint(f\"DataFrame has nulls: {has_nulls}\")\n\n\nColumn-Specific Null Checks\n# Count nulls per column\nfor col in df.columns:\n    null_count = df[col].is_null().sum()\n    print(f\"{col}: {null_count} nulls\")\n\n# Alternative: Get null percentage\nnull_pct = df.null_count() / df.height * 100\nprint(null_pct)\n\n\nFinding Rows with Nulls\n# Find rows with any null values\ndf_with_nulls = df.filter(\n    pl.any_horizontal(pl.all().is_null())\n)\nprint(df_with_nulls)\n\n# Find rows where specific columns have nulls\ndf_name_or_age_null = df.filter(\n    pl.col(\"name\").is_null() | pl.col(\"age\").is_null()\n)\nprint(df_name_or_age_null)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#handling-null-values",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#handling-null-values",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Handling Null Values",
    "text": "Handling Null Values\n\nStrategy 1: Drop Nulls\n# Drop rows with ANY null values\ndf_clean = df.drop_nulls()\n\n# Drop rows with nulls in specific columns\ndf_clean_names = df.drop_nulls(subset=[\"name\"])\n\n# Drop columns that contain any nulls\ndf_no_null_cols = df.select([\n    col for col in df.columns \n    if df[col].null_count() == 0\n])\n\n\nStrategy 2: Fill Nulls\n# Fill with a constant value\ndf_filled = df.with_columns(\n    pl.col(\"age\").fill_null(0),\n    pl.col(\"name\").fill_null(\"Unknown\")\n)\n\n# Fill with forward fill (previous value)\ndf_ffill = df.with_columns(\n    pl.col(\"age\").fill_null(strategy=\"forward\")\n)\n\n# Fill with backward fill (next value)\ndf_bfill = df.with_columns(\n    pl.col(\"age\").fill_null(strategy=\"backward\")\n)\n\n# Fill with mean (for numeric columns)\ndf_mean_filled = df.with_columns(\n    pl.col(\"age\").fill_null(pl.col(\"age\").mean())\n)\n\n# Fill with median\ndf_median_filled = df.with_columns(\n    pl.col(\"age\").fill_null(pl.col(\"age\").median())\n)\n\n# Fill with interpolation (linear)\ndf_interpolated = df.with_columns(\n    pl.col(\"age\").interpolate()\n)\n\n\nStrategy 3: Conditional Filling\n# Fill based on conditions\ndf_conditional = df.with_columns(\n    pl.when(pl.col(\"age\").is_null())\n    .then(18)  # Default age if missing\n    .otherwise(pl.col(\"age\"))\n    .alias(\"age_filled\")\n)\n\n# Fill based on group statistics\ndf_grouped_fill = df.with_columns(\n    pl.col(\"age\").fill_null(\n        pl.col(\"age\").mean().over(\"department\")  # If you have a department column\n    )\n)\n\n\nHandling NaN Values\n# Detect NaN (different from null)\ndf_with_nan_check = df.with_columns(\n    pl.col(\"score\").is_nan().alias(\"is_nan\")\n)\n\n# Replace NaN with null first, then handle\ndf_no_nan = df.with_columns(\n    pl.col(\"score\").replace({float(\"nan\"): None})\n)\n\n# Or fill NaN directly\ndf_filled_nan = df.with_columns(\n    pl.when(pl.col(\"score\").is_nan())\n    .then(0.0)\n    .otherwise(pl.col(\"score\"))\n)\n\ndf_filled_nan = df.fill_nan(0.0)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#schema-validation",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#schema-validation",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Schema Validation",
    "text": "Schema Validation\n\nUnderstanding Polars Schemas\n# Check current schema\nprint(df.schema)\n\n# Get detailed schema information\nfor name, dtype in df.schema.items():\n    print(f\"{name}: {dtype}\")\n\n\nStrict Schema Definition\nfrom typing import Dict\n\n# Define expected schema\nexpected_schema: Dict[str, pl.DataType] = {\n    \"id\": pl.Int64,\n    \"name\": pl.Utf8,\n    \"age\": pl.Int64,\n    \"score\": pl.Float64,\n    \"is_active\": pl.Boolean\n}\n\ndef validate_schema(df: pl.DataFrame, expected: Dict[str, pl.DataType]) -&gt; bool:\n    \"\"\"Validate that DataFrame matches expected schema.\"\"\"\n    actual_schema = df.schema\n    \n    # Check all expected columns exist\n    for col, dtype in expected.items():\n        if col not in actual_schema:\n            print(f\"Missing column: {col}\")\n            return False\n        if actual_schema[col] != dtype:\n            print(f\"Type mismatch for {col}: expected {dtype}, got {actual_schema[col]}\")\n            return False\n    \n    return True\n\n# Use the validator\nis_valid = validate_schema(df, expected_schema)\nprint(f\"Schema valid: {is_valid}\")\n\n\nSchema Enforcement\ndef enforce_schema(df: pl.DataFrame, schema: Dict[str, pl.DataType]) -&gt; pl.DataFrame:\n    \"\"\"Cast columns to expected types, creating missing columns with nulls.\"\"\"\n    columns = []\n    \n    for col_name, dtype in schema.items():\n        if col_name in df.columns:\n            # Cast existing column to expected type\n            columns.append(pl.col(col_name).cast(dtype))\n        else:\n            # Create column with nulls of expected type\n            columns.append(pl.lit(None).cast(dtype).alias(col_name))\n    \n    return df.select(columns)\n\n# Apply schema enforcement\ndf_enforced = enforce_schema(df, expected_schema)\nprint(df_enforced)\nprint(df_enforced.schema)\n\n\nType Coercion with Error Handling\n# Safe casting that handles errors\ndef safe_cast(df: pl.DataFrame, column: str, dtype: pl.DataType, \n              default=None) -&gt; pl.DataFrame:\n    \"\"\"Attempt to cast column, filling errors with default value.\"\"\"\n    try:\n        return df.with_columns(\n            pl.col(column).cast(dtype, strict=False)\n        )\n    except Exception as e:\n        print(f\"Error casting {column}: {e}\")\n        if default is not None:\n            return df.with_columns(\n                pl.lit(default).cast(dtype).alias(column)\n            )\n        return df\n\n# Example: Cast age to Int32 with fallback\ndf_casted = safe_cast(df, \"age\", pl.Int32, default=0)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#comprehensive-data-quality-checks",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#comprehensive-data-quality-checks",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Comprehensive Data Quality Checks",
    "text": "Comprehensive Data Quality Checks\n\nUniqueness Constraints\n# Check for duplicates\ndef check_uniqueness(df: pl.DataFrame, columns: list) -&gt; bool:\n    \"\"\"Check if specified columns have unique values.\"\"\"\n    unique_count = df.select(columns).n_unique()\n    total_count = df.height\n    \n    is_unique = unique_count == total_count\n    if not is_unique:\n        duplicates = total_count - unique_count\n        print(f\"Found {duplicates} duplicate rows in columns {columns}\")\n    \n    return is_unique\n\n# Check id column is unique\nis_id_unique = check_uniqueness(df, [\"id\"])\n\n# Find duplicates\nduplicates = df.filter(\n    pl.col(\"id\").is_duplicated()\n)\nprint(duplicates)\n\n\nPattern Validation (Strings)\n# Validate email format\ndef validate_emails(df: pl.DataFrame, email_col: str) -&gt; pl.DataFrame:\n    email_pattern = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n    \n    return df.with_columns(\n        pl.col(email_col)\n        .str.contains(email_pattern)\n        .alias(f\"{email_col}_valid\")\n    )\n\n# Create sample data with emails\nemail_df = pl.DataFrame({\n    \"id\": [1, 2, 3],\n    \"email\": [\"alice@example.com\", \"bob@invalid\", \"charlie@company.org\"]\n})\n\nemail_validated = validate_emails(email_df, \"email\")\nprint(email_validated)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#complete-data-validation-pipeline",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#complete-data-validation-pipeline",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Complete Data Validation Pipeline",
    "text": "Complete Data Validation Pipeline\nclass DataValidator:\n    \"\"\"Comprehensive data validation for Polars DataFrames.\"\"\"\n    \n    def __init__(self, schema: Dict[str, pl.DataType]):\n        self.schema = schema\n        self.errors = []\n    \n    def validate(self, df: pl.DataFrame) -&gt; tuple[bool, pl.DataFrame]:\n        \"\"\"Run all validations and return (is_valid, validated_df).\"\"\"\n        self.errors = []\n        \n        # Step 1: Schema validation\n        if not self._validate_schema(df):\n            return False, df\n        \n        # Step 2: Enforce schema\n        df = self._enforce_schema(df)\n        \n        # Step 3: Check null thresholds\n        self._check_nulls(df)\n        \n        # Step 4: Check duplicates\n        self._check_duplicates(df)\n        \n        is_valid = len(self.errors) == 0\n        return is_valid, df\n    \n    def _validate_schema(self, df: pl.DataFrame) -&gt; bool:\n        for col, dtype in self.schema.items():\n            if col not in df.columns:\n                self.errors.append(f\"Missing required column: {col}\")\n        return len(self.errors) == 0\n    \n    def _enforce_schema(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n        columns = []\n        for col, dtype in self.schema.items():\n            if col in df.columns:\n                columns.append(pl.col(col).cast(dtype, strict=False))\n            else:\n                columns.append(pl.lit(None).cast(dtype).alias(col))\n        return df.select(columns)\n    \n    def _check_nulls(self, df: pl.DataFrame, threshold: float = 0.1):\n        for col in df.columns:\n            null_pct = df[col].is_null().sum() / df.height\n            if null_pct &gt; threshold:\n                self.errors.append(f\"{col}: {null_pct:.1%} nulls exceeds threshold\")\n    \n    def _check_duplicates(self, df: pl.DataFrame):\n        dups = df.height - df.n_unique()\n        if dups &gt; 0:\n            self.errors.append(f\"Found {dups} duplicate rows\")\n    \n    def get_errors(self) -&gt; list:\n        return self.errors\n\n# Usage example\nvalidator = DataValidator(expected_schema)\nis_valid, validated_df = validator.validate(df)\n\nif not is_valid:\n    print(\"Validation errors:\")\n    for error in validator.get_errors():\n        print(f\"  - {error}\")\nelse:\n    print(\"Data validation passed!\")\n    print(validated_df)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#best-practices-for-data-integrity",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#best-practices-for-data-integrity",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Best Practices for Data Integrity",
    "text": "Best Practices for Data Integrity\n\n1. Always Validate at Ingestion\ndef load_and_validate_data(path: str, validator: DataValidator) -&gt; pl.DataFrame:\n    \"\"\"Load data with validation.\"\"\"\n    df = pl.read_csv(path)\n    \n    is_valid, df = validator.validate(df)\n    if not is_valid:\n        raise ValueError(f\"Data validation failed: {validator.get_errors()}\")\n    \n    return df"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#practice-exercise",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#practice-exercise",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Practice Exercise",
    "text": "Practice Exercise\nScenario: You’re building a data pipeline for customer information:\nimport polars as pl\n\n# Sample data with quality issues\ncustomers = pl.DataFrame({\n    \"customer_id\": [1, 2, 3, 4, 4, 6],  # Duplicate ID (4)\n    \"name\": [\"Alice\", None, \"Charlie\", \"David\", \"Eve\", \"Frank\"],\n    \"email\": [\"alice@example.com\", \"bob@invalid\", None, \"david@test.com\", \n              \"eve@company.org\", \"frank@example.com\"],\n    \"age\": [25, 150, 30, -5, 35, None],  # Invalid ages (150, -5)\n    \"registration_date\": [\"2023-01-15\", \"invalid_date\", \"2023-03-20\",\n                          None, \"2023-05-10\", \"2023-06-01\"],\n    \"is_premium\": [True, False, True, None, False, True]\n})\nTasks:\n\nDefine a proper schema for this data with appropriate types\nDetect and report all null values\nFix the duplicate customer_id (keep the first occurrence)\nValidate and clean the age column (valid range: 18-100)\nValidate email format for non-null emails\nParse registration_date as Date type, handling errors gracefully\nGenerate a data quality report after cleaning\n\nBonus Challenge:\nCreate a validation pipeline that: - Rejects rows with invalid emails - Fills missing ages with the median age - Drops rows with duplicate IDs (keep first) - Converts registration_date to Date type or null if invalid\n\n\nClick to see solutions\n\nimport polars as pl\nfrom datetime import datetime\n\n# Task 1: Define schema\nexpected_schema = {\n    \"customer_id\": pl.Int64,\n    \"name\": pl.Utf8,\n    \"email\": pl.Utf8,\n    \"age\": pl.Int64,\n    \"registration_date\": pl.Date,\n    \"is_premium\": pl.Boolean\n}\n\n# Task 2: Detect nulls\nprint(\"Null counts:\")\nfor col in customers.columns:\n    null_count = customers[col].null_count()\n    print(f\"  {col}: {null_count}\")\n\n# Task 3: Remove duplicates (keep first)\ncustomers_clean = customers.unique(subset=[\"customer_id\"], keep=\"first\")\n\n# Task 4: Validate and clean age\n# First, see invalid ages\ninvalid_ages = customers_clean.filter(\n    (pl.col(\"age\") &lt; 18) | (pl.col(\"age\") &gt; 100)\n)\nprint(f\"\\nInvalid ages: {invalid_ages.height} rows\")\n\n# Clean ages: set invalid to null, then fill with median\nvalid_ages = customers_clean.with_columns(\n    pl.when((pl.col(\"age\") &gt;= 18) & (pl.col(\"age\") &lt;= 100))\n    .then(pl.col(\"age\"))\n    .otherwise(None)\n    .alias(\"age\")\n)\nmedian_age = valid_ages[\"age\"].median()\ncustomers_clean = valid_ages.with_columns(\n    pl.col(\"age\").fill_null(int(median_age))\n)\n\n# Task 5: Validate email format\nemail_pattern = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\ncustomers_clean = customers_clean.with_columns(\n    pl.when(pl.col(\"email\").is_null())\n    .then(None)\n    .when(pl.col(\"email\").str.contains(email_pattern))\n    .then(pl.col(\"email\"))\n    .otherwise(None)\n    .alias(\"email\")\n)\n\n# Task 6: Parse dates\ncustomers_clean = customers_clean.with_columns(\n    pl.col(\"registration_date\").str.to_date(format=\"%Y-%m-%d\", strict=False)\n)\n\n# Task 7: Generate report\ndef generate_report(df):\n    return {\n        \"shape\": (df.height, df.width),\n        \"null_percentages\": {\n            col: df[col].null_count() / df.height \n            for col in df.columns\n        },\n        \"duplicates\": df.height - df.n_unique()\n    }\n\nreport = generate_report(customers_clean)\nprint(\"\\nFinal Data Quality Report:\")\nprint(f\"  Shape: {report['shape']}\")\nprint(f\"  Duplicates: {report['duplicates']}\")\nprint(\"  Null Percentages:\")\nfor col, pct in report['null_percentages'].items():\n    print(f\"    {col}: {pct:.1%}\")\n\nprint(\"\\nCleaned DataFrame:\")\nprint(customers_clean)\n\n# Bonus: Complete validation pipeline\ndef validate_customers(df: pl.DataFrame) -&gt; pl.DataFrame:\n    # 1. Remove duplicates\n    df = df.unique(subset=[\"customer_id\"], keep=\"first\")\n    \n    # 2. Clean and fill ages\n    df = df.with_columns(\n        pl.when((pl.col(\"age\") &gt;= 18) & (pl.col(\"age\") &lt;= 100))\n        .then(pl.col(\"age\"))\n        .otherwise(None)\n        .alias(\"age\")\n    )\n    median_age = int(df[\"age\"].median())\n    df = df.with_columns(pl.col(\"age\").fill_null(median_age))\n    \n    # 3. Validate emails (set invalid to null)\n    email_pattern = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n    df = df.with_columns(\n        pl.when(pl.col(\"email\").str.contains(email_pattern))\n        .then(pl.col(\"email\"))\n        .otherwise(None)\n    )\n    \n    # 4. Parse dates\n    df = df.with_columns(\n        pl.col(\"registration_date\").str.to_date(format=\"%Y-%m-%d\", strict=False)\n    )\n    \n    # 5. Fill missing names\n    df = df.with_columns(pl.col(\"name\").fill_null(\"Unknown\"))\n    \n    # 6. Fill missing premium status\n    df = df.with_columns(pl.col(\"is_premium\").fill_null(False))\n    \n    return df\n\nfinal_df = validate_customers(customers)\nprint(\"\\nBonus - Validated DataFrame:\")\nprint(final_df)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#resources",
    "href": "posts/100-days-of-polars/2026-02-02-day006-data-integrity.html#resources",
    "title": "100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation",
    "section": "Resources",
    "text": "Resources\n\nPolars Null Handling Documentation\nData Types and Schema\nCasting and Type Conversion\nData Validation Best Practices"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "",
    "text": "Today we will explore what polars selectors are, and what we can do with them."
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#introduction",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#introduction",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "",
    "text": "Today we will explore what polars selectors are, and what we can do with them."
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#what-are-polars-selectors",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#what-are-polars-selectors",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "What are Polars Selectors?",
    "text": "What are Polars Selectors?\nSelectors in polars are tools that you can use to select columns based on their properties, data types or patterns. For example if you want to select all string/numeric columns in your data frames:\nimport polars as pl\nimport polars.selectors as cs\n\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"test_score\": [88, 92],\n    \"final_score\": [95, 89],\n    \"category\": [\"A\", \"B\"],\n    \"updated_at\": [None, None]\n})\n\n# select all string columns\ndf.select(cs.string())\n\n# select all numeric columns\ndf.select(cs.numeric())\nThere are mainly three types of selectors in polars, 1. type based - you select columns based on data type; 2. pattern based - select columns based on pattern matching; 3. set logic selectors - combing multiple selectors together.\nWe will look at them by category"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#type-based-selectors",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#type-based-selectors",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "Type Based Selectors",
    "text": "Type Based Selectors\n\ncs.numeric()\n\n\ncs.string()\n\n\ncs.temporal()\nSelector targets columns with time-based data type.\nimport polars as pl\nimport polars.selectors as cs\nfrom datetime import datetime\n\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"transaction_date\": [datetime(2023, 5, 12), datetime(2023, 6, 15)],\n    \"upload_at\": [datetime(2023, 5, 13, 10, 0), datetime(2023, 6, 16, 11, 0)],\n    \"amount\": [100.0, 200.0]\n})\n\n# Use cs.temporal() to apply a date operation to all time columns\nresult = df.with_columns(\n    cs.temporal().dt.month_start()\n)\n\n\ncs.by_name()\n\nSelecting columns by name\n\nimport polars as pl\nimport polars.selectors as cs\n\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"test_score\": [88, 92],\n    \"final_score\": [95, 89],\n    \"category\": [\"A\", \"B\"],\n    \"updated_at\": [None, None]\n})\n\n# Select specific columns by exact name\ndf.select(cs.by_name(\"id\", \"category\"))\n\n\ncs.by_dtype()\n\nSelecting columns by data type"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#pattern-based-selectors",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#pattern-based-selectors",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "Pattern-based Selectors",
    "text": "Pattern-based Selectors\n\ncs.contains()\ncs.contains(\"score\")\n\n\ncs.matches()\n\nAllow to use regex patterns\n\nimport polars as pl\nimport polars.selectors as cs\n\ndf = pl.DataFrame({\n    \"abc_123\": [1],\n    \"abc_456\": [2],\n    \"xyz_123\": [3],\n    \"id_primary\": [4],\n    \"id_secondary\": [5]\n})\n# Select columns that start with 3 letters, an underscore, and then numbers\n# Pattern: ^[a-z]{3}_\\d+$\nresult = df.select(\n    cs.matches(r\"^[a-z]{3}_\\d+$\")\n)\n\n\ncs.starts_with()\ncs.starts_with(\"sale_\")\n\n\ncs.ends_with()\ncs.ends_with(\"sum\")"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#combining-selectors",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#combining-selectors",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "Combining Selectors",
    "text": "Combining Selectors\n\nSet Operations\n\nUnion (|)\n\ntarget_cols = cs.starts_with(\"sale_\") | cs.numeric()\n\ndf.select(target_cols)\n\nIntersection (&)\n\ntarget_cols = cs.starts_with(\"sale_\") & cs.numeric()\n\ndf.select(target_cols)\n\nDifference (-): used to exclude some columns\n\n# Logic: All Numerics MINUS the columns we want to protect\nfeatures = cs.numeric() - cs.by_name(\"user_id\", \"target_label\")\n\ndf.with_columns(\n    features.standardize()\n)\n\nComplement (~)\n\ndf.select(~cs.string())"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#practice-exercise",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#practice-exercise",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "Practice Exercise",
    "text": "Practice Exercise\nNow it’s time to practice! Try solving this exercise using selectors:\nScenario: You have a sales dataset with the following structure:\nimport polars as pl\nimport polars.selectors as cs\nfrom datetime import datetime\n\nsales_df = pl.DataFrame({\n    \"order_id\": [1001, 1002, 1003, 1004, 1005],\n    \"customer_id\": [501, 502, 503, 504, 505],\n    \"product_price\": [29.99, 149.99, 79.99, 199.99, 49.99],\n    \"shipping_cost\": [5.99, 12.99, 8.99, 15.99, 6.99],\n    \"tax_amount\": [2.40, 12.00, 6.40, 16.00, 4.00],\n    \"sale_region\": [\"North\", \"South\", \"East\", \"West\", \"North\"],\n    \"sale_channel\": [\"Online\", \"Store\", \"Online\", \"Store\", \"Online\"],\n    \"order_date\": [\n        datetime(2026, 1, 10),\n        datetime(2026, 1, 11),\n        datetime(2026, 1, 12),\n        datetime(2026, 1, 13),\n        datetime(2026, 1, 14)\n    ],\n    \"delivery_date\": [\n        datetime(2026, 1, 15),\n        datetime(2026, 1, 16),\n        datetime(2026, 1, 17),\n        datetime(2026, 1, 18),\n        datetime(2026, 1, 19)\n    ]\n})\nTasks:\n\nSelect all columns that contain the word “sale” in their name\nSelect all numeric columns EXCEPT the ID columns (order_id and customer_id)\nCalculate the sum of all columns that end with “_cost” or “_amount”\nExtract just the month from all temporal columns\nSelect all string columns that start with “sale_” and convert them to lowercase\n\nBonus Challenge: Create a single expression that selects all numeric columns (except IDs), rounds them to 2 decimal places, and adds a “_rounded” suffix to each column name.\n\n\nClick to see solutions\n\n# Task 1: Select columns containing \"sale\"\nsales_df.select(cs.contains(\"sale\"))\n\n# Task 2: Numeric columns excluding IDs\nsales_df.select(cs.numeric() - cs.by_name(\"order_id\", \"customer_id\"))\n\n# Task 3: Sum of cost and amount columns\nsales_df.select(\n    (cs.ends_with(\"_cost\") | cs.ends_with(\"_amount\")).sum()\n)\n\n# Task 4: Extract month from temporal columns\nsales_df.with_columns(\n    cs.temporal().dt.month()\n)\n\n# Task 5: Lowercase string columns starting with \"sale_\"\nsales_df.with_columns(\n    (cs.starts_with(\"sale_\") & cs.string()).str.to_lowercase()\n)\n\n# Bonus: Round numeric columns (except IDs) with suffix\nsales_df.with_columns(\n    (cs.numeric() - cs.by_name(\"order_id\", \"customer_id\"))\n    .round(2)\n    .name.suffix(\"_rounded\")\n)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#resources",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#resources",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "Resources",
    "text": "Resources\n\nPolars Documentation on Selectors"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a machine learning enginer with focus on computer vision and effcient machine learning solutions on edge based in Sweden.\nI mainly work with image related tasks nowadays from image signal processing, object detection/tracking/segmentation, face recognition systems."
  },
  {
    "objectID": "posts/2025-08-05-tfl-vs-tflm/index.html",
    "href": "posts/2025-08-05-tfl-vs-tflm/index.html",
    "title": "Difference between TFLite Python API vs TFLite Micro Python API",
    "section": "",
    "text": "TFLite Micro is the most popular Neural Network inference engine for micro CPUs. It’s designed for light weight model running on low power hardwares.\nMost common scenario is you use some training framework like PyTorch and TensorFlow, quantize it to tflite model. Finally you run your model on embedded device through the TFLite Micro library. However, the precision of the model usually loses during the quantization process. People would like to use the TFLite Python API to run the model on their computer, and verify the accuracy of the model is reduced to an acceptable level before deploying it to the embedded device. The problem is that the TFLite Python API cannot really simulate the model’s precision when running on the embedded device with TFLite Micro library. There are already lots of discussions about this issue, see github discussions 1 and 2\nUnknown to many people, TFLite Micro has a Python API, but the TFLite Micro Python API is different from the TFLite Python API. The TFLite Micro Python API is designed to run on the same principles as the TFLite Micro C++ library, which means it can provide a more accurate simulation of how the model will perform on embedded devices.\nIn this blog I will compare the TFLite Python API and the TFLite Micro Python API differences, and show how the two different APIs can be used to run the same model but have different results."
  },
  {
    "objectID": "posts/2025-08-05-tfl-vs-tflm/index.html#tflite-python-api",
    "href": "posts/2025-08-05-tfl-vs-tflm/index.html#tflite-python-api",
    "title": "Difference between TFLite Python API vs TFLite Micro Python API",
    "section": "TFLite Python API",
    "text": "TFLite Python API\nThe TFLite was previously decoupled from TensorFlow and was a standalone library called LiteRT now.\n\nLoading model:\nfrom ai-edge-litert.interpreter import Interpreter\ninterpreter = Interpreter(model_path=model)\nAllocate tensors:\ninterpreter.allocate_tensors()\nIt performs dynamic memory allocation for: a. allocates memory buffers for model inputs and outputs, b. allocates memory for all intermediate computation results between layers, c. allocates workspace memory needed during inference, d. finalizes the computation graph and prepares it for execution.\nGet input and output tensors details:\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nSet input tensor:\ninterpreter.set_tensor(input_details[0]['index'], x)\nRun inference:\ninterpreter.invoke()\nGet output tensor:\noutput_data = interpreter.get_tensor(output_details[0]['index'])"
  },
  {
    "objectID": "posts/2025-08-05-tfl-vs-tflm/index.html#tflite-micro-python-api",
    "href": "posts/2025-08-05-tfl-vs-tflm/index.html#tflite-micro-python-api",
    "title": "Difference between TFLite Python API vs TFLite Micro Python API",
    "section": "TFLite Micro Python API",
    "text": "TFLite Micro Python API\nThe TFLite Micro Python API is designed to run on the same principles as the TFLite Micro C++ library, it’s more aligned with the C++ API.\n\nLoading model, there are two ways to load the model file, one is from file, another is from bytearray.\nfrom tflite_micro.python.tflite_micro import runtime\n# Load from file\ninterpreter = runtime.Interpreter.from_file(model_path=model)\n# Load from bytearray\ninterpreter = runtime.Interpreter.from_bytes(model=model_bytes)\nThere is no need to allocate tensors. TFLite Micro pre-allocates all memory at compile time or initialization using a fixed-size arena. Embedded systems often lack heap allocation or have strict memory constraints, so TFLite Micro avoids malloc/free entirely.\nGet input and output tensors details:\ninput_details = interpreter.get_input_details(index)\noutput_details = interpreter.get_output_details(index)\nCompare to TFLite Python API where you can get all input and output details through one function call. In TFLite Micro Python API, you need to specify the index of the input or output tensor to get its details. For example, if you have multiple inputs or outputs, you can get the details of each one by specifying its index:\ninput_details = interpreter.get_input_details(0)  # Get details of first input tensor\noutput_details = interpreter.get_output_details(1)  # Get details of second output tensor\nSet input tensor:\n# Set input tensor using the input index\ninterpreter.set_input(x, 0)\n# If you have second input tensor:\ninterpreter.set_input(y, 1)\nCompared to TFLite Python API where you set the input tensor using the input details index, in TFLite Micro Python API, you set the input tensor using the input index directly. The index range depends on how many inputs you have. The order between the data and the index was reversed as well.\nRun inference:\ninterpreter.invoke()\nGet output tensor:\noutput_data = interpreter.get_output(0)\nThe same as set input tensor, you can use the output index depending on how many outputs you have."
  },
  {
    "objectID": "posts/2025-08-05-tfl-vs-tflm/index.html#difference-between-tflite-and-tflite-micro-output",
    "href": "posts/2025-08-05-tfl-vs-tflm/index.html#difference-between-tflite-and-tflite-micro-output",
    "title": "Difference between TFLite Python API vs TFLite Micro Python API",
    "section": "Difference between TFLite and TFLite Micro Output",
    "text": "Difference between TFLite and TFLite Micro Output\nI will use the yolov8n example from ultralytics to compare the difference between tflite and tflite-micro. They provide a script to inference yolov8 tflite model and visualize the result, makes it quite convenient. I make another class that subclass the YOLOv8TFLite example but using tflite-micro python package to do the inference.\nIf you are interested to follow the example you can refer my repo tflite_vs_tflite-micro.\n\nInference Speed\nCompared to TFLite python package, the TFLite Micro python package is much slower to run on PC. In this example, tflite python package only takes 0.04 second while tflite micro python package takes 76.7 second - 1917 times slower!!.\n\n\nInference Result\n\n\n \n\nIf you observe in detail, you could see that the bounding box of the bus is different and tflite predicts 0.34 confidence for the cut off person while tflite-micro predicts 0.39."
  },
  {
    "objectID": "posts/2025-08-05-tfl-vs-tflm/index.html#conclusion",
    "href": "posts/2025-08-05-tfl-vs-tflm/index.html#conclusion",
    "title": "Difference between TFLite Python API vs TFLite Micro Python API",
    "section": "Conclusion",
    "text": "Conclusion\nFrom the example above, we could see that due to the implementation difference TFLite Python API and TFLite Micro Python produced different result. I have seen worse cases than where the tflite result and tflite-micro result diverge a lot this from my past experience. My suggestion is if you want to check or reproduce your embedded tflite-micro result with python script, tflite-micro python package is a better choice than tflite python package. But due to the slow inference speed of tflite-micro package, it’s not suitable for evaluation the whole evaluation dataset."
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "",
    "text": "Today we’ll explore how to work with list columns in Polars. Lists are a powerful feature that allow you to store multiple values in a single cell, making it easy to represent nested or hierarchical data. We’ll cover explode to flatten lists, implode to create lists, and various list operations.\nimport polars as pl\n\n# Create a DataFrame with a list column\ndf = pl.DataFrame({\n    \"user_id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"purchases\": [\n        [\"laptop\", \"mouse\", \"keyboard\"],\n        [\"phone\"],\n        [\"tablet\", \"headphones\"]\n    ]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n})\n\nprint(df)\nshape: (3, 3)\n┌─────────┬─────────┬──────────────────────────────┐\n│ user_id ┆ name    ┆ purchases                    │\n│ ---     ┆ ---     ┆ ---                          │\n│ i64     ┆ str     ┆ list[str]                    │\n╞═════════╪═════════╪══════════════════════════════╡\n│ 1       ┆ Alice   ┆ [\"laptop\", \"mouse\", \"keyb... │\n│ 2       ┆ Bob     ┆ [\"phone\"]                    │\n│ 3       ┆ Charlie ┆ [\"tablet\", \"headphones\"]     │\n└─────────┴─────────┴──────────────────────────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#introduction",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#introduction",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "",
    "text": "Today we’ll explore how to work with list columns in Polars. Lists are a powerful feature that allow you to store multiple values in a single cell, making it easy to represent nested or hierarchical data. We’ll cover explode to flatten lists, implode to create lists, and various list operations.\nimport polars as pl\n\n# Create a DataFrame with a list column\ndf = pl.DataFrame({\n    \"user_id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"purchases\": [\n        [\"laptop\", \"mouse\", \"keyboard\"],\n        [\"phone\"],\n        [\"tablet\", \"headphones\"]\n    ]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n})\n\nprint(df)\nshape: (3, 3)\n┌─────────┬─────────┬──────────────────────────────┐\n│ user_id ┆ name    ┆ purchases                    │\n│ ---     ┆ ---     ┆ ---                          │\n│ i64     ┆ str     ┆ list[str]                    │\n╞═════════╪═════════╪══════════════════════════════╡\n│ 1       ┆ Alice   ┆ [\"laptop\", \"mouse\", \"keyb... │\n│ 2       ┆ Bob     ┆ [\"phone\"]                    │\n│ 3       ┆ Charlie ┆ [\"tablet\", \"headphones\"]     │\n└─────────┴─────────┴──────────────────────────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#exploding-lists",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#exploding-lists",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "Exploding Lists",
    "text": "Exploding Lists\nThe explode method transforms each element of a list into a separate row, duplicating the other column values. This is useful when you need to analyze individual list elements.\n# Explode the purchases column\nexploded_df = df.explode(\"purchases\")\nprint(exploded_df)\nshape: (6, 3)\n┌─────────┬─────────┬───────────┐\n│ user_id ┆ name    ┆ purchases │\n│ ---     ┆ ---     ┆ ---       │\n│ i64     ┆ str     ┆ str       │\n╞═════════╪═════════╪═══════════╡\n│ 1       ┆ Alice   ┆ laptop    │\n│ 1       ┆ Alice   ┆ mouse     │\n│ 1       ┆ Alice   ┆ keyboard  │\n│ 2       ┆ Bob     ┆ phone     │\n│ 3       ┆ Charlie ┆ tablet    │\n│ 3       ┆ Charlie ┆ headphones│\n└─────────┴─────────┴───────────┘\nYou can explode multiple columns simultaneously if they have the same length:\ndf_multi = pl.DataFrame({\n    \"id\": [1, 2],\n    \"values_a\": [[1, 2, 3], [4, 5]],\n    \"values_b\": [[\"a\", \"b\", \"c\"], [\"d\", \"e\"]]\n})\n\n# Explode both list columns\nexploded_multi = df_multi.explode([\"values_a\", \"values_b\"])\nprint(exploded_multi)\nshape: (5, 3)\n┌─────┬──────────┬──────────┐\n│ id  ┆ values_a ┆ values_b │\n│ --- ┆ ---      ┆ ---      │\n│ i64 ┆ i64      ┆ str      │\n╞═════╪══════════╪══════════╡\n│ 1   ┆ 1        ┆ a        │\n│ 1   ┆ 2        ┆ b        │\n│ 1   ┆ 3        ┆ c        │\n│ 2   ┆ 4        ┆ d        │\n│ 2   ┆ 5        ┆ e        │\n└─────┴──────────┴──────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#creating-lists-with-implode",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#creating-lists-with-implode",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "Creating Lists with implode",
    "text": "Creating Lists with implode\nThe implode method (also known as list in aggregation context) groups values back into lists. This is the inverse operation of explode. implode is called automatically in group_by().agg() operations.\n# Create individual purchase records\npurchases_df = pl.DataFrame({\n    \"user_id\": [1, 1, 1, 2, 3, 3],\n    \"item\": [\"laptop\", \"mouse\", \"keyboard\", \"phone\", \"tablet\", \"headphones\"],\n    \"price\": [1200, 25, 75, 800, 600, 150]\n})\n\n# Group items back into lists per user\ngrouped_df = purchases_df.group_by(\"user_id\").agg([\n    pl.col(\"item\").alias(\"items\"),\n    pl.col(\"price\").sum().alias(\"total_spent\")\n])\n\nprint(grouped_df)\nshape: (3, 3)\n┌─────────┬──────────────────────────────┬─────────────┐\n│ user_id ┆ items                        ┆ total_spent │\n│ ---     ┆ ---                          ┆ ---         │\n│ i64     ┆ list[str]                    ┆ i64         │\n╞═════════╪══════════════════════════════╪═════════════╡\n│ 1       ┆ [\"laptop\", \"mouse\", \"keyb... ┆ 1300        │\n│ 2       ┆ [\"phone\"]                    ┆ 800         │\n│ 3       ┆ [\"tablet\", \"headphones\"]     ┆ 750         │\n└─────────┴──────────────────────────────┴─────────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#list-operations",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#list-operations",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "List Operations",
    "text": "List Operations\nPolars provides a rich set of operations for working with list columns through the .list namespace.\n\nGetting List Length\ndf_with_length = df.with_columns(\n    pl.col(\"purchases\").list.len().alias(\"num_purchases\")\n)\nprint(df_with_length)\nshape: (3, 4)\n┌─────────┬─────────┬──────────────────────────────┬───────────────┐\n│ user_id ┆ name    ┆ purchases                    ┆ num_purchases │\n│ ---     ┆ ---     ┆ ---                          ┆ ---           │\n│ i64     ┆ str     ┆ list[str]                    ┆ u32           │\n╞═════════╪═════════╪══════════════════════════════╪═══════════════╡\n│ 1       ┆ Alice   ┆ [\"laptop\", \"mouse\", \"keyb... ┆ 3             │\n│ 2       ┆ Bob     ┆ [\"phone\"]                    ┆ 1             │\n│ 3       ┆ Charlie ┆ [\"tablet\", \"headphones\"]     ┆ 2             │\n└─────────┴─────────┴──────────────────────────────┴───────────────┘\n\n\nAccessing List Elements\n# Get the first item from each list\ndf_first = df.with_columns(\n    pl.col(\"purchases\").list.first().alias(\"first_purchase\")\n)\nprint(df_first)\n\n# Get the last item\ndf_last = df.with_columns(\n    pl.col(\"purchases\").list.last().alias(\"last_purchase\")\n)\n\n# Get item at specific index\ndf_indexed = df.with_columns(\n    pl.col(\"purchases\").list.get(1).alias(\"second_purchase\")\n)\nprint(df_indexed)\n\n\nSlicing Lists\n# Get first 2 items from each list\ndf_sliced = df.with_columns(\n    pl.col(\"purchases\").list.head(2).alias(\"first_two\")\n)\nprint(df_sliced)\n\n# Get all items except the first\ndf_tail = df.with_columns(\n    pl.col(\"purchases\").list.tail(-1).alias(\"rest\")\n)\n\n\nFiltering and Transforming Lists\n# Check if lists contain specific values\ndf_contains = df.with_columns(\n    pl.col(\"purchases\").list.contains(\"laptop\").alias(\"bought_laptop\")\n)\nprint(df_contains)\n\n# Apply expressions to list elements\ndf_numbers = pl.DataFrame({\n    \"id\": [1, 2, 3],\n    \"values\": [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n})\n\n# Square all values in each list\ndf_squared = df_numbers.with_columns(\n    pl.col(\"values\").list.eval(pl.element() * 2).alias(\"doubled\")\n)\nprint(df_squared)\n\n\nConcatenating Lists\ndf_concat = pl.DataFrame({\n    \"list1\": [[1, 2], [3, 4]],\n    \"list2\": [[5, 6], [7, 8]]\n})\n\n# Concatenate two list columns\ndf_merged = df_concat.with_columns(\n    pl.concat_list([\"list1\", \"list2\"]).alias(\"combined\")\n)\nprint(df_merged)\nshape: (2, 3)\n┌───────────┬───────────┬──────────────────┐\n│ list1     ┆ list2     ┆ combined         │\n│ ---       ┆ ---       ┆ ---              │\n│ list[i64] ┆ list[i64] ┆ list[i64]        │\n╞═══════════╪═══════════╪══════════════════╡\n│ [1, 2]    ┆ [5, 6]    ┆ [1, 2, 5, 6]     │\n│ [3, 4]    ┆ [7, 8]    ┆ [3, 4, 7, 8]     │\n└───────────┴───────────┴──────────────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#performance-considerations",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#performance-considerations",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "Performance Considerations",
    "text": "Performance Considerations\nWorking with list columns is generally efficient in Polars, but keep these tips in mind:\n\nUse native list operations instead of exploding and re-aggregating when possible\nExplode strategically - only when you need row-level operations\nList operations are vectorized - they’re much faster than using map_elements\nConsider Arrow’s memory layout - lists are stored efficiently in memory\n\nimport time\n\n# Create a larger dataset\nn = 100_000\nlarge_df = pl.DataFrame({\n    \"id\": range(n),\n    \"values\": [[i, i+1, i+2] for i in range(n)]\n})\n\n# Method 1: Using native list operations (FAST)\nstart = time.time()\nresult1 = large_df.with_columns(\n    pl.col(\"values\").list.sum().alias(\"sum\")\n)\ntime1 = time.time() - start\nprint(f\"Native list.sum(): {time1:.4f} seconds\")\n\n# Method 2: Explode and aggregate (SLOWER)\nstart = time.time()\nresult2 = large_df.explode(\"values\").group_by(\"id\").agg(\n    pl.col(\"values\").sum().alias(\"sum\")\n)\ntime2 = time.time() - start\nprint(f\"Explode + aggregate: {time2:.4f} seconds\")\n\nprint(f\"Native is {time2/time1:.1f}x faster\")"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#practice-exercise",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#practice-exercise",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "Practice Exercise",
    "text": "Practice Exercise\nNow it’s time to practice! Try solving this exercise using list operations:\nScenario: You’re analyzing e-commerce data with order information:\nimport polars as pl\n\n# Create sample e-commerce data\norders = pl.DataFrame({\n    \"order_id\": [1, 2, 3, 4],\n    \"customer_id\": [101, 102, 101, 103],\n    \"items\": [\n        [\"laptop\", \"mouse\"],\n        [\"phone\", \"case\", \"charger\"],\n        [\"keyboard\", \"monitor\"],\n        [\"tablet\"]\n    ],\n    \"prices\": [\n        [1200, 25],\n        [800, 15, 30],\n        [75, 300],\n        [600]\n    ]\n})\nTasks:\n\nAdd a column item_count that shows the number of items in each order\nExplode the data to create one row per item (hint: explode both items and prices columns)\nFilter to find only orders that have multiple items\nAdd a column max_price that shows the most expensive item in each order\nCreate a customer purchase history showing all items purchased, total amount spent, and number of unique orders per customer\n\nBonus Challenge: For each customer, create a new column that shows only items that cost more than $100. Use list operations to filter the prices and corresponding items lists.\n\n\nClick to see solutions\n\n# Task 1: Total items per order\nitems_per_order = orders.with_columns(\n    pl.col(\"items\").list.len().alias(\"item_count\")\n)\nprint(items_per_order)\n\n# Task 2: Explode to analyze individual items\nexploded_orders = orders.explode([\"items\", \"prices\"])\nprint(exploded_orders)\n\n# Task 3: Find customers who bought multiple items in an order\nmulti_item_orders = orders.filter(\n    pl.col(\"items\").list.len() &gt; 1\n)\nprint(multi_item_orders)\n\n# Task 4: Get most expensive item per order\nmost_expensive = orders.with_columns(\n    pl.col(\"prices\").list.max().alias(\"max_price\")\n)\nprint(most_expensive)\n\n# Task 5: Customer purchase history\ncustomer_history = exploded_orders.group_by(\"customer_id\").agg([\n    pl.col(\"items\").alias(\"all_items\"),\n    pl.col(\"prices\").sum().alias(\"total_spent\"),\n    pl.col(\"order_id\").n_unique().alias(\"num_orders\")\n])\nprint(customer_history)\n\n# Bonus: Filter expensive items per customer\nexpensive_items = orders.with_columns(\n    pl.col(\"items\").list.gather(\n        pl.col(\"prices\").list.eval(pl.arg_where(pl.element() &gt; 100))\n    ).alias(\"expensive_items\"),\n    \n    pl.col(\"prices\").list.gather(\n        pl.col(\"prices\").list.eval(pl.arg_where(pl.element() &gt; 100))\n    ).alias(\"expensive_prices\")\n)\nExpected output for Task 5:\nshape: (3, 4)\n┌─────────────┬──────────────────────────────┬─────────────┬────────────┐\n│ customer_id ┆ all_items                    ┆ total_spent ┆ num_orders │\n│ ---         ┆ ---                          ┆ ---         ┆ ---        │\n│ i64         ┆ list[str]                    ┆ i64         ┆ u32        │\n╞═════════════╪══════════════════════════════╪═════════════╪════════════╡\n│ 101         ┆ [\"laptop\", \"mouse\", \"keyb... ┆ 1600        ┆ 2          │\n│ 102         ┆ [\"phone\", \"case\", \"charger\"] ┆ 845         ┆ 1          │\n│ 103         ┆ [\"tablet\"]                   ┆ 600         ┆ 1          │\n└─────────────┴──────────────────────────────┴─────────────┴────────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#resources",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#resources",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "Resources",
    "text": "Resources\n\nPolars List Operations Documentation\nPolars explode Documentation\nWorking with Nested Data in Polars"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-29-day005-structs.html",
    "href": "posts/100-days-of-polars/2026-01-29-day005-structs.html",
    "title": "100 Days of Polars - Day 005: Working with Structs - Nested Data Structures",
    "section": "",
    "text": "Today we’ll explore structs in Polars, which allow you to store multiple fields as a single column. Think of structs as nested dictionaries or JSON objects within your DataFrame. They’re perfect for representing complex, hierarchical data while keeping your DataFrame organized.\nimport polars as pl\n\n# Create a DataFrame with a struct column\ndf = pl.DataFrame({\n    \"user_id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"address\": [\n        {\"street\": \"123 Main St\", \"city\": \"NYC\", \"zip\": \"10001\"},\n        {\"street\": \"456 Oak Ave\", \"city\": \"LA\", \"zip\": \"90001\"},\n        {\"street\": \"789 Pine Rd\", \"city\": \"Chicago\", \"zip\": \"60601\"}\n    ]\n})\n\nprint(df)\nshape: (3, 3)\n┌─────────┬─────────┬─────────────────────────────┐\n│ user_id ┆ name    ┆ address                     │\n│ ---     ┆ ---     ┆ ---                         │\n│ i64     ┆ str     ┆ struct[3]                   │\n╞═════════╪═════════╪═════════════════════════════╡\n│ 1       ┆ Alice   ┆ {123 Main St,NYC,10001}     │\n│ 2       ┆ Bob     ┆ {456 Oak Ave,LA,90001}      │\n│ 3       ┆ Charlie ┆ {789 Pine Rd,Chicago,60601} │\n└─────────┴─────────┴─────────────────────────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-29-day005-structs.html#introduction",
    "href": "posts/100-days-of-polars/2026-01-29-day005-structs.html#introduction",
    "title": "100 Days of Polars - Day 005: Working with Structs - Nested Data Structures",
    "section": "",
    "text": "Today we’ll explore structs in Polars, which allow you to store multiple fields as a single column. Think of structs as nested dictionaries or JSON objects within your DataFrame. They’re perfect for representing complex, hierarchical data while keeping your DataFrame organized.\nimport polars as pl\n\n# Create a DataFrame with a struct column\ndf = pl.DataFrame({\n    \"user_id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"address\": [\n        {\"street\": \"123 Main St\", \"city\": \"NYC\", \"zip\": \"10001\"},\n        {\"street\": \"456 Oak Ave\", \"city\": \"LA\", \"zip\": \"90001\"},\n        {\"street\": \"789 Pine Rd\", \"city\": \"Chicago\", \"zip\": \"60601\"}\n    ]\n})\n\nprint(df)\nshape: (3, 3)\n┌─────────┬─────────┬─────────────────────────────┐\n│ user_id ┆ name    ┆ address                     │\n│ ---     ┆ ---     ┆ ---                         │\n│ i64     ┆ str     ┆ struct[3]                   │\n╞═════════╪═════════╪═════════════════════════════╡\n│ 1       ┆ Alice   ┆ {123 Main St,NYC,10001}     │\n│ 2       ┆ Bob     ┆ {456 Oak Ave,LA,90001}      │\n│ 3       ┆ Charlie ┆ {789 Pine Rd,Chicago,60601} │\n└─────────┴─────────┴─────────────────────────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-29-day005-structs.html#creating-structs",
    "href": "posts/100-days-of-polars/2026-01-29-day005-structs.html#creating-structs",
    "title": "100 Days of Polars - Day 005: Working with Structs - Nested Data Structures",
    "section": "Creating Structs",
    "text": "Creating Structs\n\nFrom Python Dictionaries\nThe simplest way to create structs is from Python dictionaries:\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"person\": [\n        {\"name\": \"Alice\", \"age\": 30},\n        {\"name\": \"Bob\", \"age\": 25}\n    ]\n})\n\n\nUsing pl.struct()\nYou can create struct columns from existing columns:\ndf = pl.DataFrame({\n    \"first_name\": [\"Alice\", \"Bob\"],\n    \"last_name\": [\"Smith\", \"Jones\"],\n    \"age\": [30, 25]\n})\n\n# Combine columns into a struct\ndf_with_struct = df.select([\n    pl.struct([\"first_name\", \"last_name\", \"age\"]).alias(\"person\")\n])\nprint(df_with_struct)\nshape: (2, 1)\n┌─────────────────────────┐\n│ person                  │\n│ ---                     │\n│ struct[3]               │\n╞═════════════════════════╡\n│ {Alice,Smith,30}        │\n│ {Bob,Jones,25}          │\n└─────────────────────────┘\nYou can also create named structs with different field names:\ndf_renamed = df.select([\n    pl.struct(\n        first=pl.col(\"first_name\"),\n        last=pl.col(\"last_name\"),\n        years=pl.col(\"age\")\n    ).alias(\"person\")\n])"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-29-day005-structs.html#accessing-struct-fields",
    "href": "posts/100-days-of-polars/2026-01-29-day005-structs.html#accessing-struct-fields",
    "title": "100 Days of Polars - Day 005: Working with Structs - Nested Data Structures",
    "section": "Accessing Struct Fields",
    "text": "Accessing Struct Fields\n\nUsing .field()\nAccess individual fields from a struct column:\ndf = pl.DataFrame({\n    \"id\": [1, 2, 3],\n    \"address\": [\n        {\"street\": \"123 Main St\", \"city\": \"NYC\"},\n        {\"street\": \"456 Oak Ave\", \"city\": \"LA\"},\n        {\"street\": \"789 Pine Rd\", \"city\": \"Chicago\"}\n    ]\n})\n\n# Extract specific fields\ndf_with_fields = df.with_columns([\n    pl.col(\"address\").struct.field(\"city\").alias(\"city\"),\n    pl.col(\"address\").struct.field(\"street\").alias(\"street\")\n])\nprint(df_with_fields)\nshape: (3, 4)\n┌─────┬──────────────────────────┬─────────┬──────────────┐\n│ id  ┆ address                  ┆ city    ┆ street       │\n│ --- ┆ ---                      ┆ ---     ┆ ---          │\n│ i64 ┆ struct[2]                ┆ str     ┆ str          │\n╞═════╪══════════════════════════╪═════════╪══════════════╡\n│ 1   ┆ {123 Main St,NYC}        ┆ NYC     ┆ 123 Main St  │\n│ 2   ┆ {456 Oak Ave,LA}         ┆ LA      ┆ 456 Oak Ave  │\n│ 3   ┆ {789 Pine Rd,Chicago}    ┆ Chicago ┆ 789 Pine Rd  │\n└─────┴──────────────────────────┴─────────┴──────────────┘\n\n\nUsing .struct.unnest()\nUnnest a struct to expand all fields into separate columns:\ndf_unnested = df.unnest(\"address\")\nprint(df_unnested)\nshape: (3, 3)\n┌─────┬──────────────┬─────────┐\n│ id  ┆ street       ┆ city    │\n│ --- ┆ ---          ┆ ---     │\n│ i64 ┆ str          ┆ str     │\n╞═════╪══════════════╪═════════╡\n│ 1   ┆ 123 Main St  ┆ NYC     │\n│ 2   ┆ 456 Oak Ave  ┆ LA      │\n│ 3   ┆ 789 Pine Rd  ┆ Chicago │\n└─────┴──────────────┴─────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-29-day005-structs.html#renaming-struct-fields",
    "href": "posts/100-days-of-polars/2026-01-29-day005-structs.html#renaming-struct-fields",
    "title": "100 Days of Polars - Day 005: Working with Structs - Nested Data Structures",
    "section": "Renaming Struct Fields",
    "text": "Renaming Struct Fields\nYou can rename fields within a struct:\ndf = pl.DataFrame({\n    \"person\": [\n        {\"first\": \"Alice\", \"last\": \"Smith\"},\n        {\"first\": \"Bob\", \"last\": \"Jones\"}\n    ]\n})\n\n# Rename struct fields\ndf_renamed = df.with_columns(\n    pl.col(\"person\").struct.rename_fields([\"first_name\", \"last_name\"])\n)\nprint(df_renamed)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-29-day005-structs.html#working-with-nested-structs",
    "href": "posts/100-days-of-polars/2026-01-29-day005-structs.html#working-with-nested-structs",
    "title": "100 Days of Polars - Day 005: Working with Structs - Nested Data Structures",
    "section": "Working with Nested Structs",
    "text": "Working with Nested Structs\nStructs can contain other structs, creating deeply nested data:\ndf = pl.DataFrame({\n    \"user_id\": [1, 2],\n    \"profile\": [\n        {\n            \"name\": {\"first\": \"Alice\", \"last\": \"Smith\"},\n            \"contact\": {\"email\": \"alice@example.com\", \"phone\": \"555-0001\"}\n        },\n        {\n            \"name\": {\"first\": \"Bob\", \"last\": \"Jones\"},\n            \"contact\": {\"email\": \"bob@example.com\", \"phone\": \"555-0002\"}\n        }\n    ]\n})\n\n# Access nested fields\ndf_with_nested = df.with_columns([\n    pl.col(\"profile\").struct.field(\"name\").struct.field(\"first\").alias(\"first_name\"),\n    pl.col(\"profile\").struct.field(\"contact\").struct.field(\"email\").alias(\"email\")\n])\nprint(df_with_nested)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-29-day005-structs.html#json-to-struct",
    "href": "posts/100-days-of-polars/2026-01-29-day005-structs.html#json-to-struct",
    "title": "100 Days of Polars - Day 005: Working with Structs - Nested Data Structures",
    "section": "JSON to Struct",
    "text": "JSON to Struct\nPolars can parse JSON strings into structs:\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"json_data\": [\n        '{\"name\": \"Alice\", \"age\": 30, \"city\": \"NYC\"}',\n        '{\"name\": \"Bob\", \"age\": 25, \"city\": \"LA\"}'\n    ]\n})\n\n# Parse JSON strings to structs\ndf_parsed = df.with_columns(\n    pl.col(\"json_data\").str.json_decode(pl.Struct([\n            pl.Field(\"name\", pl.Utf8),\n            pl.Field(\"age\", pl.Int64),\n            pl.Field(\"city\", pl.Utf8)\n        ])).alias(\"parsed\")\n)\nprint(df_parsed)\n\n# Extract specific fields from parsed JSON\ndf_extracted = df_parsed.with_columns([\n    pl.col(\"parsed\").struct.field(\"name\").alias(\"name\"),\n    pl.col(\"parsed\").struct.field(\"age\").alias(\"age\")\n])\nprint(df_extracted)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-29-day005-structs.html#struct.with_fields",
    "href": "posts/100-days-of-polars/2026-01-29-day005-structs.html#struct.with_fields",
    "title": "100 Days of Polars - Day 005: Working with Structs - Nested Data Structures",
    "section": ".struct.with_fields()",
    "text": ".struct.with_fields()\nimport polars as pl\n\ndf = pl.DataFrame({\n    \"user_data\": [\n        {\"name\": \"alice\", \"score\": 85, \"active\": True},\n        {\"name\": \"bob\", \"score\": 42, \"active\": False}\n    ]\n})\n\n# Update score and add a new category field inside the struct\ndf_updated = df.with_columns(\n    pl.col(\"user_data\").struct.with_fields(\n        # 1. Update an existing field\n        pl.field(\"score\") + 5,\n        \n        # 2. Add a brand new field inside the struct\n        is_passing = pl.field(\"score\") &gt;= 50,\n        \n        # 3. Use logic from one field to update another\n        active = pl.when(pl.field(\"score\") &gt; 80).then(True).otherwise(pl.field(\"active\"))\n    )\n)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-29-day005-structs.html#combining-structs-and-lists",
    "href": "posts/100-days-of-polars/2026-01-29-day005-structs.html#combining-structs-and-lists",
    "title": "100 Days of Polars - Day 005: Working with Structs - Nested Data Structures",
    "section": "Combining Structs and Lists",
    "text": "Combining Structs and Lists\nYou can have lists of structs or structs containing lists:\n# List of structs\ndf_list_of_structs = pl.DataFrame({\n    \"user_id\": [1, 2],\n    \"orders\": [\n        [\n            {\"order_id\": 101, \"amount\": 50.0},\n            {\"order_id\": 102, \"amount\": 75.0}\n        ],\n        [\n            {\"order_id\": 201, \"amount\": 100.0}\n        ]\n    ]\n})\n\n# Explode the list and access struct fields\ndf_exploded = df_list_of_structs.explode(\"orders\").with_columns([\n    pl.col(\"orders\").struct.field(\"order_id\").alias(\"order_id\"),\n    pl.col(\"orders\").struct.field(\"amount\").alias(\"amount\")\n])\nprint(df_exploded)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-29-day005-structs.html#practice-exercise",
    "href": "posts/100-days-of-polars/2026-01-29-day005-structs.html#practice-exercise",
    "title": "100 Days of Polars - Day 005: Working with Structs - Nested Data Structures",
    "section": "Practice Exercise",
    "text": "Practice Exercise\nNow it’s time to practice! Try solving this exercise using struct operations:\nScenario: You’re analyzing customer data from an e-commerce API that returns nested JSON data:\nimport polars as pl\n\ncustomers = pl.DataFrame({\n    \"customer_id\": [1, 2, 3],\n    \"profile\": [\n        {\n            \"name\": {\"first\": \"Alice\", \"last\": \"Johnson\"},\n            \"age\": 28,\n            \"address\": {\"city\": \"NYC\", \"state\": \"NY\", \"zip\": \"10001\"}\n        },\n        {\n            \"name\": {\"first\": \"Bob\", \"last\": \"Smith\"},\n            \"age\": 35,\n            \"address\": {\"city\": \"Los Angeles\", \"state\": \"CA\", \"zip\": \"90001\"}\n        },\n        {\n            \"name\": {\"first\": \"Charlie\", \"last\": \"Brown\"},\n            \"age\": 42,\n            \"address\": {\"city\": \"Chicago\", \"state\": \"IL\", \"zip\": \"60601\"}\n        }\n    ],\n    \"orders\": [\n        [{\"order_id\": 101, \"total\": 150.50}, {\"order_id\": 102, \"total\": 200.00}],\n        [{\"order_id\": 201, \"total\": 89.99}],\n        [{\"order_id\": 301, \"total\": 450.00}, {\"order_id\": 302, \"total\": 125.75}, {\"order_id\": 303, \"total\": 75.00}]\n    ]\n})\nTasks:\n\nExtract the first name and city from each customer into separate columns\nCreate a new column full_name by combining first and last names\nUnnest the address information into separate columns (city, state, zip)\nFilter customers who are older than 30 years\nExplode the orders list and calculate the total amount spent per customer\n\nBonus Challenge: Create a new struct column called customer_summary that contains: - full_name (first + last) - location (city, state) - order_count (number of orders) - total_spent (sum of all order totals)\n\n\nClick to see solutions\n\n# Task 1: Extract first name and city\ntask1 = customers.with_columns([\n    pl.col(\"profile\").struct.field(\"name\").struct.field(\"first\").alias(\"first_name\"),\n    pl.col(\"profile\").struct.field(\"address\").struct.field(\"city\").alias(\"city\")\n])\nprint(task1)\n\n# Task 2: Create full_name column\ntask2 = customers.with_columns(\n    (pl.col(\"profile\").struct.field(\"name\").struct.field(\"first\") + \" \" +\n     pl.col(\"profile\").struct.field(\"name\").struct.field(\"last\")).alias(\"full_name\")\n)\nprint(task2)\n\n# Task 3: Unnest address information\ntask3 = customers.with_columns(\n    pl.col(\"profile\").struct.field(\"address\").alias(\"address\")\n).unnest(\"address\")\nprint(task3)\n\n# Task 4: Filter customers older than 30\ntask4 = customers.filter(\n    pl.col(\"profile\").struct.field(\"age\") &gt; 30\n)\nprint(task4)\n\n# Task 5: Explode orders and calculate total spent per customer\ntask5 = customers.explode(\"orders\").with_columns([\n    pl.col(\"orders\").struct.field(\"order_id\").alias(\"order_id\"),\n    pl.col(\"orders\").struct.field(\"total\").alias(\"order_total\")\n]).group_by(\"customer_id\").agg([\n    pl.col(\"order_total\").sum().alias(\"total_spent\"),\n    pl.col(\"order_id\").count().alias(\"order_count\")\n])\nprint(task5)\n\n# Bonus: Create customer_summary struct\nbonus = customers.explode(\"orders\").with_columns([\n    pl.col(\"orders\").struct.field(\"total\").alias(\"order_total\")\n]).group_by(\"customer_id\").agg([\n    pl.col(\"profile\").first().alias(\"profile\"),\n    pl.col(\"order_total\").sum().alias(\"total_spent\"),\n    pl.col(\"order_total\").count().alias(\"order_count\")\n]).with_columns(\n    pl.struct([\n        (pl.col(\"profile\").struct.field(\"name\").struct.field(\"first\") + \" \" +\n         pl.col(\"profile\").struct.field(\"name\").struct.field(\"last\")).alias(\"full_name\"),\n        (pl.col(\"profile\").struct.field(\"address\").struct.field(\"city\") + \", \" +\n         pl.col(\"profile\").struct.field(\"address\").struct.field(\"state\")).alias(\"location\"),\n        pl.col(\"order_count\"),\n        pl.col(\"total_spent\")\n    ]).alias(\"customer_summary\")\n)\nprint(bonus.select([\"customer_id\", \"customer_summary\"]))\nExpected output for Task 5:\nshape: (3, 3)\n┌─────────────┬─────────────┬─────────────┐\n│ customer_id ┆ total_spent ┆ order_count │\n│ ---         ┆ ---         ┆ ---         │\n│ i64         ┆ f64         ┆ u32         │\n╞═════════════╪═════════════╪═════════════╡\n│ 1           ┆ 350.5       ┆ 2           │\n│ 2           ┆ 89.99       ┆ 1           │\n│ 3           ┆ 650.75      ┆ 3           │\n└─────────────┴─────────────┴─────────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-29-day005-structs.html#resources",
    "href": "posts/100-days-of-polars/2026-01-29-day005-structs.html#resources",
    "title": "100 Days of Polars - Day 005: Working with Structs - Nested Data Structures",
    "section": "Resources",
    "text": "Resources\n\nPolars Struct Operations Documentation\nWorking with Nested Data Types\nJSON Parsing in Polars"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html",
    "href": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html",
    "title": "100 Days of Polars - Day 003: map_elements vs map_batches vs Native Expressions",
    "section": "",
    "text": "Today we’ll explore how to apply custom logic in polars with map_elements when the functionality isn’t available through polars’s built-in expression.\nimport random\nimport string\nimport time\nimport polars as pl\n\ndef random_string(length=8):\n    return ''.join(random.choices(string.ascii_lowercase, k=length))\n\ndf = pl.DataFrame({\n    \"text\": [random_string() for _ in range(1_000_000)]\n})\n\n# Apply a custom function to each element\nt0 = time.time()\nresult = df.with_columns(\n    pl.col(\"text\").map_elements(lambda x: x.upper()).alias(\"uppercase\")\n)\nt1 = time.time()\nprint(f\"map_elements: {t1-t0}\")\nWhen you run this code in polars you will get a warning like this:\n&lt;ipython-input-51-4b0fe28e2086&gt;:9: PolarsInefficientMapWarning: \nExpr.map_elements is significantly slower than the native expressions API.\nOnly use if you absolutely CANNOT implement your logic otherwise.\nReplace this expression...\n  - pl.col(\"text\").map_elements(lambda x: ...)\nwith this one instead:\n  + pl.col(\"text\").str.to_uppercase()\n\n  pl.col(\"text\").map_elements(lambda x: x.upper()).alias(\"uppercase\")\nThe reason for this warning is unlike native polars expressions, map_elements are not parallelized and usually involves data copy betten polars’s rust engine and python interpreter. When we use the suggested code, time performance increased more than 3x for this small dataset and simple task.\nt0 = time.time()\nresult = df.with_columns(\n    pl.col(\"text\").str.to_uppercase().alias(\"uppercase\")\n)\nt1 = time.time()\nprint(f\"native expression: {t1-t0}\")"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html#introduction",
    "href": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html#introduction",
    "title": "100 Days of Polars - Day 003: map_elements vs map_batches vs Native Expressions",
    "section": "",
    "text": "Today we’ll explore how to apply custom logic in polars with map_elements when the functionality isn’t available through polars’s built-in expression.\nimport random\nimport string\nimport time\nimport polars as pl\n\ndef random_string(length=8):\n    return ''.join(random.choices(string.ascii_lowercase, k=length))\n\ndf = pl.DataFrame({\n    \"text\": [random_string() for _ in range(1_000_000)]\n})\n\n# Apply a custom function to each element\nt0 = time.time()\nresult = df.with_columns(\n    pl.col(\"text\").map_elements(lambda x: x.upper()).alias(\"uppercase\")\n)\nt1 = time.time()\nprint(f\"map_elements: {t1-t0}\")\nWhen you run this code in polars you will get a warning like this:\n&lt;ipython-input-51-4b0fe28e2086&gt;:9: PolarsInefficientMapWarning: \nExpr.map_elements is significantly slower than the native expressions API.\nOnly use if you absolutely CANNOT implement your logic otherwise.\nReplace this expression...\n  - pl.col(\"text\").map_elements(lambda x: ...)\nwith this one instead:\n  + pl.col(\"text\").str.to_uppercase()\n\n  pl.col(\"text\").map_elements(lambda x: x.upper()).alias(\"uppercase\")\nThe reason for this warning is unlike native polars expressions, map_elements are not parallelized and usually involves data copy betten polars’s rust engine and python interpreter. When we use the suggested code, time performance increased more than 3x for this small dataset and simple task.\nt0 = time.time()\nresult = df.with_columns(\n    pl.col(\"text\").str.to_uppercase().alias(\"uppercase\")\n)\nt1 = time.time()\nprint(f\"native expression: {t1-t0}\")"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html#map_batches",
    "href": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html#map_batches",
    "title": "100 Days of Polars - Day 003: map_elements vs map_batches vs Native Expressions",
    "section": "map_batches",
    "text": "map_batches\nmap_batches is another tool that applies a Python function to batches (chunks) of data in polars. It’s a middle group beteewn map_elements and native expression. It also allows using numpy/other libraries on chunks, so it might be much faster than native expression when these library are highly optimized.\nLet’s look at an example below:\nimport polars as pl\nimport numpy as np\nimport time\n\nn_rows = 1_000_000\nn_groups = 4\nunique_groups = [f\"grp_{i}\" for i in range(n_groups)]\ndf = pl.DataFrame({\n    \"value\": np.random.randn(n_rows),\n    \"group\": np.random.choice(unique_groups, n_rows)\n})\n\n\ngroup_stats = df.group_by(\"group\").agg([\n    pl.col(\"value\").mean().alias(\"mean\"),\n    pl.col(\"value\").std().alias(\"std\")\n])\n# Method 1: map_elements (SLOW)\nstart = time.time()\nresult = df.join(group_stats, on=\"group\").with_columns(\n    pl.struct([\"value\", \"mean\", \"std\"])\n    .map_elements(\n        lambda row: (row[\"value\"] - row[\"mean\"]) / row[\"std\"],\n        return_dtype=pl.Float64\n    )\n    .alias(\"z_score\")\n)\ntotal = time.time() - start\nprint(f\"map_elements (1M rows):     {total:.4f} seconds\")\n\n# Method 2: Native expression (FAST)\nstart = time.time()\nresult = df.with_columns(\n    ((pl.col(\"value\") - pl.col(\"value\").mean().over(\"group\")) / \n     pl.col(\"value\").std().over(\"group\"))\n    .alias(\"z_score\")\n)\ntotal = time.time() - start\nprint(f\"Native expression (1M rows): {total:.4f} seconds\")\n\n# Method 3: map_batches (MIDDLE GROUND)\ndef normalize_batch(series: pl.Series) -&gt; pl.Series:\n    # Using numpy for vectorized operations on the batch\n    arr = series.to_numpy()\n    return pl.Series((arr - arr.mean()) / arr.std())\n\nstart = time.time()\nresult = df.with_columns(\n    pl.col(\"value\").map_batches(normalize_batch).over(\"group\").alias(\"z_score\")\n)\ntotal = time.time() - start\nprint(f\"map_batches (1M rows): {total:.4f} seconds\")\nWhen you look at the result below, you will find out map_batches is faster than native expression and map_elements is the slowest.\nmap_elements (1M rows):     0.3763 seconds\nNative expression (1M rows): 0.0362 seconds\nmap_batches (1M rows): 0.0299 seconds\nBut if we increase the n_groups to 10000, we will get result below:\nmap_elements (1M rows):     0.6927 seconds\nNative expression (1M rows): 0.0621 seconds\nmap_batches (1M rows): 0.4327 seconds\nThe result reverted, the number of groups decies how many context switches polars has to perform from rust engine to numpy. The more complicate the logic is, the more benefits native expression will bring."
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html#resources",
    "href": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html#resources",
    "title": "100 Days of Polars - Day 003: map_elements vs map_batches vs Native Expressions",
    "section": "Resources",
    "text": "Resources\n\nPolars map_elements Documentation\nPolars map_batches Documentation\nPolars Expression API"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-08-day008-joins.html",
    "href": "posts/100-days-of-polars/2026-02-08-day008-joins.html",
    "title": "100 Days of Polars - Day 008: Mastering Joins - From Basic to Advanced",
    "section": "",
    "text": "Joins are fundamental to data engineering - they allow us to combine data from multiple sources. Polars offers a rich set of join operations that are optimized for performance. Today we’ll explore all join types from basic to advanced, with practical examples you can use immediately."
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-08-day008-joins.html#introduction",
    "href": "posts/100-days-of-polars/2026-02-08-day008-joins.html#introduction",
    "title": "100 Days of Polars - Day 008: Mastering Joins - From Basic to Advanced",
    "section": "",
    "text": "Joins are fundamental to data engineering - they allow us to combine data from multiple sources. Polars offers a rich set of join operations that are optimized for performance. Today we’ll explore all join types from basic to advanced, with practical examples you can use immediately."
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-08-day008-joins.html#quick-reference",
    "href": "posts/100-days-of-polars/2026-02-08-day008-joins.html#quick-reference",
    "title": "100 Days of Polars - Day 008: Mastering Joins - From Basic to Advanced",
    "section": "Quick Reference",
    "text": "Quick Reference\n\n\n\n\n\n\n\n\nJoin Type\nWhat It Returns\nCommon Use Case\n\n\n\n\ninner\nOnly matching rows from both tables\nFind common records\n\n\nleft\nAll rows from left + matching from right\nEnrich data with reference info\n\n\nright\nAll rows from right + matching from left\nWhen right table is primary\n\n\nfull\nAll rows from both tables\nCombine two datasets completely\n\n\ncross\nCartesian product (every combination)\nGenerate all combinations\n\n\nsemi\nLeft rows where key exists in right\nFilter based on existence\n\n\nanti\nLeft rows where key NOT in right\nFind missing records\n\n\nasof\nMatch on nearest key (time-series)\nJoin time-series data"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-08-day008-joins.html#setup-sample-data",
    "href": "posts/100-days-of-polars/2026-02-08-day008-joins.html#setup-sample-data",
    "title": "100 Days of Polars - Day 008: Mastering Joins - From Basic to Advanced",
    "section": "Setup: Sample Data",
    "text": "Setup: Sample Data\nimport polars as pl\n\n# Left table: Orders\norders = pl.DataFrame({\n    \"order_id\": [1, 2, 3, 4, 5],\n    \"customer_id\": [101, 102, 103, 104, 105],\n    \"product\": [\"A\", \"B\", \"A\", \"C\", \"B\"],\n    \"amount\": [100, 200, 150, 300, 250],\n})\n\n# Right table: Customers\ncustomers = pl.DataFrame({\n    \"customer_id\": [101, 102, 103, 106],\n    \"name\": [\"Alice\", \"Bob\", \"Carol\", \"David\"],\n    \"city\": [\"NYC\", \"LA\", \"Chicago\", \"Seattle\"],\n})\n\n# Products reference table\nproducts = pl.DataFrame({\n    \"product\": [\"A\", \"B\", \"C\", \"D\"],\n    \"category\": [\"Electronics\", \"Books\", \"Clothing\", \"Food\"],\n    \"price\": [99, 25, 50, 10],\n})"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-08-day008-joins.html#inner-join",
    "href": "posts/100-days-of-polars/2026-02-08-day008-joins.html#inner-join",
    "title": "100 Days of Polars - Day 008: Mastering Joins - From Basic to Advanced",
    "section": "Inner Join",
    "text": "Inner Join\nReturns only rows where keys match in both tables.\n# Find orders with valid customers\ninner_result = orders.join(\n    customers,\n    on=\"customer_id\",\n    how=\"inner\"\n)\n\nprint(inner_result)\nResult:\norder_id | customer_id | product | amount | name  | city\n---------|-------------|---------|--------|-------|--------\n1        | 101         | A       | 100    | Alice | NYC\n2        | 102         | B       | 200    | Bob   | LA\n3        | 103         | A       | 150    | Carol | Chicago\nKey points: - Order 4 and 5 (customers 104, 105) are excluded - not in customers table - Customer David (106) is excluded - has no orders - Only 3 rows returned"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-08-day008-joins.html#left-join",
    "href": "posts/100-days-of-polars/2026-02-08-day008-joins.html#left-join",
    "title": "100 Days of Polars - Day 008: Mastering Joins - From Basic to Advanced",
    "section": "Left Join",
    "text": "Left Join\nReturns all rows from left + matching from right (nulls where no match).\n# All orders, enrich with customer info where available\nleft_result = orders.join(\n    customers,\n    on=\"customer_id\",\n    how=\"left\"\n)\n\nprint(left_result)\nResult:\norder_id | customer_id | product | amount | name   | city\n---------|-------------|---------|--------|--------|--------\n1        | 101         | A       | 100    | Alice  | NYC\n2        | 102         | B       | 200    | Bob    | LA\n3        | 103         | A       | 150    | Carol  | Chicago\n4        | 104         | C       | 300    | null   | null\n5        | 105         | B       | 250    | null   | null\nKey points: - All 5 orders preserved - Orders 4 and 5 have null customer info (customers 104, 105 not in reference table) - Most common join type for data enrichment"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-08-day008-joins.html#right-join",
    "href": "posts/100-days-of-polars/2026-02-08-day008-joins.html#right-join",
    "title": "100 Days of Polars - Day 008: Mastering Joins - From Basic to Advanced",
    "section": "Right Join",
    "text": "Right Join\nReturns all rows from right + matching from left (nulls where no match).\n# All customers, show their orders (if any)\nright_result = orders.join(\n    customers,\n    on=\"customer_id\",\n    how=\"right\"\n)\n\nprint(right_result)\nResult:\norder_id | customer_id | product | amount | name  | city\n---------|-------------|---------|--------|-------|--------\n1        | 101         | A       | 100    | Alice | NYC\n2        | 102         | B       | 200    | Bob   | LA\n3        | 103         | A       | 150    | Carol | Chicago\nnull     | 106         | null    | null   | David | Seattle\nKey points: - All 4 customers preserved - David (106) has null order info - no orders - Similar to left join but keeps right side"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-08-day008-joins.html#fullouter-join",
    "href": "posts/100-days-of-polars/2026-02-08-day008-joins.html#fullouter-join",
    "title": "100 Days of Polars - Day 008: Mastering Joins - From Basic to Advanced",
    "section": "Full/Outer Join",
    "text": "Full/Outer Join\nReturns all rows from both tables (nulls where no match).\n# Complete view: all orders and all customers\nfull_result = orders.join(\n    customers,\n    on=\"customer_id\",\n    how=\"full\"\n)\n\nprint(full_result)\nResult:\norder_id | customer_id | product | amount | name   | city\n---------|-------------|---------|--------|--------|--------\n1        | 101         | A       | 100    | Alice  | NYC\n2        | 102         | B       | 200    | Bob    | LA\n3        | 103         | A       | 150    | Carol  | Chicago\n4        | 104         | C       | 300    | null   | null\n5        | 105         | B       | 250    | null   | null\nnull     | 106         | null    | null   | David  | Seattle\nKey points: - 6 rows total (union of both sets) - Preserves all information from both tables - Use when you need complete data from both sources"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-08-day008-joins.html#cross-join",
    "href": "posts/100-days-of-polars/2026-02-08-day008-joins.html#cross-join",
    "title": "100 Days of Polars - Day 008: Mastering Joins - From Basic to Advanced",
    "section": "Cross Join",
    "text": "Cross Join\nReturns Cartesian product - every row from left combined with every row from right.\n# Generate all combinations\ncross_result = orders.join(\n    products.select(\"category\").unique(),\n    how=\"cross\"\n)\n\nprint(f\"Orders: {len(orders)}, Categories: {4}, Result: {len(cross_result)}\")\nprint(cross_result.head(8))\nKey points: - No on parameter needed - Result has len(left) × len(right) rows - Use sparingly - can create huge datasets - Common use: generating test data or all possible combinations"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-08-day008-joins.html#semi-join",
    "href": "posts/100-days-of-polars/2026-02-08-day008-joins.html#semi-join",
    "title": "100 Days of Polars - Day 008: Mastering Joins - From Basic to Advanced",
    "section": "Semi Join",
    "text": "Semi Join\nReturns left rows where key exists in right (no right columns added).\n# Find orders from known customers only (filter, don't enrich)\nsemi_result = orders.join(\n    customers,\n    on=\"customer_id\",\n    how=\"semi\"\n)\n\nprint(semi_result)\nResult:\norder_id | customer_id | product | amount\n---------|-------------|---------|--------\n1        | 101         | A       | 100\n2        | 102         | B       | 200\n3        | 103         | A       | 150\nKey points: - Like inner but doesn’t add right columns - Used for filtering only - More efficient than inner join when you don’t need right columns - Orders 4 and 5 excluded (unknown customers)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-08-day008-joins.html#anti-join",
    "href": "posts/100-days-of-polars/2026-02-08-day008-joins.html#anti-join",
    "title": "100 Days of Polars - Day 008: Mastering Joins - From Basic to Advanced",
    "section": "Anti Join",
    "text": "Anti Join\nReturns left rows where key does NOT exist in right.\n# Find orders from unknown/missing customers\nanti_result = orders.join(\n    customers,\n    on=\"customer_id\",\n    how=\"anti\"\n)\n\nprint(anti_result)\nResult:\norder_id | customer_id | product | amount\n---------|-------------|---------|--------\n4        | 104         | C       | 300\n5        | 105         | B       | 250\nKey points: - Opposite of semi join - Great for data quality checks - Find orphaned records, missing references"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-08-day008-joins.html#asof-join-time-series",
    "href": "posts/100-days-of-polars/2026-02-08-day008-joins.html#asof-join-time-series",
    "title": "100 Days of Polars - Day 008: Mastering Joins - From Basic to Advanced",
    "section": "AsOf Join (Time-Series)",
    "text": "AsOf Join (Time-Series)\nMatches on nearest key without exceeding - perfect for time-series.\n# Stock prices at different times\nprices = pl.DataFrame({\n    \"time\": [\n        \"2026-01-01 09:00\", \"2026-01-01 09:05\",\n        \"2026-01-01 09:10\", \"2026-01-01 09:15\"\n    ],\n    \"price\": [100.0, 101.5, 99.8, 102.0]\n}).with_columns(pl.col(\"time\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M\"))\n\n# Trades happening at various times\ntrades = pl.DataFrame({\n    \"time\": [\n        \"2026-01-01 09:02\", \"2026-01-01 09:07\",\n        \"2026-01-01 09:12\"\n    ],\n    \"volume\": [100, 200, 150]\n}).with_columns(pl.col(\"time\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M\"))\n\n# Match each trade with the most recent price\nasof_result = trades.join_asof(\n    prices,\n    on=\"time\",\n    strategy=\"backward\"  # Use most recent price before trade\n)\n\nprint(asof_result)\nResult:\ntime                | volume | price\n--------------------|--------|-------\n2026-01-01 09:02:00 | 100    | 100.0  &lt;- price at 09:00\n2026-01-01 09:07:00 | 200    | 101.5  &lt;- price at 09:05\n2026-01-01 09:12:00 | 150    | 99.8   &lt;- price at 09:10\nKey points: - No exact time match needed - strategy=\"backward\" - nearest earlier time - strategy=\"forward\" - nearest later time - Essential for financial data, sensor readings"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-08-day008-joins.html#multi-key-joins",
    "href": "posts/100-days-of-polars/2026-02-08-day008-joins.html#multi-key-joins",
    "title": "100 Days of Polars - Day 008: Mastering Joins - From Basic to Advanced",
    "section": "Multi-Key Joins",
    "text": "Multi-Key Joins\nJoin on multiple columns for more precise matching:\n# Sales data with region\nsales = pl.DataFrame({\n    \"date\": [\"2026-01-01\", \"2026-01-01\", \"2026-01-02\"],\n    \"region\": [\"North\", \"South\", \"North\"],\n    \"amount\": [100, 200, 150]\n})\n\n# Targets by date and region\ntargets = pl.DataFrame({\n    \"date\": [\"2026-01-01\", \"2026-01-01\", \"2026-01-02\"],\n    \"region\": [\"North\", \"South\", \"North\"],\n    \"target\": [120, 180, 140]\n})\n\n# Join on both date and region\nmulti_join = sales.join(\n    targets,\n    on=[\"date\", \"region\"],\n    how=\"left\"\n).with_columns(\n    (pl.col(\"amount\") - pl.col(\"target\")).alias(\"variance\")\n)\n\nprint(multi_join)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-08-day008-joins.html#join-with-suffix",
    "href": "posts/100-days-of-polars/2026-02-08-day008-joins.html#join-with-suffix",
    "title": "100 Days of Polars - Day 008: Mastering Joins - From Basic to Advanced",
    "section": "Join with Suffix",
    "text": "Join with Suffix\nWhen both tables have columns with same names (other than join keys):\n# Both tables have 'amount' column\norders_with_price = pl.DataFrame({\n    \"product\": [\"A\", \"B\", \"C\"],\n    \"amount\": [100, 200, 300],  # order amount\n})\n\nproducts_with_cost = pl.DataFrame({\n    \"product\": [\"A\", \"B\", \"C\"],\n    \"amount\": [80, 150, 250],   # product cost\n})\n\n# Add suffix to distinguish columns\nresult = orders_with_price.join(\n    products_with_cost,\n    on=\"product\",\n    how=\"left\",\n    suffix=\"_cost\"  # right table columns get this suffix\n)\n\nprint(result)\n# Columns: product, amount, amount_cost"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-08-day008-joins.html#practice-exercise",
    "href": "posts/100-days-of-polars/2026-02-08-day008-joins.html#practice-exercise",
    "title": "100 Days of Polars - Day 008: Mastering Joins - From Basic to Advanced",
    "section": "Practice Exercise",
    "text": "Practice Exercise\nYou have three datasets for an e-commerce platform:\nimport polars as pl\nfrom datetime import datetime\n\n# User registrations\nusers = pl.DataFrame({\n    \"user_id\": [1, 2, 3, 4, 5],\n    \"email\": [\"alice@example.com\", \"bob@example.com\", \n              \"carol@example.com\", \"david@example.com\", \"eve@example.com\"],\n    \"signup_date\": [datetime(2026, 1, 1), datetime(2026, 1, 5),\n                    datetime(2026, 1, 10), datetime(2026, 1, 15),\n                    datetime(2026, 1, 20)],\n})\n\n# Orders placed\norders = pl.DataFrame({\n    \"order_id\": [101, 102, 103, 104, 105, 106],\n    \"user_id\": [1, 2, 1, 3, 99, 2],  # Note: user 99 doesn't exist\n    \"order_date\": [datetime(2026, 1, 5), datetime(2026, 1, 8),\n                   datetime(2026, 1, 12), datetime(2026, 1, 15),\n                   datetime(2026, 1, 18), datetime(2026, 1, 22)],\n    \"total\": [50.0, 75.5, 120.0, 30.0, 200.0, 45.0],\n})\n\n# Order items (some orders have multiple items)\nitems = pl.DataFrame({\n    \"order_id\": [101, 101, 102, 103, 103, 103, 104, 105, 106],\n    \"product\": [\"A\", \"B\", \"C\", \"A\", \"D\", \"E\", \"B\", \"F\", \"A\"],\n    \"quantity\": [2, 1, 3, 1, 2, 1, 1, 5, 2],\n    \"price\": [20.0, 10.0, 25.0, 20.0, 30.0, 50.0, 10.0, 40.0, 20.0],\n})\n\n# Support tickets\ntickets = pl.DataFrame({\n    \"ticket_id\": [1, 2, 3],\n    \"user_id\": [1, 1, 99],  # User 99 doesn't exist\n    \"issue\": [\"Refund\", \"Question\", \"Complaint\"],\n})\nTasks:\n\nEnrich Orders: Create a report showing all orders with user email (left join). How many orders have no matching user?\nFind Orphans: Identify orders placed by non-existent users using anti join.\nActive Users: Find users who have placed at least one order (semi join). What percentage of registered users are active?\nUser Order Summary: For each user, calculate: total orders, total spent, average order value. Include users with zero orders.\nOrder Items Aggregation: Join orders with items and calculate: total items per order, verify order total matches sum(items).\nSupport Analysis: Find users who have opened support tickets but never placed an order.\nTime-Series Match: Using the signup_date and order_date, find each user’s first order time using asof join.\n\n\n\nClick to see solutions\n\n# Task 1: Enrich orders with user emails\nenriched_orders = orders.join(\n    users.select(\"user_id\", \"email\"),\n    on=\"user_id\",\n    how=\"left\"\n)\nprint(f\"Orders without matching user: {enriched_orders.filter(pl.col('email').is_null()).shape[0]}\")\n\n# Task 2: Find orphan orders (anti join)\norphan_orders = orders.join(\n    users.select(\"user_id\"),\n    on=\"user_id\",\n    how=\"anti\"\n)\nprint(\"Orphan orders:\")\nprint(orphan_orders)\n\n# Task 3: Active users (semi join)\nactive_users = users.join(\n    orders.select(\"user_id\").unique(),\n    on=\"user_id\",\n    how=\"semi\"\n)\nactive_pct = len(active_users) / len(users) * 100\nprint(f\"Active users: {len(active_users)}/{len(users)} ({active_pct:.1f}%)\")\n\n# Task 4: User order summary (groupby with left join for all users)\nuser_summary = orders.group_by(\"user_id\").agg([\n    pl.count().alias(\"total_orders\"),\n    pl.col(\"total\").sum().alias(\"total_spent\"),\n    pl.col(\"total\").mean().alias(\"avg_order_value\"),\n]).join(\n    users.select(\"user_id\", \"email\"),\n    on=\"user_id\",\n    how=\"right\"  # Keep all users even without orders\n).fill_null(0).select([\n    \"user_id\", \"email\", \"total_orders\", \"total_spent\", \"avg_order_value\"\n])\nprint(user_summary)\n\n# Task 5: Verify order totals\norder_totals = items.group_by(\"order_id\").agg(\n    pl.col(\"quantity\").sum().alias(\"total_items\"),\n    (pl.col(\"quantity\") * pl.col(\"price\")).sum().alias(\"calculated_total\")\n).join(\n    orders.select(\"order_id\", \"total\"),\n    on=\"order_id\"\n).with_columns(\n    (pl.col(\"calculated_total\") == pl.col(\"total\")).alias(\"total_matches\")\n)\nprint(order_totals)\n\n# Task 6: Support tickets from users without orders\nticket_only_users = tickets.join(\n    orders.select(\"user_id\").unique(),\n    on=\"user_id\",\n    how=\"anti\"\n).join(users.select(\"user_id\", \"email\"), on=\"user_id\", how=\"left\")\nprint(\"Users with tickets but no orders:\")\nprint(ticket_only_users)\n\n# Task 7: First order time per user (asof join)\nuser_first_order = users.join_asof(\n    orders.select(\"user_id\", \"order_date\").sort(\"order_date\"),\n    left_on=\"signup_date\",\n    right_on=\"order_date\",\n    by=\"user_id\",\n    strategy=\"forward\"\n).select([\n    \"user_id\", \"email\", \"signup_date\", \"order_date\"\n])\nprint(\"User signup and first order:\")\nprint(user_first_order)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-02-08-day008-joins.html#resources",
    "href": "posts/100-days-of-polars/2026-02-08-day008-joins.html#resources",
    "title": "100 Days of Polars - Day 008: Mastering Joins - From Basic to Advanced",
    "section": "Resources",
    "text": "Resources\n\nPolars Join Documentation\nAsOf Join Guide"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "June 2024 - Present\n\nBuilding computer vision solutions for edge devices"
  },
  {
    "objectID": "cv.html#experience",
    "href": "cv.html#experience",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "June 2024 - Present\n\nBuilding computer vision solutions for edge devices"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "Education",
    "text": "Education\n\nMaster’s Degree in High Performance Computing, Chalmers University of Technology, Sweden\nBachelor’s Degree in Electrical Engineering, Beijing Normal University, China"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Curriculum Vitae",
    "section": "Skills",
    "text": "Skills\n\nSpecilization: Digital Image Processing, Computer Vision, Object Detection, Object Segmentation, Edge Computing, Efficient ML\nTools: PyTorch/Keras/TensorFlow, TFLite/Micro\nProgramming Languages: Python/C++"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blogs",
    "section": "",
    "text": "100 Days of Polars - Day 008: Mastering Joins - From Basic to Advanced\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\njoins\n\n\n100-days-of-polars\n\n\n\nComprehensive guide to all join types in Polars with practical examples\n\n\n\n\n\nFeb 8, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\n100 Days of Polars - Day 007: Date/Time — Parsing, Durations, and Time Zones\n\n\n\n\n\n\npolars\n\n\ndatetime\n\n\n100-days-of-polars\n\n\n\nPractical guide to parsing dates/times, working with durations, and handling time zones in Polars\n\n\n\n\n\nFeb 3, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\n100 Days of Polars - Day 006: Data Integrity - Handling Nulls and Schema Validation\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\n100-days-of-polars\n\n\n\nLearn to maintain data integrity in Polars through null handling techniques and schema validation strategies\n\n\n\n\n\nFeb 2, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\n100 Days of Polars - Day 005: Working with Structs - Nested Data Structures\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\n100-days-of-polars\n\n\n\nUnderstanding how to work with struct columns in Polars for hierarchical and nested data\n\n\n\n\n\nJan 29, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\n100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\n100-days-of-polars\n\n\n\nUnderstanding how to work with nested list data in Polars using explode, implode, and list operations\n\n\n\n\n\nJan 28, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\n100 Days of Polars - Day 003: map_elements vs map_batches vs Native Expressions\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\n100-days-of-polars\n\n\n\nUnderstanding when to use map_elements, map_batches, and native Polars expressions\n\n\n\n\n\nJan 19, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\n100 Days of Polars - Day 002: Window Functions\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\n100-days-of-polars\n\n\n\nMaster window functions in Polars for advanced data transformations\n\n\n\n\n\nJan 18, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\n100 Days of Polars - Day 001: Polars Selectors\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\n100-days-of-polars\n\n\n\nPolars selectors for efficient column selection\n\n\n\n\n\nJan 17, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\nDifference between TFLite Python API vs TFLite Micro Python API\n\n\n\n\n\n\ntflite\n\n\ntflite-micro\n\n\ncode\n\n\ntinyml\n\n\n\n\n\n\n\n\n\nAug 5, 2025\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nAug 2, 2025\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  }
]