[
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "June 2024 - Present\n\nBuilding computer vision solutions for edge devices"
  },
  {
    "objectID": "cv.html#experience",
    "href": "cv.html#experience",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "June 2024 - Present\n\nBuilding computer vision solutions for edge devices"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "Education",
    "text": "Education\n\nMaster’s Degree in High Performance Computing, Chalmers University of Technology, Sweden\nBachelor’s Degree in Electrical Engineering, Beijing Normal University, China"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Curriculum Vitae",
    "section": "Skills",
    "text": "Skills\n\nSpecilization: Digital Image Processing, Computer Vision, Object Detection, Object Segmentation, Edge Computing, Efficient ML\nTools: PyTorch/Keras/TensorFlow, TFLite/Micro\nProgramming Languages: Python/C++"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blogs",
    "section": "",
    "text": "Difference between TFLite Python API vs TFLite Micro Python API\n\n\n\ntflite\n\ntflite-micro\n\ncode\n\ntinyml\n\n\n\n\n\n\n\n\n\nAug 5, 2025\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nAug 2, 2025\n\n\nTristan O’Malley\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-08-05-tfl-vs-tflm/index.html",
    "href": "posts/2025-08-05-tfl-vs-tflm/index.html",
    "title": "Difference between TFLite Python API vs TFLite Micro Python API",
    "section": "",
    "text": "TFLite Micro is the most popular Neural Network inference engine for micro CPUs. It’s designed for light weight model running on low power hardwares.\nMost common scenario is you use some training framework like PyTorch and TensorFlow, quantize it to tflite model. Finally you run your model on embedded device through the TFLite Micro library. However, the precision of the model usually loses during the quantization process. People would like to use the TFLite Python API to run the model on their computer, and verify the accuracy of the model is reduced to an acceptable level before deploying it to the embedded device. The problem is that the TFLite Python API cannot really simulate the model’s precision when running on the embedded device with TFLite Micro library. There are already lots of discussions about this issue, see github discussions 1 and 2\nUnknown to many people, TFLite Micro has a Python API, but the TFLite Micro Python API is different from the TFLite Python API. The TFLite Micro Python API is designed to run on the same principles as the TFLite Micro C++ library, which means it can provide a more accurate simulation of how the model will perform on embedded devices.\nIn this blog I will compare the TFLite Python API and the TFLite Micro Python API differences, and show how the two different APIs can be used to run the same model but have different results."
  },
  {
    "objectID": "posts/2025-08-05-tfl-vs-tflm/index.html#tflite-python-api",
    "href": "posts/2025-08-05-tfl-vs-tflm/index.html#tflite-python-api",
    "title": "Difference between TFLite Python API vs TFLite Micro Python API",
    "section": "TFLite Python API",
    "text": "TFLite Python API\nThe TFLite was previously decoupled from TensorFlow and was a standalone library called LiteRT now.\n\nLoading model:\nfrom ai-edge-litert.interpreter import Interpreter\ninterpreter = Interpreter(model_path=model)\nAllocate tensors:\ninterpreter.allocate_tensors()\nIt performs dynamic memory allocation for: a. allocates memory buffers for model inputs and outputs, b. allocates memory for all intermediate computation results between layers, c. allocates workspace memory needed during inference, d. finalizes the computation graph and prepares it for execution.\nGet input and output tensors details:\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nSet input tensor:\ninterpreter.set_tensor(input_details[0]['index'], x)\nRun inference:\ninterpreter.invoke()\nGet output tensor:\noutput_data = interpreter.get_tensor(output_details[0]['index'])"
  },
  {
    "objectID": "posts/2025-08-05-tfl-vs-tflm/index.html#tflite-micro-python-api",
    "href": "posts/2025-08-05-tfl-vs-tflm/index.html#tflite-micro-python-api",
    "title": "Difference between TFLite Python API vs TFLite Micro Python API",
    "section": "TFLite Micro Python API",
    "text": "TFLite Micro Python API\nThe TFLite Micro Python API is designed to run on the same principles as the TFLite Micro C++ library, it’s more aligned with the C++ API.\n\nLoading model, there are two ways to load the model file, one is from file, another is from bytearray.\nfrom tflite_micro.python.tflite_micro import runtime\n# Load from file\ninterpreter = runtime.Interpreter.from_file(model_path=model)\n# Load from bytearray\ninterpreter = runtime.Interpreter.from_bytes(model=model_bytes)\nThere is no need to allocate tensors. TFLite Micro pre-allocates all memory at compile time or initialization using a fixed-size arena. Embedded systems often lack heap allocation or have strict memory constraints, so TFLite Micro avoids malloc/free entirely.\nGet input and output tensors details:\ninput_details = interpreter.get_input_details(index)\noutput_details = interpreter.get_output_details(index)\nCompare to TFLite Python API where you can get all input and output details through one function call. In TFLite Micro Python API, you need to specify the index of the input or output tensor to get its details. For example, if you have multiple inputs or outputs, you can get the details of each one by specifying its index:\ninput_details = interpreter.get_input_details(0)  # Get details of first input tensor\noutput_details = interpreter.get_output_details(1)  # Get details of second output tensor\nSet input tensor:\n# Set input tensor using the input index\ninterpreter.set_input(x, 0)\n# If you have second input tensor:\ninterpreter.set_input(y, 1)\nCompared to TFLite Python API where you set the input tensor using the input details index, in TFLite Micro Python API, you set the input tensor using the input index directly. The index range depends on how many inputs you have. The order between the data and the index was reversed as well.\nRun inference:\ninterpreter.invoke()\nGet output tensor:\noutput_data = interpreter.get_output(0)\nThe same as set input tensor, you can use the output index depending on how many outputs you have."
  },
  {
    "objectID": "posts/2025-08-05-tfl-vs-tflm/index.html#difference-between-tflite-and-tflite-micro-output",
    "href": "posts/2025-08-05-tfl-vs-tflm/index.html#difference-between-tflite-and-tflite-micro-output",
    "title": "Difference between TFLite Python API vs TFLite Micro Python API",
    "section": "Difference between TFLite and TFLite Micro Output",
    "text": "Difference between TFLite and TFLite Micro Output\nI will use the yolov8n example from ultralytics to compare the difference between tflite and tflite-micro. They provide a script to inference yolov8 tflite model and visualize the result, makes it quite convenient. I make another class that subclass the YOLOv8TFLite example but using tflite-micro python package to do the inference.\nIf you are interested to follow the example you can refer my repo tflite_vs_tflite-micro.\n\nInference Speed\nCompared to TFLite python package, the TFLite Micro python package is much slower to run on PC. In this example, tflite python package only takes 0.04 second while tflite micro python package takes 76.7 second - 1917 times slower!!.\n\n\nInference Result\n\n\n \n\nIf you observe in detail, you could see that the bounding box of the bus is different and tflite predicts 0.34 confidence for the cut off person while tflite-micro predicts 0.39."
  },
  {
    "objectID": "posts/2025-08-05-tfl-vs-tflm/index.html#conclusion",
    "href": "posts/2025-08-05-tfl-vs-tflm/index.html#conclusion",
    "title": "Difference between TFLite Python API vs TFLite Micro Python API",
    "section": "Conclusion",
    "text": "Conclusion\nFrom the example above, we could see that due to the implementation difference TFLite Python API and TFLite Micro Python produced different result. I have seen worse cases than where the tflite result and tflite-micro result diverge a lot this from my past experience. My suggestion is if you want to check or reproduce your embedded tflite-micro result with python script, tflite-micro python package is a better choice than tflite python package. But due to the slow inference speed of tflite-micro package, it’s not suitable for evaluation the whole evaluation dataset."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Nomad's Wonderland",
    "section": "",
    "text": "List of projects I worked with."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a machine learning enginer with focus on computer vision and effcient machine learning solutions on edge based in Sweden.\nI mainly work with image related tasks nowadays from image signal processing, object detection/tracking/segmentation, face recognition systems."
  }
]