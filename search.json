[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Nomad's Wonderland",
    "section": "",
    "text": "List of projects I worked with."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "June 2024 - Present\n\nBuilding computer vision solutions for edge devices"
  },
  {
    "objectID": "cv.html#experience",
    "href": "cv.html#experience",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "June 2024 - Present\n\nBuilding computer vision solutions for edge devices"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "Education",
    "text": "Education\n\nMaster’s Degree in High Performance Computing, Chalmers University of Technology, Sweden\nBachelor’s Degree in Electrical Engineering, Beijing Normal University, China"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Curriculum Vitae",
    "section": "Skills",
    "text": "Skills\n\nSpecilization: Digital Image Processing, Computer Vision, Object Detection, Object Segmentation, Edge Computing, Efficient ML\nTools: PyTorch/Keras/TensorFlow, TFLite/Micro\nProgramming Languages: Python/C++"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html",
    "href": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html",
    "title": "100 Days of Polars - Day 003: map_elements vs map_batches vs Native Expressions",
    "section": "",
    "text": "Today we’ll explore how to apply custom logic in polars with map_elements when the functionality isn’t available through polars’s built-in expression.\nimport random\nimport string\nimport time\nimport polars as pl\n\ndef random_string(length=8):\n    return ''.join(random.choices(string.ascii_lowercase, k=length))\n\ndf = pl.DataFrame({\n    \"text\": [random_string() for _ in range(1_000_000)]\n})\n\n# Apply a custom function to each element\nt0 = time.time()\nresult = df.with_columns(\n    pl.col(\"text\").map_elements(lambda x: x.upper()).alias(\"uppercase\")\n)\nt1 = time.time()\nprint(f\"map_elements: {t1-t0}\")\nWhen you run this code in polars you will get a warning like this:\n&lt;ipython-input-51-4b0fe28e2086&gt;:9: PolarsInefficientMapWarning: \nExpr.map_elements is significantly slower than the native expressions API.\nOnly use if you absolutely CANNOT implement your logic otherwise.\nReplace this expression...\n  - pl.col(\"text\").map_elements(lambda x: ...)\nwith this one instead:\n  + pl.col(\"text\").str.to_uppercase()\n\n  pl.col(\"text\").map_elements(lambda x: x.upper()).alias(\"uppercase\")\nThe reason for this warning is unlike native polars expressions, map_elements are not parallelized and usually involves data copy betten polars’s rust engine and python interpreter. When we use the suggested code, time performance increased more than 3x for this small dataset and simple task.\nt0 = time.time()\nresult = df.with_columns(\n    pl.col(\"text\").str.to_uppercase().alias(\"uppercase\")\n)\nt1 = time.time()\nprint(f\"native expression: {t1-t0}\")"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html#introduction",
    "href": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html#introduction",
    "title": "100 Days of Polars - Day 003: map_elements vs map_batches vs Native Expressions",
    "section": "",
    "text": "Today we’ll explore how to apply custom logic in polars with map_elements when the functionality isn’t available through polars’s built-in expression.\nimport random\nimport string\nimport time\nimport polars as pl\n\ndef random_string(length=8):\n    return ''.join(random.choices(string.ascii_lowercase, k=length))\n\ndf = pl.DataFrame({\n    \"text\": [random_string() for _ in range(1_000_000)]\n})\n\n# Apply a custom function to each element\nt0 = time.time()\nresult = df.with_columns(\n    pl.col(\"text\").map_elements(lambda x: x.upper()).alias(\"uppercase\")\n)\nt1 = time.time()\nprint(f\"map_elements: {t1-t0}\")\nWhen you run this code in polars you will get a warning like this:\n&lt;ipython-input-51-4b0fe28e2086&gt;:9: PolarsInefficientMapWarning: \nExpr.map_elements is significantly slower than the native expressions API.\nOnly use if you absolutely CANNOT implement your logic otherwise.\nReplace this expression...\n  - pl.col(\"text\").map_elements(lambda x: ...)\nwith this one instead:\n  + pl.col(\"text\").str.to_uppercase()\n\n  pl.col(\"text\").map_elements(lambda x: x.upper()).alias(\"uppercase\")\nThe reason for this warning is unlike native polars expressions, map_elements are not parallelized and usually involves data copy betten polars’s rust engine and python interpreter. When we use the suggested code, time performance increased more than 3x for this small dataset and simple task.\nt0 = time.time()\nresult = df.with_columns(\n    pl.col(\"text\").str.to_uppercase().alias(\"uppercase\")\n)\nt1 = time.time()\nprint(f\"native expression: {t1-t0}\")"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html#map_batches",
    "href": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html#map_batches",
    "title": "100 Days of Polars - Day 003: map_elements vs map_batches vs Native Expressions",
    "section": "map_batches",
    "text": "map_batches\nmap_batches is another tool that applies a Python function to batches (chunks) of data in polars. It’s a middle group beteewn map_elements and native expression. It also allows using numpy/other libraries on chunks, so it might be much faster than native expression when these library are highly optimized.\nLet’s look at an example below:\nimport polars as pl\nimport numpy as np\nimport time\n\nn_rows = 1_000_000\nn_groups = 4\nunique_groups = [f\"grp_{i}\" for i in range(n_groups)]\ndf = pl.DataFrame({\n    \"value\": np.random.randn(n_rows),\n    \"group\": np.random.choice(unique_groups, n_rows)\n})\n\n\ngroup_stats = df.group_by(\"group\").agg([\n    pl.col(\"value\").mean().alias(\"mean\"),\n    pl.col(\"value\").std().alias(\"std\")\n])\n# Method 1: map_elements (SLOW)\nstart = time.time()\nresult = df.join(group_stats, on=\"group\").with_columns(\n    pl.struct([\"value\", \"mean\", \"std\"])\n    .map_elements(\n        lambda row: (row[\"value\"] - row[\"mean\"]) / row[\"std\"],\n        return_dtype=pl.Float64\n    )\n    .alias(\"z_score\")\n)\ntotal = time.time() - start\nprint(f\"map_elements (1M rows):     {total:.4f} seconds\")\n\n# Method 2: Native expression (FAST)\nstart = time.time()\nresult = df.with_columns(\n    ((pl.col(\"value\") - pl.col(\"value\").mean().over(\"group\")) / \n     pl.col(\"value\").std().over(\"group\"))\n    .alias(\"z_score\")\n)\ntotal = time.time() - start\nprint(f\"Native expression (1M rows): {total:.4f} seconds\")\n\n# Method 3: map_batches (MIDDLE GROUND)\ndef normalize_batch(series: pl.Series) -&gt; pl.Series:\n    # Using numpy for vectorized operations on the batch\n    arr = series.to_numpy()\n    return pl.Series((arr - arr.mean()) / arr.std())\n\nstart = time.time()\nresult = df.with_columns(\n    pl.col(\"value\").map_batches(normalize_batch).over(\"group\").alias(\"z_score\")\n)\ntotal = time.time() - start\nprint(f\"map_batches (1M rows): {total:.4f} seconds\")\nWhen you look at the result below, you will find out map_batches is faster than native expression and map_elements is the slowest.\nmap_elements (1M rows):     0.3763 seconds\nNative expression (1M rows): 0.0362 seconds\nmap_batches (1M rows): 0.0299 seconds\nBut if we increase the n_groups to 10000, we will get result below:\nmap_elements (1M rows):     0.6927 seconds\nNative expression (1M rows): 0.0621 seconds\nmap_batches (1M rows): 0.4327 seconds\nThe result reverted, the number of groups decies how many context switches polars has to perform from rust engine to numpy. The more complicate the logic is, the more benefits native expression will bring."
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html#resources",
    "href": "posts/100-days-of-polars/2026-01-19-day003-map-functions.html#resources",
    "title": "100 Days of Polars - Day 003: map_elements vs map_batches vs Native Expressions",
    "section": "Resources",
    "text": "Resources\n\nPolars map_elements Documentation\nPolars map_batches Documentation\nPolars Expression API"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "",
    "text": "Today we’ll explore how to work with list columns in Polars. Lists are a powerful feature that allow you to store multiple values in a single cell, making it easy to represent nested or hierarchical data. We’ll cover explode to flatten lists, implode to create lists, and various list operations.\nimport polars as pl\n\n# Create a DataFrame with a list column\ndf = pl.DataFrame({\n    \"user_id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"purchases\": [\n        [\"laptop\", \"mouse\", \"keyboard\"],\n        [\"phone\"],\n        [\"tablet\", \"headphones\"]\n    ]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n})\n\nprint(df)\nshape: (3, 3)\n┌─────────┬─────────┬──────────────────────────────┐\n│ user_id ┆ name    ┆ purchases                    │\n│ ---     ┆ ---     ┆ ---                          │\n│ i64     ┆ str     ┆ list[str]                    │\n╞═════════╪═════════╪══════════════════════════════╡\n│ 1       ┆ Alice   ┆ [\"laptop\", \"mouse\", \"keyb... │\n│ 2       ┆ Bob     ┆ [\"phone\"]                    │\n│ 3       ┆ Charlie ┆ [\"tablet\", \"headphones\"]     │\n└─────────┴─────────┴──────────────────────────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#introduction",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#introduction",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "",
    "text": "Today we’ll explore how to work with list columns in Polars. Lists are a powerful feature that allow you to store multiple values in a single cell, making it easy to represent nested or hierarchical data. We’ll cover explode to flatten lists, implode to create lists, and various list operations.\nimport polars as pl\n\n# Create a DataFrame with a list column\ndf = pl.DataFrame({\n    \"user_id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"purchases\": [\n        [\"laptop\", \"mouse\", \"keyboard\"],\n        [\"phone\"],\n        [\"tablet\", \"headphones\"]\n    ]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n})\n\nprint(df)\nshape: (3, 3)\n┌─────────┬─────────┬──────────────────────────────┐\n│ user_id ┆ name    ┆ purchases                    │\n│ ---     ┆ ---     ┆ ---                          │\n│ i64     ┆ str     ┆ list[str]                    │\n╞═════════╪═════════╪══════════════════════════════╡\n│ 1       ┆ Alice   ┆ [\"laptop\", \"mouse\", \"keyb... │\n│ 2       ┆ Bob     ┆ [\"phone\"]                    │\n│ 3       ┆ Charlie ┆ [\"tablet\", \"headphones\"]     │\n└─────────┴─────────┴──────────────────────────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#exploding-lists",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#exploding-lists",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "Exploding Lists",
    "text": "Exploding Lists\nThe explode method transforms each element of a list into a separate row, duplicating the other column values. This is useful when you need to analyze individual list elements.\n# Explode the purchases column\nexploded_df = df.explode(\"purchases\")\nprint(exploded_df)\nshape: (6, 3)\n┌─────────┬─────────┬───────────┐\n│ user_id ┆ name    ┆ purchases │\n│ ---     ┆ ---     ┆ ---       │\n│ i64     ┆ str     ┆ str       │\n╞═════════╪═════════╪═══════════╡\n│ 1       ┆ Alice   ┆ laptop    │\n│ 1       ┆ Alice   ┆ mouse     │\n│ 1       ┆ Alice   ┆ keyboard  │\n│ 2       ┆ Bob     ┆ phone     │\n│ 3       ┆ Charlie ┆ tablet    │\n│ 3       ┆ Charlie ┆ headphones│\n└─────────┴─────────┴───────────┘\nYou can explode multiple columns simultaneously if they have the same length:\ndf_multi = pl.DataFrame({\n    \"id\": [1, 2],\n    \"values_a\": [[1, 2, 3], [4, 5]],\n    \"values_b\": [[\"a\", \"b\", \"c\"], [\"d\", \"e\"]]\n})\n\n# Explode both list columns\nexploded_multi = df_multi.explode([\"values_a\", \"values_b\"])\nprint(exploded_multi)\nshape: (5, 3)\n┌─────┬──────────┬──────────┐\n│ id  ┆ values_a ┆ values_b │\n│ --- ┆ ---      ┆ ---      │\n│ i64 ┆ i64      ┆ str      │\n╞═════╪══════════╪══════════╡\n│ 1   ┆ 1        ┆ a        │\n│ 1   ┆ 2        ┆ b        │\n│ 1   ┆ 3        ┆ c        │\n│ 2   ┆ 4        ┆ d        │\n│ 2   ┆ 5        ┆ e        │\n└─────┴──────────┴──────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#creating-lists-with-implode",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#creating-lists-with-implode",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "Creating Lists with implode",
    "text": "Creating Lists with implode\nThe implode method (also known as list in aggregation context) groups values back into lists. This is the inverse operation of explode. implode is called automatically in group_by().agg() operations.\n# Create individual purchase records\npurchases_df = pl.DataFrame({\n    \"user_id\": [1, 1, 1, 2, 3, 3],\n    \"item\": [\"laptop\", \"mouse\", \"keyboard\", \"phone\", \"tablet\", \"headphones\"],\n    \"price\": [1200, 25, 75, 800, 600, 150]\n})\n\n# Group items back into lists per user\ngrouped_df = purchases_df.group_by(\"user_id\").agg([\n    pl.col(\"item\").alias(\"items\"),\n    pl.col(\"price\").sum().alias(\"total_spent\")\n])\n\nprint(grouped_df)\nshape: (3, 3)\n┌─────────┬──────────────────────────────┬─────────────┐\n│ user_id ┆ items                        ┆ total_spent │\n│ ---     ┆ ---                          ┆ ---         │\n│ i64     ┆ list[str]                    ┆ i64         │\n╞═════════╪══════════════════════════════╪═════════════╡\n│ 1       ┆ [\"laptop\", \"mouse\", \"keyb... ┆ 1300        │\n│ 2       ┆ [\"phone\"]                    ┆ 800         │\n│ 3       ┆ [\"tablet\", \"headphones\"]     ┆ 750         │\n└─────────┴──────────────────────────────┴─────────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#list-operations",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#list-operations",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "List Operations",
    "text": "List Operations\nPolars provides a rich set of operations for working with list columns through the .list namespace.\n\nGetting List Length\ndf_with_length = df.with_columns(\n    pl.col(\"purchases\").list.len().alias(\"num_purchases\")\n)\nprint(df_with_length)\nshape: (3, 4)\n┌─────────┬─────────┬──────────────────────────────┬───────────────┐\n│ user_id ┆ name    ┆ purchases                    ┆ num_purchases │\n│ ---     ┆ ---     ┆ ---                          ┆ ---           │\n│ i64     ┆ str     ┆ list[str]                    ┆ u32           │\n╞═════════╪═════════╪══════════════════════════════╪═══════════════╡\n│ 1       ┆ Alice   ┆ [\"laptop\", \"mouse\", \"keyb... ┆ 3             │\n│ 2       ┆ Bob     ┆ [\"phone\"]                    ┆ 1             │\n│ 3       ┆ Charlie ┆ [\"tablet\", \"headphones\"]     ┆ 2             │\n└─────────┴─────────┴──────────────────────────────┴───────────────┘\n\n\nAccessing List Elements\n# Get the first item from each list\ndf_first = df.with_columns(\n    pl.col(\"purchases\").list.first().alias(\"first_purchase\")\n)\nprint(df_first)\n\n# Get the last item\ndf_last = df.with_columns(\n    pl.col(\"purchases\").list.last().alias(\"last_purchase\")\n)\n\n# Get item at specific index\ndf_indexed = df.with_columns(\n    pl.col(\"purchases\").list.get(1).alias(\"second_purchase\")\n)\nprint(df_indexed)\n\n\nSlicing Lists\n# Get first 2 items from each list\ndf_sliced = df.with_columns(\n    pl.col(\"purchases\").list.head(2).alias(\"first_two\")\n)\nprint(df_sliced)\n\n# Get all items except the first\ndf_tail = df.with_columns(\n    pl.col(\"purchases\").list.tail(-1).alias(\"rest\")\n)\n\n\nFiltering and Transforming Lists\n# Check if lists contain specific values\ndf_contains = df.with_columns(\n    pl.col(\"purchases\").list.contains(\"laptop\").alias(\"bought_laptop\")\n)\nprint(df_contains)\n\n# Apply expressions to list elements\ndf_numbers = pl.DataFrame({\n    \"id\": [1, 2, 3],\n    \"values\": [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n})\n\n# Square all values in each list\ndf_squared = df_numbers.with_columns(\n    pl.col(\"values\").list.eval(pl.element() * 2).alias(\"doubled\")\n)\nprint(df_squared)\n\n\nConcatenating Lists\ndf_concat = pl.DataFrame({\n    \"list1\": [[1, 2], [3, 4]],\n    \"list2\": [[5, 6], [7, 8]]\n})\n\n# Concatenate two list columns\ndf_merged = df_concat.with_columns(\n    pl.concat_list([\"list1\", \"list2\"]).alias(\"combined\")\n)\nprint(df_merged)\nshape: (2, 3)\n┌───────────┬───────────┬──────────────────┐\n│ list1     ┆ list2     ┆ combined         │\n│ ---       ┆ ---       ┆ ---              │\n│ list[i64] ┆ list[i64] ┆ list[i64]        │\n╞═══════════╪═══════════╪══════════════════╡\n│ [1, 2]    ┆ [5, 6]    ┆ [1, 2, 5, 6]     │\n│ [3, 4]    ┆ [7, 8]    ┆ [3, 4, 7, 8]     │\n└───────────┴───────────┴──────────────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#performance-considerations",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#performance-considerations",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "Performance Considerations",
    "text": "Performance Considerations\nWorking with list columns is generally efficient in Polars, but keep these tips in mind:\n\nUse native list operations instead of exploding and re-aggregating when possible\nExplode strategically - only when you need row-level operations\nList operations are vectorized - they’re much faster than using map_elements\nConsider Arrow’s memory layout - lists are stored efficiently in memory\n\nimport time\n\n# Create a larger dataset\nn = 100_000\nlarge_df = pl.DataFrame({\n    \"id\": range(n),\n    \"values\": [[i, i+1, i+2] for i in range(n)]\n})\n\n# Method 1: Using native list operations (FAST)\nstart = time.time()\nresult1 = large_df.with_columns(\n    pl.col(\"values\").list.sum().alias(\"sum\")\n)\ntime1 = time.time() - start\nprint(f\"Native list.sum(): {time1:.4f} seconds\")\n\n# Method 2: Explode and aggregate (SLOWER)\nstart = time.time()\nresult2 = large_df.explode(\"values\").group_by(\"id\").agg(\n    pl.col(\"values\").sum().alias(\"sum\")\n)\ntime2 = time.time() - start\nprint(f\"Explode + aggregate: {time2:.4f} seconds\")\n\nprint(f\"Native is {time2/time1:.1f}x faster\")"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#practice-exercise",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#practice-exercise",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "Practice Exercise",
    "text": "Practice Exercise\nNow it’s time to practice! Try solving this exercise using list operations:\nScenario: You’re analyzing e-commerce data with order information:\nimport polars as pl\n\n# Create sample e-commerce data\norders = pl.DataFrame({\n    \"order_id\": [1, 2, 3, 4],\n    \"customer_id\": [101, 102, 101, 103],\n    \"items\": [\n        [\"laptop\", \"mouse\"],\n        [\"phone\", \"case\", \"charger\"],\n        [\"keyboard\", \"monitor\"],\n        [\"tablet\"]\n    ],\n    \"prices\": [\n        [1200, 25],\n        [800, 15, 30],\n        [75, 300],\n        [600]\n    ]\n})\nTasks:\n\nAdd a column item_count that shows the number of items in each order\nExplode the data to create one row per item (hint: explode both items and prices columns)\nFilter to find only orders that have multiple items\nAdd a column max_price that shows the most expensive item in each order\nCreate a customer purchase history showing all items purchased, total amount spent, and number of unique orders per customer\n\nBonus Challenge: For each customer, create a new column that shows only items that cost more than $100. Use list operations to filter the prices and corresponding items lists.\n\n\nClick to see solutions\n\n# Task 1: Total items per order\nitems_per_order = orders.with_columns(\n    pl.col(\"items\").list.len().alias(\"item_count\")\n)\nprint(items_per_order)\n\n# Task 2: Explode to analyze individual items\nexploded_orders = orders.explode([\"items\", \"prices\"])\nprint(exploded_orders)\n\n# Task 3: Find customers who bought multiple items in an order\nmulti_item_orders = orders.filter(\n    pl.col(\"items\").list.len() &gt; 1\n)\nprint(multi_item_orders)\n\n# Task 4: Get most expensive item per order\nmost_expensive = orders.with_columns(\n    pl.col(\"prices\").list.max().alias(\"max_price\")\n)\nprint(most_expensive)\n\n# Task 5: Customer purchase history\ncustomer_history = exploded_orders.group_by(\"customer_id\").agg([\n    pl.col(\"items\").alias(\"all_items\"),\n    pl.col(\"prices\").sum().alias(\"total_spent\"),\n    pl.col(\"order_id\").n_unique().alias(\"num_orders\")\n])\nprint(customer_history)\n\n# Bonus: Filter expensive items per customer\nexpensive_items = orders.with_columns(\n    pl.col(\"items\").list.gather(\n        pl.col(\"prices\").list.eval(pl.arg_where(pl.element() &gt; 100))\n    ).alias(\"expensive_items\"),\n    \n    pl.col(\"prices\").list.gather(\n        pl.col(\"prices\").list.eval(pl.arg_where(pl.element() &gt; 100))\n    ).alias(\"expensive_prices\")\n)\nExpected output for Task 5:\nshape: (3, 4)\n┌─────────────┬──────────────────────────────┬─────────────┬────────────┐\n│ customer_id ┆ all_items                    ┆ total_spent ┆ num_orders │\n│ ---         ┆ ---                          ┆ ---         ┆ ---        │\n│ i64         ┆ list[str]                    ┆ i64         ┆ u32        │\n╞═════════════╪══════════════════════════════╪═════════════╪════════════╡\n│ 101         ┆ [\"laptop\", \"mouse\", \"keyb... ┆ 1600        ┆ 2          │\n│ 102         ┆ [\"phone\", \"case\", \"charger\"] ┆ 845         ┆ 1          │\n│ 103         ┆ [\"tablet\"]                   ┆ 600         ┆ 1          │\n└─────────────┴──────────────────────────────┴─────────────┴────────────┘"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#resources",
    "href": "posts/100-days-of-polars/2026-01-28-day004-explode-lists.html#resources",
    "title": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations",
    "section": "Resources",
    "text": "Resources\n\nPolars List Operations Documentation\nPolars explode Documentation\nWorking with Nested Data in Polars"
  },
  {
    "objectID": "posts/2025-08-05-tfl-vs-tflm/index.html",
    "href": "posts/2025-08-05-tfl-vs-tflm/index.html",
    "title": "Difference between TFLite Python API vs TFLite Micro Python API",
    "section": "",
    "text": "TFLite Micro is the most popular Neural Network inference engine for micro CPUs. It’s designed for light weight model running on low power hardwares.\nMost common scenario is you use some training framework like PyTorch and TensorFlow, quantize it to tflite model. Finally you run your model on embedded device through the TFLite Micro library. However, the precision of the model usually loses during the quantization process. People would like to use the TFLite Python API to run the model on their computer, and verify the accuracy of the model is reduced to an acceptable level before deploying it to the embedded device. The problem is that the TFLite Python API cannot really simulate the model’s precision when running on the embedded device with TFLite Micro library. There are already lots of discussions about this issue, see github discussions 1 and 2\nUnknown to many people, TFLite Micro has a Python API, but the TFLite Micro Python API is different from the TFLite Python API. The TFLite Micro Python API is designed to run on the same principles as the TFLite Micro C++ library, which means it can provide a more accurate simulation of how the model will perform on embedded devices.\nIn this blog I will compare the TFLite Python API and the TFLite Micro Python API differences, and show how the two different APIs can be used to run the same model but have different results."
  },
  {
    "objectID": "posts/2025-08-05-tfl-vs-tflm/index.html#tflite-python-api",
    "href": "posts/2025-08-05-tfl-vs-tflm/index.html#tflite-python-api",
    "title": "Difference between TFLite Python API vs TFLite Micro Python API",
    "section": "TFLite Python API",
    "text": "TFLite Python API\nThe TFLite was previously decoupled from TensorFlow and was a standalone library called LiteRT now.\n\nLoading model:\nfrom ai-edge-litert.interpreter import Interpreter\ninterpreter = Interpreter(model_path=model)\nAllocate tensors:\ninterpreter.allocate_tensors()\nIt performs dynamic memory allocation for: a. allocates memory buffers for model inputs and outputs, b. allocates memory for all intermediate computation results between layers, c. allocates workspace memory needed during inference, d. finalizes the computation graph and prepares it for execution.\nGet input and output tensors details:\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nSet input tensor:\ninterpreter.set_tensor(input_details[0]['index'], x)\nRun inference:\ninterpreter.invoke()\nGet output tensor:\noutput_data = interpreter.get_tensor(output_details[0]['index'])"
  },
  {
    "objectID": "posts/2025-08-05-tfl-vs-tflm/index.html#tflite-micro-python-api",
    "href": "posts/2025-08-05-tfl-vs-tflm/index.html#tflite-micro-python-api",
    "title": "Difference between TFLite Python API vs TFLite Micro Python API",
    "section": "TFLite Micro Python API",
    "text": "TFLite Micro Python API\nThe TFLite Micro Python API is designed to run on the same principles as the TFLite Micro C++ library, it’s more aligned with the C++ API.\n\nLoading model, there are two ways to load the model file, one is from file, another is from bytearray.\nfrom tflite_micro.python.tflite_micro import runtime\n# Load from file\ninterpreter = runtime.Interpreter.from_file(model_path=model)\n# Load from bytearray\ninterpreter = runtime.Interpreter.from_bytes(model=model_bytes)\nThere is no need to allocate tensors. TFLite Micro pre-allocates all memory at compile time or initialization using a fixed-size arena. Embedded systems often lack heap allocation or have strict memory constraints, so TFLite Micro avoids malloc/free entirely.\nGet input and output tensors details:\ninput_details = interpreter.get_input_details(index)\noutput_details = interpreter.get_output_details(index)\nCompare to TFLite Python API where you can get all input and output details through one function call. In TFLite Micro Python API, you need to specify the index of the input or output tensor to get its details. For example, if you have multiple inputs or outputs, you can get the details of each one by specifying its index:\ninput_details = interpreter.get_input_details(0)  # Get details of first input tensor\noutput_details = interpreter.get_output_details(1)  # Get details of second output tensor\nSet input tensor:\n# Set input tensor using the input index\ninterpreter.set_input(x, 0)\n# If you have second input tensor:\ninterpreter.set_input(y, 1)\nCompared to TFLite Python API where you set the input tensor using the input details index, in TFLite Micro Python API, you set the input tensor using the input index directly. The index range depends on how many inputs you have. The order between the data and the index was reversed as well.\nRun inference:\ninterpreter.invoke()\nGet output tensor:\noutput_data = interpreter.get_output(0)\nThe same as set input tensor, you can use the output index depending on how many outputs you have."
  },
  {
    "objectID": "posts/2025-08-05-tfl-vs-tflm/index.html#difference-between-tflite-and-tflite-micro-output",
    "href": "posts/2025-08-05-tfl-vs-tflm/index.html#difference-between-tflite-and-tflite-micro-output",
    "title": "Difference between TFLite Python API vs TFLite Micro Python API",
    "section": "Difference between TFLite and TFLite Micro Output",
    "text": "Difference between TFLite and TFLite Micro Output\nI will use the yolov8n example from ultralytics to compare the difference between tflite and tflite-micro. They provide a script to inference yolov8 tflite model and visualize the result, makes it quite convenient. I make another class that subclass the YOLOv8TFLite example but using tflite-micro python package to do the inference.\nIf you are interested to follow the example you can refer my repo tflite_vs_tflite-micro.\n\nInference Speed\nCompared to TFLite python package, the TFLite Micro python package is much slower to run on PC. In this example, tflite python package only takes 0.04 second while tflite micro python package takes 76.7 second - 1917 times slower!!.\n\n\nInference Result\n\n\n \n\nIf you observe in detail, you could see that the bounding box of the bus is different and tflite predicts 0.34 confidence for the cut off person while tflite-micro predicts 0.39."
  },
  {
    "objectID": "posts/2025-08-05-tfl-vs-tflm/index.html#conclusion",
    "href": "posts/2025-08-05-tfl-vs-tflm/index.html#conclusion",
    "title": "Difference between TFLite Python API vs TFLite Micro Python API",
    "section": "Conclusion",
    "text": "Conclusion\nFrom the example above, we could see that due to the implementation difference TFLite Python API and TFLite Micro Python produced different result. I have seen worse cases than where the tflite result and tflite-micro result diverge a lot this from my past experience. My suggestion is if you want to check or reproduce your embedded tflite-micro result with python script, tflite-micro python package is a better choice than tflite python package. But due to the slow inference speed of tflite-micro package, it’s not suitable for evaluation the whole evaluation dataset."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a machine learning enginer with focus on computer vision and effcient machine learning solutions on edge based in Sweden.\nI mainly work with image related tasks nowadays from image signal processing, object detection/tracking/segmentation, face recognition systems."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "",
    "text": "Today we will explore what polars selectors are, and what we can do with them."
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#introduction",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#introduction",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "",
    "text": "Today we will explore what polars selectors are, and what we can do with them."
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#what-are-polars-selectors",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#what-are-polars-selectors",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "What are Polars Selectors?",
    "text": "What are Polars Selectors?\nSelectors in polars are tools that you can use to select columns based on their properties, data types or patterns. For example if you want to select all string/numeric columns in your data frames:\nimport polars as pl\nimport polars.selectors as cs\n\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"test_score\": [88, 92],\n    \"final_score\": [95, 89],\n    \"category\": [\"A\", \"B\"],\n    \"updated_at\": [None, None]\n})\n\n# select all string columns\ndf.select(cs.string())\n\n# select all numeric columns\ndf.select(cs.numeric())\nThere are mainly three types of selectors in polars, 1. type based - you select columns based on data type; 2. pattern based - select columns based on pattern matching; 3. set logic selectors - combing multiple selectors together.\nWe will look at them by category"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#type-based-selectors",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#type-based-selectors",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "Type Based Selectors",
    "text": "Type Based Selectors\n\ncs.numeric()\n\n\ncs.string()\n\n\ncs.temporal()\nSelector targets columns with time-based data type.\nimport polars as pl\nimport polars.selectors as cs\nfrom datetime import datetime\n\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"transaction_date\": [datetime(2023, 5, 12), datetime(2023, 6, 15)],\n    \"upload_at\": [datetime(2023, 5, 13, 10, 0), datetime(2023, 6, 16, 11, 0)],\n    \"amount\": [100.0, 200.0]\n})\n\n# Use cs.temporal() to apply a date operation to all time columns\nresult = df.with_columns(\n    cs.temporal().dt.month_start()\n)\n\n\ncs.by_name()\n\nSelecting columns by name\n\nimport polars as pl\nimport polars.selectors as cs\n\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"test_score\": [88, 92],\n    \"final_score\": [95, 89],\n    \"category\": [\"A\", \"B\"],\n    \"updated_at\": [None, None]\n})\n\n# Select specific columns by exact name\ndf.select(cs.by_name(\"id\", \"category\"))\n\n\ncs.by_dtype()\n\nSelecting columns by data type"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#pattern-based-selectors",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#pattern-based-selectors",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "Pattern-based Selectors",
    "text": "Pattern-based Selectors\n\ncs.contains()\ncs.contains(\"score\")\n\n\ncs.matches()\n\nAllow to use regex patterns\n\nimport polars as pl\nimport polars.selectors as cs\n\ndf = pl.DataFrame({\n    \"abc_123\": [1],\n    \"abc_456\": [2],\n    \"xyz_123\": [3],\n    \"id_primary\": [4],\n    \"id_secondary\": [5]\n})\n# Select columns that start with 3 letters, an underscore, and then numbers\n# Pattern: ^[a-z]{3}_\\d+$\nresult = df.select(\n    cs.matches(r\"^[a-z]{3}_\\d+$\")\n)\n\n\ncs.starts_with()\ncs.starts_with(\"sale_\")\n\n\ncs.ends_with()\ncs.ends_with(\"sum\")"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#combining-selectors",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#combining-selectors",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "Combining Selectors",
    "text": "Combining Selectors\n\nSet Operations\n\nUnion (|)\n\ntarget_cols = cs.starts_with(\"sale_\") | cs.numeric()\n\ndf.select(target_cols)\n\nIntersection (&)\n\ntarget_cols = cs.starts_with(\"sale_\") & cs.numeric()\n\ndf.select(target_cols)\n\nDifference (-): used to exclude some columns\n\n# Logic: All Numerics MINUS the columns we want to protect\nfeatures = cs.numeric() - cs.by_name(\"user_id\", \"target_label\")\n\ndf.with_columns(\n    features.standardize()\n)\n\nComplement (~)\n\ndf.select(~cs.string())"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#practice-exercise",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#practice-exercise",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "Practice Exercise",
    "text": "Practice Exercise\nNow it’s time to practice! Try solving this exercise using selectors:\nScenario: You have a sales dataset with the following structure:\nimport polars as pl\nimport polars.selectors as cs\nfrom datetime import datetime\n\nsales_df = pl.DataFrame({\n    \"order_id\": [1001, 1002, 1003, 1004, 1005],\n    \"customer_id\": [501, 502, 503, 504, 505],\n    \"product_price\": [29.99, 149.99, 79.99, 199.99, 49.99],\n    \"shipping_cost\": [5.99, 12.99, 8.99, 15.99, 6.99],\n    \"tax_amount\": [2.40, 12.00, 6.40, 16.00, 4.00],\n    \"sale_region\": [\"North\", \"South\", \"East\", \"West\", \"North\"],\n    \"sale_channel\": [\"Online\", \"Store\", \"Online\", \"Store\", \"Online\"],\n    \"order_date\": [\n        datetime(2026, 1, 10),\n        datetime(2026, 1, 11),\n        datetime(2026, 1, 12),\n        datetime(2026, 1, 13),\n        datetime(2026, 1, 14)\n    ],\n    \"delivery_date\": [\n        datetime(2026, 1, 15),\n        datetime(2026, 1, 16),\n        datetime(2026, 1, 17),\n        datetime(2026, 1, 18),\n        datetime(2026, 1, 19)\n    ]\n})\nTasks:\n\nSelect all columns that contain the word “sale” in their name\nSelect all numeric columns EXCEPT the ID columns (order_id and customer_id)\nCalculate the sum of all columns that end with “_cost” or “_amount”\nExtract just the month from all temporal columns\nSelect all string columns that start with “sale_” and convert them to lowercase\n\nBonus Challenge: Create a single expression that selects all numeric columns (except IDs), rounds them to 2 decimal places, and adds a “_rounded” suffix to each column name.\n\n\nClick to see solutions\n\n# Task 1: Select columns containing \"sale\"\nsales_df.select(cs.contains(\"sale\"))\n\n# Task 2: Numeric columns excluding IDs\nsales_df.select(cs.numeric() - cs.by_name(\"order_id\", \"customer_id\"))\n\n# Task 3: Sum of cost and amount columns\nsales_df.select(\n    (cs.ends_with(\"_cost\") | cs.ends_with(\"_amount\")).sum()\n)\n\n# Task 4: Extract month from temporal columns\nsales_df.with_columns(\n    cs.temporal().dt.month()\n)\n\n# Task 5: Lowercase string columns starting with \"sale_\"\nsales_df.with_columns(\n    (cs.starts_with(\"sale_\") & cs.string()).str.to_lowercase()\n)\n\n# Bonus: Round numeric columns (except IDs) with suffix\nsales_df.with_columns(\n    (cs.numeric() - cs.by_name(\"order_id\", \"customer_id\"))\n    .round(2)\n    .name.suffix(\"_rounded\")\n)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#resources",
    "href": "posts/100-days-of-polars/2026-01-17-day001-selectors.html#resources",
    "title": "100 Days of Polars - Day 001: Polars Selectors",
    "section": "Resources",
    "text": "Resources\n\nPolars Documentation on Selectors"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-18-day002-window-function.html",
    "href": "posts/100-days-of-polars/2026-01-18-day002-window-function.html",
    "title": "100 Days of Polars - Day 002: Window Functions",
    "section": "",
    "text": "Window functions are one of the most powerful features in Polars for performing calculations across rows in a group. Unlike aggregations group_by that collapse rows into single values, window functions maintain all rows while computing values based on a “window” of data.\nWindow function allows you to:\n\nCalculate running totals and moving averages\nRank rows within groups\nCompare values with group averages\nPerform time-based calculations\n\nWindow function can be used through .over() in polars. You can achive the result as window function with group_by and join in polars.\nimport polars as pl\nfrom polars import window_function as wf\n\ndf = pl.DataFrame({\n    \"department\": [\"Sales\", \"Sales\", \"Sales\", \"Engineering\", \"Engineering\", \"Engineering\"],\n    \"employee\": [\"Alice\", \"Bob\", \"Carol\", \"David\", \"Eve\", \"Frank\"],\n    \"salary\": [50000, 60000, 55000, 75000, 80000, 72000]\n})\n\n# Add a column showing each employee's salary compared to their department's average.\ndf.with_columns(\n    dept_avg_salary=pl.col(\"salary\").mean().over(\"department\")\n)\n\n# Equivalent with group_by and join\ndept_avg = df.group_by(\"department\").agg(\n    pl.col(\"salary\").mean().alias(\"dept_avg_salary\")\n)\ndf.join(dept_avg, on=\"department\")"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#introduction-what-are-window-functions",
    "href": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#introduction-what-are-window-functions",
    "title": "100 Days of Polars - Day 002: Window Functions",
    "section": "",
    "text": "Window functions are one of the most powerful features in Polars for performing calculations across rows in a group. Unlike aggregations group_by that collapse rows into single values, window functions maintain all rows while computing values based on a “window” of data.\nWindow function allows you to:\n\nCalculate running totals and moving averages\nRank rows within groups\nCompare values with group averages\nPerform time-based calculations\n\nWindow function can be used through .over() in polars. You can achive the result as window function with group_by and join in polars.\nimport polars as pl\nfrom polars import window_function as wf\n\ndf = pl.DataFrame({\n    \"department\": [\"Sales\", \"Sales\", \"Sales\", \"Engineering\", \"Engineering\", \"Engineering\"],\n    \"employee\": [\"Alice\", \"Bob\", \"Carol\", \"David\", \"Eve\", \"Frank\"],\n    \"salary\": [50000, 60000, 55000, 75000, 80000, 72000]\n})\n\n# Add a column showing each employee's salary compared to their department's average.\ndf.with_columns(\n    dept_avg_salary=pl.col(\"salary\").mean().over(\"department\")\n)\n\n# Equivalent with group_by and join\ndept_avg = df.group_by(\"department\").agg(\n    pl.col(\"salary\").mean().alias(\"dept_avg_salary\")\n)\ndf.join(dept_avg, on=\"department\")"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#common-window-functions",
    "href": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#common-window-functions",
    "title": "100 Days of Polars - Day 002: Window Functions",
    "section": "Common Window Functions",
    "text": "Common Window Functions\n\nAggregations\nAll standard aggregation functions work as window functions, aggregation functions must come before window functions .over:\ndf.with_columns(\n    dept_salary_sum=pl.col(\"salary\").sum().over(\"department\"),\n    dept_salary_min=pl.col(\"salary\").min().over(\"department\"),\n    dept_salary_max=pl.col(\"salary\").max().over(\"department\"),\n    dept_salary_count=pl.col(\"salary\").count().over(\"department\")\n)\n\n\nCumulative Sums\ndf.sort(\"salary\").with_columns(\n    running_total=pl.col(\"salary\").cum_sum().over(\"department\")\n)\n\n\nRanking\ndf.with_columns(\n    salary_rank=pl.col(\"salary\").rank().over(\"department\")\n)\nAvailable ranking methods: - \"ordinal\" - sequential integers (1, 2, 3…) - \"dense\" - dense ranking without gaps - \"min\", \"max\", \"avg\" - various average ranking methods\n\n\nFirst and Last Values\ndf.sort(\"salary\").with_columns(\n    lowest_in_dept=pl.col(\"salary\").first().over(\"department\"),\n    highest_in_dept=pl.col(\"salary\").last().over(\"department\")\n)"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#practice-exercise",
    "href": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#practice-exercise",
    "title": "100 Days of Polars - Day 002: Window Functions",
    "section": "Practice Exercise",
    "text": "Practice Exercise\nScenario: You have a sales dataset with regional sales data:\nimport polars as pl\nfrom datetime import datetime\n\nsales_df = pl.DataFrame({\n    \"region\": [\"North\", \"North\", \"North\", \"South\", \"South\", \"South\", \"East\", \"East\"],\n    \"salesperson\": [\"Alice\", \"Bob\", \"Carol\", \"David\", \"Eve\", \"Frank\", \"Grace\", \"Henry\"],\n    \"quarter\": [\"Q1\", \"Q1\", \"Q2\", \"Q1\", \"Q2\", \"Q2\", \"Q1\", \"Q2\"],\n    \"sales\": [10000, 15000, 12000, 20000, 18000, 22000, 14000, 16000]\n})\nTasks:\n\nCalculate each salesperson’s sales as a percentage of their region’s total\nRank salespeople within each region by sales amount\nCalculate running total of sales within each region (sorted by sales amount)\nFind the top performer in each region and quarter\n\n\n\nClick to see solutions\n\n# Task 1: Percentage of regional total\nsales_df.with_columns(\n    region_total=pl.col(\"sales\").sum().over(\"region\"),\n    pct_of_region=(pl.col(\"sales\") / pl.col(\"region_total\")) * 100\n).drop(\"region_total\")\n\n# Task 2: Rank within region\nsales_df.with_columns(\n    rank_in_region=pl.col(\"sales\").rank(\"ordinal\", descending=True).over(\"region\")\n)\n\n# Task 3: Running total within region\nsales_df.sort(\"sales\", descending=True).with_columns(\n    running_total=pl.col(\"sales\").cum_sum().over(\"region\")\n)\n\n# Task 4: Top performer in each region and quarter\nsales_df.with_columns(\n    max_sales=pl.col(\"sales\").max().over(\"region\", \"quarter\")\n).filter(pl.col(\"sales\") == pl.col(\"max_sales\")).drop(\"max_sales\")"
  },
  {
    "objectID": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#resources",
    "href": "posts/100-days-of-polars/2026-01-18-day002-window-function.html#resources",
    "title": "100 Days of Polars - Day 002: Window Functions",
    "section": "Resources",
    "text": "Resources\n\nPolars Documentation on Window Functions\nPolars Window Function API"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blogs",
    "section": "",
    "text": "100 Days of Polars - Day 004: Working with Lists - explode, implode, and List Operations\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\n100-days-of-polars\n\n\n\nUnderstanding how to work with nested list data in Polars using explode, implode, and list operations\n\n\n\n\n\nJan 28, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\n100 Days of Polars - Day 003: map_elements vs map_batches vs Native Expressions\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\n100-days-of-polars\n\n\n\nUnderstanding when to use map_elements, map_batches, and native Polars expressions\n\n\n\n\n\nJan 19, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\n100 Days of Polars - Day 002: Window Functions\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\n100-days-of-polars\n\n\n\nMaster window functions in Polars for advanced data transformations\n\n\n\n\n\nJan 18, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\n100 Days of Polars - Day 001: Polars Selectors\n\n\n\n\n\n\npolars\n\n\ndata-engineering\n\n\n100-days-of-polars\n\n\n\nPolars selectors for efficient column selection\n\n\n\n\n\nJan 17, 2026\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\nDifference between TFLite Python API vs TFLite Micro Python API\n\n\n\n\n\n\ntflite\n\n\ntflite-micro\n\n\ncode\n\n\ntinyml\n\n\n\n\n\n\n\n\n\nAug 5, 2025\n\n\nNomadC\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nAug 2, 2025\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  }
]